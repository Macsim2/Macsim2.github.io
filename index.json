[{"content":"leet code WC439에 나온 palindrome 문제이다. 주요 포인트는 2가지로 말할 수 있다.\nsubsquence 문제는 DP 방법을 떠올려 볼 것. 만약 DP로 접근하기로 결정하고, 2차원 배열을 두고 DP문제로 가정했을 때, bottom-up 방식이라면 2차원 배열 DP에서는 먼저 아래 그림과 같이 생각할 줄 알아야 한다. 위 2가지 포인트를 가지고 있다면 좀 더 쉽게 문제를 풀 수 있을 것이다.\nclass Solution: def longestPalindromicSubsequence(self, s: str, k: int) -\u0026gt; int: def cost(c1, c2): dist = abs(ord(c1) - ord(c2)) return min(dist, 26-dist) # k + 1 because we store 0 to k n = len(s) dp = [[[0 for _ in range(k+1)] for _ in range(n)] for _ in range(n)] for i in range(n): for kk in range(k+1): # single character is palindrome dp[i][i][kk] = 1 for i in range(n-1, -1, -1): for j in range(i+1, n): # print(\u0026#34;i, j\u0026#34;, i, j) for kk in range(k+1): if s[i] == s[j]: dp[i][j][kk] = dp[i+1][j-1][kk] + 2 else: dp[i][j][kk] = max(dp[i+1][j][kk], dp[i][j-1][kk]) d = cost(s[i], s[j]) if d \u0026lt;= kk: dp[i][j][kk] = max(dp[i][j][kk], dp[i+1][j-1][kk-d] + 2) return dp[0][n-1][k] ","permalink":"https://macsim2.github.io/posts/tech/algorithm/leet/3472/","summary":"leet code WC439에 나온 palindrome 문제이다. 주요 포인트는 2가지로 말할 수 있다. subsquence 문제는 DP 방법을 떠올려 볼 것. 만약 DP로 접근하기로 결정하고, 2차원 배열을 두고 DP문제로 가정했을 때, bottom-up 방식이라면 2차원 배열 DP에서는 먼저 아래 그림과 같이 생각할 줄 알아야 한다. 위 2가지 포인트를 가지고 있다면 좀 더 쉽게 문제를 풀 수 있을","title":"Longest Palindromic Subsequence After at Most K Operations"},{"content":"\u0026lt; Index \u0026gt;\nSound The Nature of Sound: Waves and Vibrations Time Domain vs Frequency Domain Importance of Fourier Transform in Audio Analysis The Nature of Sound: Waves and Vibrations 양자 물리에 따르자면 이 세상 삼라만상은 모두 입자인 동시에 파동이라고 한다.\n정확하게는 입자의 성질과 파동의 성질 둘 다를 가졌으며, 상황에 따라 입자 또는 파동의 성질을 보인다는 것이다.\n입자는 우리가 직관적으로 느끼기에 좀 더 쉽다. 하지만 왠지 파동이라고 하면 파도나 잔잔한 호수의 퍼지는 물의 진동이 먼저 떠오르지 않는가?\n그 보다 우리 주변에서 지금도 느낄 수 있는 파동이 있다. 바로 빛과 소리이다.와이파이도 파동이 맞다 그중 오늘은 소리(sound)에 대해 알아보자.\n소리는 우리 일상에서 가장 기본적인 정보 전달 수단 중 하나이다. 소리라는 현상의 본질은 무엇일까? 물리학적 관점에서 소리는 \u0026lsquo;매질(일반적으로 공기)을 통해 전파되는 압력 파동\u0026rsquo;이고, 이 파동은 물체의 진동에서 시작된다.\n우리가 소리가 듣는 과정을 살펴보면, 아래와 같다.\n물체(예: 기타 현, 스피커 진동판, 성대)가 진동한다.\n이 진동은 주변 공기 분자들을 압축하고 팽창시킨다.\n압축과 팽창의 패턴이 파동의 현상으로써 매질을 통해 전파된다.\n이 파동이 귀에 도달하면 고막을 진동시켜 우리가 소리를 인식하게 됩니다.\n소리는 종파(longitudinal wave)로, 파동의 진행 방향과 매질의 진동 방향이 같다. 시간에 따른 공기에 압력의 변화는 다음과 같은 수식으로 표현할 수 있다.\n$$ p(t) = p_0 + A \\sin(2\\pi f t + \\phi) $$ $p_0$는 주변 대기압, $A$는 파동의 진폭, $f$는 주파수(Hz), $\\phi$는 위상 오프셋\n굳이 위 식을 적는 이유는 이것이 바로 우리가 마이크로 수집하고 디지털로 변환하는 오디오 신호의 수학적 정의 이기 때문이다.\n[그림 필요: 소리의 발생과 전파를 보여주는 그림 - 진동하는 물체, 압축과 팽창의 파동, 그리고 시간에 따른 압력 변화 그래프]\nTime Domain vs Frequency Domain 소리를 분석하는 데는 두 가지 기본적인 관점이 있다.\n시간 도메인(Time Domain): 이것은 우리가 가장 직관적으로 이해하는 방식이다. 시간에 따라 소리의 신호의 크기가 어떻게 변하는지 보여준다. 이는 파형(waveform)으로 표현되며, 소리의 강도와 지속 시간을 직접적으로 확인할 수 있다.\n주파수 도메인(Frequency Domain): 이 관점에서는 소리를 다양한 주파수의 사인파(sine wave)들의 합으로 본다. 주파수 스펙트럼으로 표현되며, 어떤 주파수 성분이 얼마나 강한지 보여줍니다. 주파수는 초당 진동 횟수로, 단위는 Hz(헤르츠)이다. 동일한 소리가 두 도메인에서 완전히 다르게 표현될 수 있다.\n순수한 440Hz 톤(A음)은 시간 도메인에서 매끄러운 사인파로 나타난다.\n같은 톤이 주파수 도메인에서는 440Hz 위치에 하나의 뾰족한 선으로 나타난다.\n복잡한 소리(예: 말소리, 음악)는 시간 도메인에서는 복잡하고 해석하기 어려운 파형이지만, 주파수 도메인에서는 구성 요소들로 명확하게 분해된다.\nImportance of Fourier Transform in Audio Analysis 그렇다면 시간 도메인에서 보는 파형이 어떻게 특정한 주파수 들로 만들어진 것인지 알 수 있는 것일까?\n그 해결책은 바로 그 유명한 푸리에 변환(Fourier Transform) 이라는 것이다.\n푸리에의 업적은 열 전달 현상을 설명하기 위해 사인과 코사인 함수의 무한 급수(지금의 푸리에 급수)를 사용한 것이다. 그가 사용한 이 수학적 기법이 현재의 \u0026lsquo;푸리에 변환\u0026rsquo;으로 발전되어 신호처리, 음향학, 양자역학, 이미지 처리 등 여러 분야에서 핵심적인 도구가 되었다.\n푸리에 변환(Fourier Transform)은 시간 도메인의 신호를 주파수 도메인으로 변환하는 수학적 도구이다.\n이 변환의 핵심 아이디어는 모든 복잡한 파형이 다양한 주파수의 사인파와 코사인파의 합으로 표현될 수 있다는 것이다.\n푸리에 변환은 다음 수식으로 정의된다.\n$$ X(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-j 2\\pi f t} , dt $$ 여기서: $x(t)$는 시간 도메인의 신호, $X(f)$는 주파수 도메인으로 변환된 신호, $e^{-j 2\\pi f t}$는 복소 지수 함수,\n오디오 분석에서 푸리에 변환이 중요한 이유는 다음과 같다.\n인간 청각 시스템의 작동 방식: 우리의 귀는 기본적으로 주파수 분석기이다. 달팽이관(cochlea)의 서로 다른 부분이 서로 다른 주파수에 반응하도록 설계되어 있어, 주파수 도메인 분석은 인간의 청각 인지와 밀접하게 관련된다.\n음향적 특성 파악: 주파수 성분을 분석함으로써 악기의 음색, 화자의 목소리 특성, 환경 소음의 특성 등을 정량적으로 파악할 수 있다.\n효율적인 음성 처리: 음성 인식, 화자 식별, 감정 분석 등 많은 오디오 처리 작업이 주파수 도메인 특성에 기반합니다. MFCC(Mel-Frequency Cepstral Coefficients)와 같은 대표적인 오디오 특징은 푸리에 변환에서 파생된다.\n효과적인 필터링: 특정 주파수 대역을 선택적으로 제거하거나 강화하는 오디오 필터링은 주파수 도메인에서 훨씬 간단하게 구현할 수 있다.\n데이터 압축: MP3와 같은 오디오 압축 형식은 푸리에 변환을 사용하여 인간이 덜 민감한 주파수 성분을 식별하고 제거한다.\n푸리에 변환은 단순한 수학적 도구를 넘어, 오디오 신호의 본질을 이해하고 분석하는 렌즈 역할을 한다. 현대 오디오 처리 기술의 많은 부분이 이 변환의 응용에서 비롯되었으며, 음성 인식이나 음악 정보 검색과 같은 고급 기술들도 대부분 푸리에 변환을 기본 요소로 사용한다.\n[그림 필요: 복잡한 오디오 신호가 다양한 주파수의 사인파로 분해되는 과정을 보여주는 그림 - 원래 신호와 여러 주파수 성분들, 그리고 이들의 합이 원래 신호와 일치함을 시각화]\n다음 장에서는 푸리에 변환의 수학적 기초와 구체적인 형태에 대해 더 자세히 살펴보겠습니다.\n","permalink":"https://macsim2.github.io/posts/tech/signal/sound/","summary":"\u0026lt; Index \u0026gt; Sound The Nature of Sound: Waves and Vibrations Time Domain vs Frequency Domain Importance of Fourier Transform in Audio Analysis The Nature of Sound: Waves and Vibrations 양자 물리에 따르자면 이 세상 삼라만상은 모두 입자인 동시에 파동이라고 한다. 정확하게는 입자의 성질과 파동의 성질 둘 다를 가졌으며, 상황에 따라 입자 또는 파동의 성질을 보인다는 것이다. 입자는 우리가 직관적으로 느끼기에 좀 더 쉽다. 하지만 왠지 파동이라고","title":"Sound"},{"content":"BOJ 9252 문제 LCS(Longest Common Subsequence, 최장 공통 부분 수열)문제는 두 수열이 주어졌을 때, 모두의 부분 수열이 되는 수열 중 가장 긴 것을 찾는 문제이다. 예를 들어, ACAYKP와 CAPCAK의 LCS는 ACAK가 된다.\n입력 첫째 줄과 둘째 줄에 두 문자열이 주어진다. 문자열은 알파벳 대문자로만 이루어져 있으며, 최대 1000글자로 이루어져 있다.\n출력 첫째 줄에 입력으로 주어진 두 문자열의 LCS의 길이를, 둘째 줄에 LCS를 출력한다. LCS가 여러 가지인 경우에는 아무거나 출력하고, LCS의 길이가 0인 경우에는 둘째 줄을 출력하지 않는다.\n풀이 LCS과 다르게 LCS로 정해진 수열의 길이를 출력하는 것이 아닌 아무거나 하나를 구하는 조건이 더해졌다.\n문제를 보고 어떻게 할지 고민을 조금 했는데 더 간단한 풀이방법이 있어서 기록한다. LCS 문제는 2차원 배열을 만들고, 원소의 data type은 int형을 사용하여 LCS 길이를 구할 수 있었다. 그런데 LCS2는 2차원 배열을 사용하되, 원소의 data type를 string 을 사용해 LCS를 직접 구하는 방법을 사용할 수 있다.\n$$\\f\\relax{x} = \\int_{-\\infty}^\\infty \\f\\hat\\xi,e^{2 \\pi i \\xi x} ,d\\xi$$\n$$A=\\begin{bmatrix} 0 \u0026amp; 1 \u0026amp; 2 \\\\ 3 \u0026amp; 4 \u0026amp; 5 \\\\ 6 \u0026amp; 7 \u0026amp; 8 \\end{bmatrix} $$\n$$\\exp_a b = a^b$$ $\\sum\\nolimits_{k=1}^N k^2$\n$$\\begin{equation} f(x) = x^2+4x+16 \\end{equation}$$\n","permalink":"https://macsim2.github.io/posts/tech/algorithm/boj/lcs2/","summary":"BOJ 9252 문제 LCS(Longest Common Subsequence, 최장 공통 부분 수열)문제는 두 수열이 주어졌을 때, 모두의 부분 수열이 되는 수열 중 가장 긴 것을 찾는 문제이다. 예를 들어, ACAYKP와 CAPCAK의 LCS는 ACAK가 된다. 입력 첫째 줄과 둘째 줄에 두 문자열이 주어진다. 문자열은 알파벳 대문자로만 이루어져 있으며, 최대 1000글자로 이루어져 있다","title":"LCS2"},{"content":"","permalink":"https://macsim2.github.io/posts/life/test/","summary":"","title":"Test"},{"content":"AI 모델을 배포해보려 공부해본 엔지니어라면 누구나 이런 고민을 해봤을 것이다. \u0026ldquo;어떻게 하면 내 모델이 프로덕션 환경에서 더 빠르게 동작할 수 있을까?\u0026rdquo; 처음 이런 task를 접했을 때는 단순 C/C++ 프로그래밍을 통해서 해결할 수 있을 줄 알았다. 그러나, 물론 naive한 python 보다야 낫지만, request가 많아질 수록 다른 해결책이 필요하다고 생각이 들 것이다.\n그러던 와중에 나온 NVIDIA의 TensorRT 라는 놈이 있다. 오늘은 모델 최적화와 프로덕션 배포의 세계에서 강력한 도구로 자리잡은 TensorRT와 Triton Inference Server에 대해서 알아보자.\n이 글에서는 단순한 \u0026ldquo;TensorRT 사용법\u0026quot;이나 \u0026ldquo;Triton 서버 설정법\u0026quot;이 아닌, 이 두 기술의 내부 구조, 최적화 원리, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 얘기해보고자 한다.\nTensorRT란? TensorRT는 NVIDIA에서 개발한 고성능 딥러닝 추론 최적화 라이브러리로, 딥러닝 모델을 NVIDIA GPU에서 최대 성능으로 실행할 수 있도록 해준다. 이런 설명을 듣고 생각하면 일종의 CUDA 가속 추론 라이브러리 구나 생각이 들고, 으레 짐작할 수 있지만, TensorRT는 그 이상의 가치를 제공한다.\nPyTorch나 TensorFlow에서 모델을 학습시킬 때, 이 프레임워크들은 유연성과 학습 편의성에 초점을 맞추고 있다. 그러나 프로덕션 환경에서는 순수한 추론 성능과 효율성이 훨씬 중요해진다. 여기서 TensorRT가 등장한다.\nTensorRT의 핵심은 **계산 그래프 최적화(Computational Graph Optimization)**와 **하드웨어 최적화 커널(Hardware-Optimized Kernels)**이다. 여기서 부터 먼가 머리가 아파올 수 있다.. 계산 그래프를 최적화 하고 하드웨어 최적화 커널 이라니..? 무시무시한 최적화 방법인 것 같아 보인다. TensorRT는 모델의 구조를 분석하고 NVIDIA GPU에 최적화된 형태로 변환하여, 때로는 원본 모델보다 5-10배까지 빠른 추론 속도를 달성할 수 있다.\nTensorRT의 마법: 내부 최적화 메커니즘 그렇다면 어떻게 TensorRT가 이렇게 극적인 성능 향상을 가능하게 하는지 조금 더 구체적으로 살펴보자.\n1. 그래프 최적화 기법 TensorRT는 신경망 모델을 최적화하기 위해 여러 그래프 변환 기법을 사용한다:\n레이어 융합(Layer Fusion): 여러 레이어를 단일 최적화된 레이어로 결합 커널 튜닝(Kernel Auto-Tuning): 하드웨어에 맞게 최적의 커널 구현 선택 텐서 메모리 최적화: 메모리 사용량을 최소화하고 캐시 효율성 극대화 불필요한 연산 제거: 추론 시 필요없는 계산 생략 내 경험상 레이어 융합은 가장 극적인 성능 개선을 가져오는 최적화 중 하나였다. 특히 Convolution + BatchNorm + ReLU와 같은 일반적인 패턴은 단일 최적화 커널로 대체되어 메모리 액세스를 크게 줄일 수 있었다. 나중에 OpenAI의 Triton에 대해서도 posting 하려고 하는데 여기서의 메모리 액세스 를 줄인다는게 최적화 과정의 아주 큰 요소이다. 이 메모리 액세스를 어떻게 줄인것인가에 많은 연구자들이 목을 메고 있다.\nNVIDIA의 연구[7]에 따르면, 레이어 융합을 통해 중간 텐서의 메모리 액세스가 50% 이상 감소하고, 이는 대규모 모델에서 최대 25%의 추론 성능 향상으로 이어진다고 한다. Han 등의 연구[6]에서는 메모리 액세스 감소가 모바일 환경에서 에너지 효율성을 최대 3배까지 개선할 수 있음을 보여준다고 한다.\n// TensorRT의 레이어 융합 예시 (의사 코드) // 이런 세 개의 레이어가 있다고 가정해보자 IConvolutionLayer* conv = network-\u0026gt;addConvolution(...); IBatchNormLayer* bn = network-\u0026gt;addBatchNorm(...); IActivationLayer* relu = network-\u0026gt;addActivation(*bn-\u0026gt;getOutput(0), ActivationType::kRELU); // TensorRT는 내부적으로 이를 단일 최적화 커널로 융합한다 // network-\u0026gt;addFusedConvBNRelu(...); (실제 API는 아님) 2. 정밀도 최적화(Precision Optimization) TensorRT의 가장 강력한 기능 중 하나는 **혼합 정밀도 연산(Mixed Precision Computing)**이다. 다양한 수치 정밀도를 지원한다:\nFP32: 32비트 단정밀도 부동 소수점 FP16: 16비트 반정밀도 부동 소수점 INT8: 8비트 정수 양자화 TF32: NVIDIA Ampere GPU부터 지원하는 19비트 텐서 부동 소수점 포맷 실제 프로젝트에서, FP16으로 전환하는 것만으로도 거의 정확도 손실 없이 2배 이상의 성능 향상을 얻을 수 있었다. INT8로 더 나아가면 대략 4배까지 처리속도가 향상될 수 있지만, Quantization 과정에서 정확도 저하가 있기에 trade-off를 따져야 했다.\n# TensorRT의 정밀도 설정 예시 import tensorrt as trt # builder 설정 builder = trt.Builder(TRT_LOGGER) network = builder.create_network(1 \u0026lt;\u0026lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) config = builder.create_builder_config() # FP16 정밀도 활성화 config.set_flag(trt.BuilderFlag.FP16) # INT8 양자화 활성화 (보정 필요) config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = calibrator # 보정기 인스턴스 # 엔진 생성 engine = builder.build_engine(network, config) 3. 동적 텐서 메모리와 실행 최적화 TensorRT는 실행 중에 텐서 메모리 할당을 최적화하기 위해 동적 메모리 관리를 사용한다. 이게 무슨 의미일까? 쉽게 설명하자면, 신경망이 연산을 수행할 때 필요한 메모리를 더 효율적으로 사용하는 방법들을 적용한다는 의미다. 여기에는 세 가지 핵심 전략이 있다.\n메모리 풀링(Memory Pooling) 기존 방식에서는 모델이 연산을 수행할 때마다 메모리를 할당하고 해제하는 과정을 반복했다. 이 과정은 GPU에게 \u0026ldquo;이 크기의 메모리가 필요해\u0026rdquo;, \u0026ldquo;이제 이 메모리는 필요 없어\u0026rdquo; 라고 계속 말하는 것과 같은데, 이는 상당한 오버헤드를 발생시킨다.\n메모리 풀링에서는 TensorRT가 미리 대량의 메모리 풀을 할당해두고, 필요할 때마다 이 풀에서 빠르게 메모리를 가져와 사용한다. 마치 공유 오피스에서 매번 책상을 사고 버리는 대신, 미리 준비된 책상을 필요할 때 사용하는 것과 같다.\n# 기존 방식 (의사 코드) for each_operation in model: memory = allocate_new_memory(size_needed) # 시간 소요 perform_operation_using(memory) free_memory(memory) # 또 시간 소요 # 메모리 풀링 방식 (의사 코드) memory_pool = allocate_large_pool_once() # 초기에 한 번만 비용 지불 for each_operation in model: memory = get_from_pool(memory_pool, size_needed) # 매우 빠름 perform_operation_using(memory) return_to_pool(memory) # 해제가 아닌 반환 (빠름) 텐서 재사용(Tensor Reuse) 딥러닝 모델 내부에서는 많은 중간 결과(텐서)가 생성된다. 이들은 보통 비슷한 크기를 가지는 경우가 많고, 한 번 사용된 후에는 더 이상 필요하지 않은 경우가 많다.\nTensorRT는 똑똑하게도 \u0026ldquo;이 텐서의 데이터는 더 이상 필요 없으니, 이 메모리 공간을 다른 비슷한 크기의 텐서를 위해 재사용할 수 있겠군!\u0026ldquo;이라고 판단한다. 이는 특히 메모리가 제한적인 엣지 디바이스에서 중요하다.\n예를 들어, ResNet과 같은 모델에서는 각 레이어의 출력 텐서가 비슷한 크기를 갖는다. 1번 레이어의 결과가 2번 레이어에 전달된 후에는 1번 레이어의 출력 메모리 공간을 3번 레이어의 출력을 저장하는 데 재사용할 수 있다.\n[초기 상태] Layer 1 출력 → 메모리 A 사용 Layer 2 출력 → 메모리 B 사용 Layer 3 출력 → 새 메모리 필요... [텐서 재사용 적용] Layer 1 출력 → 메모리 A 사용 Layer 2 출력 → 메모리 B 사용 Layer 3 출력 → 메모리 A 재사용 (Layer 1 출력은 이미 Layer 2에 전달됨) 실행 병렬화(Execution Parallelism) 그래프 관점에서 보면, 딥러닝 모델은 일부 연산들이 서로 독립적이라 동시에 실행될 수 있다. TensorRT는 이러한 독립적 연산들을 찾아내 병렬로 실행한다.\n예를 들어, 멀티 헤드 어텐션에서 각 \u0026ldquo;헤드\u0026quot;는 독립적으로 계산될 수 있다. 또한 모델의 여러 브랜치 (예: ResNet의 residual connection과 main path)도 병렬로 처리될 수 있다.\nNVIDIA의 연구[7]에서는 다중 스트림 병렬화가 특히 Transformer 모델에서 효과적임을 보여주었다. 각 어텐션 헤드를 별도의 CUDA 스트림에 할당하는 기법만으로도 약 40%의 처리량 증가를 달성했다고 한다.\n// TensorRT의 작업 스케줄링 최적화 (의사 코드) // 실제 구현은 더 복잡하지만, 개념적으로는 이런 방식 // 독립적인 두 연산 병렬 실행 // A와 B는 서로 의존성이 없다고 가정 (예: 두 개의 독립된 컨볼루션) executeParallel([\u0026amp;]() { executeOperation(operationA); // GPU의 일부 코어에서 실행 }, [\u0026amp;]() { executeOperation(operationB); // 동시에 다른 코어에서 실행 }); // C는 A와 B의 결과에 의존 (예: A와 B의 결과를 합치는 연산) // A와 B가 모두 완료될 때까지 기다린 후 실행 executeOperation(operationC); 이런 최적화 방법을 통해 지연 시간이 감소 되고, 그에 따라 처리량도 자연스레 증가 할 수 있을것이다.\n실전 TensorRT 모델 변환과 최적화 실제 프로젝트에서 TensorRT를 사용할 때 알아야 할 중요한 워크플로우와 실무 팁을 살펴보자.\n1. 모델 변환 파이프라인 TensorRT 모델 변환은 일반적으로 다음 단계를 따른다:\n원본 모델 내보내기: PyTorch/TensorFlow 모델을 ONNX 또는 UFF 포맷으로 변환 TensorRT 엔진 빌드: ONNX/UFF 모델을 TensorRT 엔진으로 최적화 정밀도 선택 및 보정: 필요시 INT8 양자화를 위한 보정 수행 엔진 직렬화(Serialization): 최적화된 엔진을 디스크에 저장하여 로딩 시간 단축 ONNX를 통한 변환이 가장 안정적이고 지원되는 경로였지만, PyTorch 모델에 따라 ONNX 변환 과정에서 여러 난관을 마주치기도 했다.\n# PyTorch에서 TensorRT 변환 예시 import torch import tensorrt as trt # 1. ONNX로 내보내기 torch.onnx.export( model, dummy_input, \u0026#34;model.onnx\u0026#34;, opset_version=13, do_constant_folding=True, input_names=[\u0026#34;input\u0026#34;], output_names=[\u0026#34;output\u0026#34;], dynamic_axes={\u0026#34;input\u0026#34;: {0: \u0026#34;batch_size\u0026#34;}, \u0026#34;output\u0026#34;: {0: \u0026#34;batch_size\u0026#34;}} ) # 2. TensorRT 엔진 생성 TRT_LOGGER = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(TRT_LOGGER) network = builder.create_network(1 \u0026lt;\u0026lt; int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, TRT_LOGGER) with open(\u0026#34;model.onnx\u0026#34;, \u0026#34;rb\u0026#34;) as f: parser.parse(f.read()) config = builder.create_builder_config() config.max_workspace_size = 1 \u0026lt;\u0026lt; 30 # 1GB engine = builder.build_engine(network, config) # 3. 엔진 저장 with open(\u0026#34;model.trt\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(engine.serialize()) 2. 동적 형상(Dynamic Shapes) 처리의 함정 TensorRT 7.0부터 동적 입력 크기가 잘 지원되지만, 최적의 성능을 얻기 위해서는 **최적화 프로필(Optimization Profiles)**을 신중하게 설정해야 한다.\nNLP 모델을 배포했을 때 가변 길이 시퀀스를 효율적으로 처리하는 것이 큰 도전이었다. 예상 시퀀스 길이 범위에 맞게 최적화 프로필을 구성했을 때 가장 좋은 결과를 얻을 수 있었다.\n# 동적 형상을 위한 최적화 프로필 설정 config = builder.create_builder_config() profile = builder.create_optimization_profile() # 최소, 최적, 최대 입력 크기 지정 profile.set_shape( \u0026#34;input\u0026#34;, # 입력 텐서 이름 (1, 3, 224, 224), # 최소 크기 (8, 3, 224, 224), # 최적 크기 (가장 자주 사용될 것으로 예상) (16, 3, 224, 224) # 최대 크기 ) config.add_optimization_profile(profile) 또한 다양한 배치 크기에 최적화된 다중 프로필을 사용하여 성능을 더욱 향상시킬 수 있었다.\n3. INT8 양자화와 보정의 과학 FP16보다 더 극적인 성능 향상을 위해 INT8 양자화를 적용할 수 있지만, 이는 세심한 보정(Calibration) 과정이 필요하다:\n보정 데이터셋 준비: 실제 데이터의 통계적 특성을 대표하는 샘플 준비 보정기(Calibrator) 구현: 다양한 보정 알고리즘 중 선택 (엔트로피, 최소-최대 등) 보정 실행: 보정 데이터를 통해 모델의 활성화 범위 분석 정확도 검증: 양자화 후 정확도 손실 평가 NVIDIA 개발자 블로그와 TVM 프로젝트 연구[8]에 따르면, 양자화 방식 중 특히 엔트로피 보정 방식이 가장 낮은 정보 손실로 양자화를 수행할 수 있어 정확도 측면에서 우수한 결과를 제공한다. 이 방법은 활성화 분포를 고려하여 양자화 스케일을 동적으로 조정하기 때문에 이미지 분류 모델에서 특히 효과적이다.\n# TensorRT INT8 보정 예시 import tensorrt as trt import numpy as np # 보정기 클래스 정의 class EntropyCalibrator(trt.IInt8EntropyCalibrator2): def __init__(self, calibration_data, batch_size, input_name): super().__init__() self.data = calibration_data self.batch_size = batch_size self.input_name = input_name self.current_idx = 0 self.device_input = cuda.mem_alloc(self.data[0].nbytes * self.batch_size) def get_batch_size(self): return self.batch_size def get_batch(self, names): if self.current_idx \u0026gt;= len(self.data): return None batch = self.data[self.current_idx:self.current_idx+self.batch_size] self.current_idx += self.batch_size cuda.memcpy_htod(self.device_input, np.ascontiguousarray(batch)) return [self.device_input] def read_calibration_cache(self): # 이전 보정 캐시 읽기 (있는 경우) return None def write_calibration_cache(self, cache): # 보정 결과 저장 return None # 보정기 사용 calibration_data = [...] # 보정 데이터 준비 calibrator = EntropyCalibrator(calibration_data, batch_size=8, input_name=\u0026#34;input\u0026#34;) config.int8_calibrator = calibrator config.set_flag(trt.BuilderFlag.INT8) 실제로 대규모 모델의 경우, INT8 양자화를 통해 최대 4배의 처리량 향상을 경험했지만, 이는 약간의 정확도 희생을 동반했다. 트레이드오프가 용인되는 비즈니스 시나리오에서만 활용했다.\nNVIDIA Triton Inference Server: 확장 가능한 모델 서빙의 핵심 TensorRT가 단일 모델의 성능을 최적화하는 데 초점을 맞춘다면, Triton Inference Server는 대규모 프로덕션 환경에서의 모델 서빙을 처리한다.\n1. Triton의 아키텍처와 주요 기능 Triton Inference Server는 다양한 프레임워크와 하드웨어에서 추론 서비스를 제공하기 위한 컨테이너화된 솔루션이다. NVIDIA 개발자 컨퍼런스[7]에서 소개된 Triton의 주요 기능은 다음과 같다:\n다중 프레임워크 지원: TensorRT, ONNX Runtime, TensorFlow, PyTorch 등 동적 배치 처리: 요청을 자동으로 배치화하여 처리량 최적화 동시 모델 실행: 여러 모델을 동시에 서빙 모델 앙상블: 복잡한 추론 파이프라인 구성 모델 버전 관리: 무중단 업데이트 지원 하드웨어 리소스 관리: GPU 메모리와 계산 자원 할당 최적화 2. 모델 리포지토리 구성과 최적화 Triton은 모델 리포지토리라는 표준화된 디렉토리 구조를 통해 모델을 관리한다. 이 구조는 각 모델의 다양한 버전과 구성을 체계적으로 관리할 수 있게 해준다.\nmodel_repository/ ├── model1/ │ ├── config.pbtxt # 모델 구성 파일 │ └── 1/ # 버전 1 │ └── model.plan # TensorRT 엔진 ├── model2/ │ ├── config.pbtxt │ ├── 1/ # 버전 1 │ │ └── model.onnx # ONNX 모델 │ └── 2/ # 버전 2 │ └── model.onnx # 업데이트된 ONNX 모델 └── ensemble_model/ ├── config.pbtxt # 앙상블 구성 └── 1/ # 버전 1 3. 동적 배치 처리와 수평적 확장 Triton의 강력한 기능 중 하나는 **동적 배치 처리(Dynamic Batching)**이다. 이는 개별 추론 요청을 자동으로 배치화하여 처리량을 극대화한다.\n# config.pbtxt의 동적 배치 설정 예시 name: \u0026#34;resnet50\u0026#34; platform: \u0026#34;tensorrt_plan\u0026#34; max_batch_size: 128 dynamic_batching { preferred_batch_size: [8, 16, 32, 64] max_queue_delay_microseconds: 5000 } NVIDIA의 연구[7]와 실제 테스트에 따르면, 적절한 배치 크기와 대기 시간 설정은 GPU 활용도를 최대 95%까지 높일 수 있는 것으로 나타났다. 이 설정을 통해 Triton은 최대 5ms 동안 요청을 대기시키면서 선호하는 배치 크기로 그룹화하여 처리한다.\n또한 Kubernetes와 같은 환경에서 Triton 서버를 수평적으로 확장하여 대규모 트래픽을 처리할 수 있었다:\n# Kubernetes 배포 예시 (간략화) apiVersion: apps/v1 kind: Deployment metadata: name: triton-server spec: replicas: 5 # 서버 인스턴스 수 selector: matchLabels: app: triton template: metadata: labels: app: triton spec: containers: - name: triton-server image: nvcr.io/nvidia/tritonserver:22.07-py3 args: [\u0026#34;tritonserver\u0026#34;, \u0026#34;--model-repository=/models\u0026#34;] resources: limits: nvidia.com/gpu: 1 # 인스턴스당 GPU 수 volumeMounts: - name: model-volume mountPath: /models volumes: - name: model-volume persistentVolumeClaim: claimName: model-pvc 실전 TensorRT와 Triton 적용 사례와 도전 실제 프로젝트에서 TensorRT와 Triton을 적용하면서 마주친 다양한 도전과 해결책을 살펴보자.\n1. 복잡한 아키텍처 모델의 변환 도전 최신 AI 연구에서 나온 복잡한 아키텍처의 모델을 TensorRT로 변환하는 과정은 종종 도전적이었다. 특히 트랜스포머 기반 모델의 경우, 동적 형상과 함께 변환하는 것이 까다로웠다.\n예를 들어, BERT 모델을 TensorRT로 최적화할 때는 다음과 같은 접근 방식이 도움이 되었다:\n플러그인 활용: NVIDIA의 TensorRT 플러그인 사용 (예: Multi-Head Attention) 퓨전 허용 패턴: 특정 연산 시퀀스를 TensorRT가 최적화할 수 있도록 모델 구조 조정 최적의 ONNX opset 버전 선택: 모델 구조에 맞는 적절한 ONNX opset 사용 # BERT용 TensorRT 최적화 설정 예시 config = builder.create_builder_config() config.max_workspace_size = 4 \u0026lt;\u0026lt; 30 # 4GB # 플러그인 활용 plugin_registry = trt.get_plugin_registry() bert_plugin_creator = plugin_registry.get_plugin_creator(\u0026#34;CustomBertPlugin\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;\u0026#34;) plugin = bert_plugin_creator.create_plugin(\u0026#34;bert_plugin\u0026#34;, plugin_params) # 네트워크에 플러그인 레이어 추가 plugin_layer = network.add_plugin_v2([...], plugin) 2. 추론 지연 시간 vs 처리량 최적화 실시간 추론이 필요한 애플리케이션(예: 자율주행)과 대량 배치 처리가 필요한 애플리케이션(예: 오프라인 영상 분석) 사이에는 상충관계가 있다.\n지연 시간 최적화를 위해서는:\n작은 배치 크기 사용 스트림 병렬성 최소화 정확한 워크로드에 맞는 프로필 최적화 처리량 최적화를 위해서는:\n큰 배치 크기 활용 다중 CUDA 스트림으로 병렬 처리 메모리 사용량과 계산 효율성 균형 서비스의 SLA(Service Level Agreement)에 따라 이 두 목표 사이의 적절한 균형점을 찾아야 했다.\n# Triton에서 지연 시간 vs 처리량 최적화 설정 예시 instance_group { count: 2 # GPU당 모델 인스턴스 수 kind: KIND_GPU gpus: [0] } # 지연 시간 최적화 dynamic_batching { max_queue_delay_microseconds: 100 # 매우 짧은 대기 시간 } # 처리량 최적화 dynamic_batching { preferred_batch_size: [64, 128] max_queue_delay_microseconds: 5000 # 더 긴 대기 시간 허용 } 3. 메모리 최적화와 다중 모델 배포 대규모 멀티 모델 시스템에서는 GPU 메모리 관리가 중요한 도전 과제였다. Triton에서는 다음과 같은 전략을 사용했다:\n인스턴스 그룹 최적화: GPU당 적절한 모델 인스턴스 수 설정 모델 로드 정책: 필요시에만 메모리에 로드하는 정책 사용 우선순위 스케줄링: 중요한 모델에 리소스 우선 할당 # config.pbtxt의 메모리 최적화 설정 instance_group { count: 1 kind: KIND_GPU gpus: [0] } # 메모리 효율적인 로드 정책 model_transaction_policy { decoupled: false } dynamic_batching { ... } # 필요시에만 모델 로드 model_control_mode: EXPLICIT 특히 여러 모델을 동시에 서빙해야 하는 경우, 모델별 리소스 할당을 세심하게 조정하여 전체 시스템 성능을 최적화할 수 있었다.\n실전 디버깅과 성능 분석 도구 TensorRT와 Triton을 효과적으로 활용하려면 강력한 디버깅 및 성능 분석 도구가 필수적이다.\n1. NVIDIA Nsight Systems NVIDIA Nsight Systems는 GPU 작업의 타임라인을 시각화하여 병목 현상을 식별하는 데 유용하다. 특히 호스트와 디바이스 간의 동기화 이슈나 커널 실행 패턴을 분석하는 데 탁월했다.\n# Nsight Systems로 Triton 서버 프로파일링 nsys profile -t cuda,nvtx -o profile_report --capture-range=cudaProfilerApi \\ tritonserver --model-repository=/models 2. TensorRT trtexec 도구 trtexec는 TensorRT 엔진을 벤치마킹하고 분석하기 위한 명령줄 도구로, 다양한 설정에서 모델 성능을 빠르게 평가할 수 있다.\n# trtexec을 사용한 성능 벤치마킹 trtexec --onnx=model.onnx \\ --saveEngine=model.trt \\ --fp16 \\ --verbose \\ --shapes=input:8x3x224x224 \\ --iterations=100 \\ --avgRuns=10 이 도구를 통해 다양한 배치 크기, 정밀도 및 최적화 설정에 따른 성능 차이를 체계적으로 측정할 수 있었다.\n3. Triton 모델 분석기 Triton은 배포된 모델의 성능을 분석하기 위한 perf_analyzer 도구를 제공한다:\n# Triton 모델 성능 분석 perf_analyzer -m resnet50 \\ -u localhost:8000 \\ -i grpc \\ --concurrency-range 1:16 \\ --shape input:3,224,224 이 도구를 통해 동시성 수준에 따른 지연 시간과 처리량을 측정하고, 최적의 서버 구성을 결정할 수 있었다.\nTensorRT와 Triton의 현실적인 한계와 대안 TensorRT와 Triton은 강력하지만, 몇 가지 제한 사항도 존재한다:\n1. 모델 호환성 도전 모든 모델이 TensorRT로 완벽하게 변환되지는 않는다. 특히:\n커스텀 연산자: 표준 연산자가 아닌 경우 플러그인 개발 필요 동적 제어 흐름: 조건문과 루프가 복잡한 모델의 경우 변환 어려움 최신 아키텍처: 최신 연구 모델은 지원이 지연될 수 있음 이런 경우의 대안으로, 복잡한 부분만 PyTorch로 유지하고 나머지를 TensorRT로 최적화하는 하이브리드 접근법을 사용했다.\n2. 개발 편의성 vs 최적화 수준 TensorRT는 학습 프레임워크에 비해 개발 편의성이 떨어진다. 모델 최적화에 소요되는 엔지니어링 시간과 얻을 수 있는 성능 향상 사이의 균형을 고려해야 한다.\n간단한 경우, PyTorch 2.0의 torch.compile()과 같은 기능이 적절한 대안이 될 수 있었다.\n3. 멀티 클라우드 및 이종 환경 지원 NVIDIA GPU에 최적화된 TensorRT는 다른 하드웨어 환경에서는 활용할 수 없다. 멀티 클라우드 전략이나 이종 하드웨어 환경을 고려한다면, Apache TVM이나 ONNX Runtime과 같은 대안을 함께 검토해야 했다.\n결론 TensorRT와 Triton Inference Server는 딥러닝 모델의 프로덕션 배포에 있어 강력한 도구이다. 적절한 최적화와 구성을 통해 극적인 성능 향상과 비용 절감을 달성할 수 있지만, 이는 깊은 이해와 체계적인 접근을 필요로 한다. 결국은 많이 사용해보고 공부해야 적절한 최적화를 감당할 수 있을 것이라는 생각이 들었다.\n앞으로도 이런 최적화 방법론들은 계속 발전할 것이며, 엔지니어로서 이러한 도구의 내부 작동 방식을 이해하는 것은 효과적인 AI 시스템 구축에 있어 도움이 되지 않을까 싶다.\nReferences [1] NVIDIA TensorRT 공식 문서: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html [2] NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server [3] TensorRT 최적화 기술: https://on-demand.gputechconf.com/gtc/2019/presentation/s9431-tensorrt-inference-optimization.pdf [4] Triton 성능 모범 사례: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/performance_tuning.md [5] NVIDIA Deep Learning Examples: https://github.com/NVIDIA/DeepLearningExamples [6] Han, S., Shen, H., Philipose, M., Agarwal, S., Wolman, A., \u0026amp; Krishnamurthy, A. (2016). MCDNN: An approximation-based execution framework for deep stream processing under resource constraints. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services (pp. 123-136). [7] Nvidia. (2022). TensorRT: A platform for deep learning inference. In GTC 2022. [8] Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., \u0026hellip; \u0026amp; Ren, M. (2018). TVM: An automated end-to-end optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (pp. 578-594). ","permalink":"https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/","summary":"\u003cp\u003eAI 모델을 배포해보려 공부해본 엔지니어라면 누구나 이런 고민을 해봤을 것이다. \u003cstrong\u003e\u0026ldquo;어떻게 하면 내 모델이 프로덕션 환경에서 더 빠르게 동작할 수 있을까?\u0026rdquo;\u003c/strong\u003e  \u003cbr\u003e\n처음 이런 task를 접했을 때는 단순 C/C++ 프로그래밍을 통해서 해결할 수 있을 줄 알았다. 그러나, 물론 naive한 python 보다야 낫지만, request가 많아질 수록 다른 해결책이 필요하다고 생각이 들 것이다.\u003c/p\u003e\n\u003cp\u003e그러던 와중에 나온 NVIDIA의 TensorRT 라는 놈이 있다. 오늘은 모델 최적화와 프로덕션 배포의 세계에서 강력한 도구로 자리잡은 TensorRT와 Triton Inference Server에 대해서 알아보자.\u003c/p\u003e\n\u003cp\u003e이 글에서는 단순한 \u0026ldquo;TensorRT 사용법\u0026quot;이나 \u0026ldquo;Triton 서버 설정법\u0026quot;이 아닌, 이 두 기술의 내부 구조, 최적화 원리, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 얘기해보고자 한다.\u003c/p\u003e","title":"TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합"},{"content":"딥러닝 모델을 프로덕션에 배포해본 개발자라면 누구나 이런 의문을 품어봤을 것이다. \u0026ldquo;내 모델이 ONNX로 변환되면 내부적으로 어떻게 표현되고 실행될까?\u0026rdquo; 🤔 모델 최적화와 프로덕션 배포의 세계에서 ONNX(Open Neural Network Exchange)(오닉스라고 읽더라)는 대중적인 도구가 되었지만, 그 내부 구조와 최적화 메커니즘에 대해 깊이 이해하는 것은 어렵기도 하고, 그런 개발자가 많지도 않아보인다.\n이 글에서는 단순히 \u0026ldquo;ONNX 변환 방법\u0026quot;이 아닌, ONNX Graph의 내부 구조, 최적화 과정, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 파헤쳐보려 한다.\nONNX란? ONNX(Open Neural Network Exchange)는 2017년 Facebook과 Microsoft가 공동으로 발표한 오픈 포맷으로, 다양한 머신러닝 프레임워크 간의 모델 교환을 가능하게 하는 표준이다. 하지만 ONNX는 단순한 \u0026ldquo;파일 포맷\u0026rdquo; 이상의 의미를 가진다.\n여러분이 PyTorch나 TensorFlow에서 모델을 만들 때, 그 모델은 해당 프레임워크의 특정 추상화와 인터페이스를 통해 표현된다. 그러나 프로덕션 환경에서는 최적의 성능과 호환성을 위해 이 모델을 \u0026ldquo;중립적인\u0026rdquo; 형태로 표현할 필요가 있다. 여기서 ONNX가 등장한다.\n\u0026ldquo;중립적\u0026quot;이라는 표현도 좀 추상적이긴 하다. 나중에 설명할테니 여기선 \u0026ldquo;표준적\u0026quot;인 의미로 알고 넘어가도 괜찮을거 같다.\nONNX의 핵심은 계산 그래프(Computational Graph) 이다. 이 그래프는 모델의 연산과 데이터 흐름을 노드와 엣지로 표현한다. 각 노드는 특정 연산(Conv, MatMul, Add 등)을 나타내며, 엣지는 텐서 데이터의 흐름을 나타낸다.\nONNX Graph의 해부학 ONNX 모델 파일을 열어보면, 그 내부에는 복잡한 계산 그래프가 Proto 버퍼 형식으로 저장되어 있다. 이 그래프의 기본 구성 요소를 살펴보자:\n1. 노드(Node)와 연산자(Operator) ONNX 그래프의 핵심 구성 요소는 노드다. 각 노드는 다음과 같은 속성을 가진다:\nop_type: 노드가 수행하는 연산의 유형 (예: Conv, MatMul, Relu) inputs: 입력 텐서의 이름 목록 outputs: 출력 텐서의 이름 목록 attributes: 연산에 필요한 매개변수 (예: 커널 크기, 스트라이드 등) # 간단한 ONNX 노드 예시 node { input: [\u0026#34;X\u0026#34;, \u0026#34;W\u0026#34;] output: [\u0026#34;Y\u0026#34;] name: \u0026#34;conv1\u0026#34; op_type: \u0026#34;Conv\u0026#34; attribute { name: \u0026#34;kernel_shape\u0026#34; ints: 3, 3 type: INTS } attribute { name: \u0026#34;strides\u0026#34; ints: 1, 1 type: INTS } } ONNX를 사용하며 내가 마주한 사실중에 하나는, 동일한 PyTorch 모델도 ONNX로 변환하는 방식과 옵션에 따라 생성되는 노드의 종류와 수가 크게 달라질 수 있다는 점이었다. 특히 torch.onnx.export() 함수의 opset_version 매개변수는 그래프 구조에 상당한 영향을 미친다.\n2. 텐서(Tensor)와 값(Value) ONNX 그래프에서 노드 간에 흐르는 데이터는 텐서로 표현된다. 이 텐서들은 다음과 같은 속성을 가진다:\n이름: 텐서의 고유 식별자 데이터 타입: 텐서의 요소 타입 (float32, int64 등) 형상(Shape): 텐서의 차원과 크기 특히 주목할 점은 ONNX에서 모델 가중치도 그래프의 일부로 직접 저장된다는 것이다. 이는 PyTorch의 state_dict나 TensorFlow의 체크포인트와는 다른 접근 방식이다.\n# ONNX 모델에서 가중치 텐서 예시 initializer { dims: 64 dims: 3 dims: 3 dims: 3 data_type: FLOAT name: \u0026#34;conv1.weight\u0026#34; raw_data: \u0026#34;...\u0026#34; # 실제 바이너리 데이터는 여기에 저장 } 3. 그래프 입력과 출력 ONNX 그래프는 명시적인 입력과 출력 정의를 가진다:\n# 그래프 입력 예시 input { name: \u0026#34;input\u0026#34; type { tensor_type { elem_type: FLOAT shape { dim { dim_value: 1 # 배치 크기 } dim { dim_value: 3 # 채널 } dim { dim_value: 224 # 높이 } dim { dim_value: 224 # 너비 } } } } } 이 명시적인 입출력 정의는 모델 배포 시 큰 장점이 된다. 모델이 어떤 형태의 입력을 기대하고 어떤 형태의 출력을 생성할지 명확히 알 수 있기 때문이다.\nONNX Graph 생성의 비밀 PyTorch나 TensorFlow 모델을 ONNX로 변환할 때, 단순히 \u0026ldquo;내보내기\u0026rdquo; 버튼을 누르는 것처럼 보일 수 있지만, 내부적으로는 복잡한 변환 과정이 일어난다.\n1. 추적(Tracing)과 스크립팅(Scripting) PyTorch에서 ONNX 변환은 주로 두 가지 방식으로 이루어진다:\n추적(Tracing): 모델에 실제 입력 데이터를 통과시키면서 실행되는 연산을 기록 스크립팅(Scripting): 모델 코드를 분석하여 정적 그래프를 생성 이 두 방식은 각각 장단점이 있는데, 내 경험상 동적 제어 흐름(if문, 루프 등)이 포함된 모델에서는 trace 방식이 문제를 일으키는 경우가 많았다. 특히 입력 크기에 따라 동작이 달라지는 모델에서는 스크립팅 방식이 더 안정적인 결과를 제공했다.\n# 추적 방식 예시 dummy_input = torch.randn(1, 3, 224, 224) torch.onnx.export(model, dummy_input, \u0026#34;model.onnx\u0026#34;, export_params=True, opset_version=12, do_constant_folding=True) # 스크립팅 방식 예시 (TorchScript 이용) scripted_model = torch.jit.script(model) torch.onnx.export(scripted_model, dummy_input, \u0026#34;model_scripted.onnx\u0026#34;) 2. 연산자 매핑의 함정 PyTorch나 TensorFlow의 연산자가 ONNX로 매핑되는 과정에서 많은 함정이 있다. 특히 커스텀 연산자나 최신 연산자의 경우 직접적인 ONNX 대응이 없을 수 있다.\n내가 경험한 가장 까다로운 사례는 attention 메커니즘이 포함된 트랜스포머 모델의 변환이었다. PyTorch의 MultiheadAttention 모듈이 ONNX로 변환될 때, 단일 노드가 아닌 여러 기본 연산자(MatMul, Add, Softmax 등)의 복잡한 조합으로 분해되었다. 이로 인해 그래프가 매우 복잡해지고 최적화 기회가 제한되었다.\n# PyTorch의 MultiheadAttention이 ONNX로 변환되면 # 아래와 같은 여러 노드로 분해된다 (간략화된 버전) node { op_type: \u0026#34;MatMul\u0026#34; input: [\u0026#34;queries\u0026#34;, \u0026#34;key_weights\u0026#34;] output: [\u0026#34;QK\u0026#34;] } node { op_type: \u0026#34;Transpose\u0026#34; input: [\u0026#34;QK\u0026#34;] output: [\u0026#34;QK_transposed\u0026#34;] } node { op_type: \u0026#34;Div\u0026#34; input: [\u0026#34;QK_transposed\u0026#34;, \u0026#34;scaling_factor\u0026#34;] output: [\u0026#34;scaled_QK\u0026#34;] } node { op_type: \u0026#34;Softmax\u0026#34; input: [\u0026#34;scaled_QK\u0026#34;] output: [\u0026#34;attention_weights\u0026#34;] } # ... 등등 ONNX opset 버전에 따라 이러한 패턴이 크게 달라질 수 있으며, 최신 opset에서는 더 효율적인 변환이 이루어지는 경우가 많다. 이는 ONNX 생태계가 지속적으로 발전하고 있음을 보여준다.\nONNX Graph 최적화의 기술 ONNX 모델을 얻은 후에는 추가적인 최적화를 통해 더 나은 성능을 얻을 수 있다. 여기서 ONNX Runtime과 같은 추론 엔진이 핵심 역할을 한다.\n1. 그래프 변환 최적화 ONNX Runtime은 그래프에 다양한 변환을 적용하여 최적화한다:\n상수 폴딩(Constant Folding): 상수 입력만 가지는 노드의 결과를 미리 계산 노드 융합(Node Fusion): 특정 패턴의 노드들을 단일 최적화된 노드로 결합 불필요한 노드 제거: 출력에 영향을 미치지 않는 연산 제거 이중 노드 융합은 가장 큰 성능 향상을 가져오는 최적화 중 하나이다. 예를 들어, Conv+BatchNorm+ReLU 패턴은 단일 융합 노드로 대체될 수 있어 메모리 접근과 연산을 크게 줄일 수 있다.\n# ONNX Runtime의 그래프 최적화 예시 import onnxruntime as ort # 기본 세션 vs 최적화된 세션 sess_options = ort.SessionOptions() sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL optimized_session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;, sess_options) # 최적화 전후 그래프 비교 baseline_session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;, ort.SessionOptions()) print(f\u0026#34;원본 그래프 노드 수: {len(baseline_session._model_meta.custom_metadata_map)}\u0026#34;) print(f\u0026#34;최적화 그래프 노드 수: {len(optimized_session._model_meta.custom_metadata_map)}\u0026#34;) 2. 양자화(Quantization)의 비밀 ONNX 모델의 또 다른 강력한 최적화는 양자화이다. FP32 정밀도의 모델을 INT8이나 심지어 INT4로 변환하여 메모리 사용량과 계산 비용을 크게 줄일 수 있다.\n그러나 양자화에는 함정이 있다. 내 경험에서, 단순히 \u0026ldquo;양자화 버튼\u0026quot;을 누르는 것 같은 사후 훈련 양자화(Post-Training Quantization, PTQ)는 때때로 정확도를 크게 저하시킬 수 있다. 특히 작은 모델이나 희소한 활성화 함수를 가진 모델에서 이 문제가 두드러졌다.\n이런 경우, 양자화 인식 훈련(Quantization-Aware Training, QAT)이나 보정 데이터를 사용한 세심한 PTQ가 더 나은 결과를 제공했다.\n# ONNX 모델의 양자화 예시 (간략화됨) from onnxruntime.quantization import quantize_dynamic # 동적 양자화 quantize_dynamic(\u0026#34;model.onnx\u0026#34;, \u0026#34;model_quantized.onnx\u0026#34;, weight_type=QuantType.QInt8) # 보정 데이터를 사용한 정적 양자화 from onnxruntime.quantization import quantize_static, CalibrationDataReader calibration_data = CalibrationDataReader(...) # 보정 데이터 로더 quantize_static(\u0026#34;model.onnx\u0026#34;, \u0026#34;model_quantized.onnx\u0026#34;, calibration_data) 3. 하드웨어 특화 최적화 ONNX의 또 다른 강점은 다양한 하드웨어 백엔드에 대한 최적화가 가능하다는 점이다. 특히 ONNX Runtime은 CPU, GPU, EdgeTPU, DSP 등 다양한 하드웨어에 대한 실행 제공자(Execution Provider)를 지원한다.\n내 프로젝트에서는 동일한 ONNX 모델을 CPU, CUDA, TensorRT 제공자로 실행했을 때 성능 차이가 극적이었다:\n| 실행 환경 | 추론 시간 (ms) | |--------------------------|---------------| | CPU (OpenMP) | 85.2 | | CUDA | 27.3 | | TensorRT (FP16) | 12.1 | | TensorRT (INT8 양자화) | 5.8 | 특히 TensorRT 제공자는 그래프를 더욱 최적화하고 텐서 코어와 같은 하드웨어 가속기를 활용하여 놀라운 성능 향상을 제공했다.\n실전 ONNX Graph 디버깅과 해결책 ONNX 모델을 프로덕션에 배포하면서 몇 가지 까다로운 문제를 마주쳤다. 이러한 경험은 ONNX Graph의 내부 구조를 더 깊이 이해하는 데 도움이 되었다.\n1. 동적 입력 크기의 함정 CNN 기반 모델에서는 입력 크기가 고정되어 있는 경우가 많지만, NLP나 시계열 모델에서는 가변 길이 입력을 처리해야 하는 경우가 많다. ONNX에서 이를 처리하기 위해 **동적 축(Dynamic Axes)**을 지정할 수 있지만, 이로 인해 특정 최적화가 불가능해지는 부작용이 있었다.\n예를 들어, TensorRT 변환 시 동적 축을 가진 모델은 일부 최적화를 적용할 수 없었고, 특정 차원에 대해 최적화된 프로필을 명시적으로 제공해야 했다.\n# 동적 축을 가진 ONNX 모델 내보내기 dynamic_axes = { \u0026#39;input\u0026#39;: {0: \u0026#39;batch_size\u0026#39;, 2: \u0026#39;seq_length\u0026#39;}, \u0026#39;output\u0026#39;: {0: \u0026#39;batch_size\u0026#39;, 1: \u0026#39;seq_length\u0026#39;} } torch.onnx.export(model, dummy_input, \u0026#34;dynamic_model.onnx\u0026#34;, dynamic_axes=dynamic_axes) # TensorRT에서는 최적화 프로필 지정이 필요 import tensorrt as trt profile = builder.create_optimization_profile() profile.set_shape(\u0026#34;input\u0026#34;, (1, 3, 16), (8, 3, 64), (16, 3, 128)) config.add_optimization_profile(profile) 2. 커스텀 연산자의 도전 표준 연산자만으로는 구현하기 어려운 특별한 로직이 필요한 경우 혹은 성능 최적화가 필수적인 경우 커스텀 ONNX 연산자를 구현할 수 있다. 이는 복잡한 과정이지만, ONNX의 확장성을 보여주는 사례다.\n특수한 비디오 처리 연산이 필요했다고 하면, 이를 위해 C++로 커스텀 ONNX 연산자를 구현하고 ONNX Runtime에 등록할 수 있다.\n// 커스텀 ONNX 연산자 구현 (C++) struct CustomOp { static constexpr const char* OpName = \u0026#34;CustomVideoProcessor\u0026#34;; static Status Compute(OpKernelContext* context) { // 구현 로직 return Status::OK(); } static ONNX_NAMESPACE::OpSchema GetOpSchema() { ONNX_NAMESPACE::OpSchema schema; schema.SetName(OpName); schema.SetDomain(\u0026#34;MyDomain\u0026#34;); schema.SetDoc(\u0026#34;Custom video processing operation\u0026#34;); schema.Input(0, \u0026#34;X\u0026#34;, \u0026#34;Input tensor\u0026#34;, \u0026#34;T\u0026#34;); schema.Output(0, \u0026#34;Y\u0026#34;, \u0026#34;Output tensor\u0026#34;, \u0026#34;T\u0026#34;); schema.TypeConstraint(\u0026#34;T\u0026#34;, {\u0026#34;tensor(float)\u0026#34;}, \u0026#34;Supported types\u0026#34;); return schema; } }; // 연산자 등록 ORT_REGISTER_CUSTOM_OP(CustomOp); 이 접근 방식의 장점은 특수 로직을 효율적으로 구현할 수 있다는 것이지만, 단점은 이식성이 떨어진다는 점이다. 커스텀 연산자를 사용하는 ONNX 모델은 해당 연산자가 등록된 환경에서만 실행할 수 있다.\n3. 메모리 최적화의 미학 메모리 패턴 최적화: 동일한 형상의 텐서를 재사용 외부 메모리 사용: 모델 가중치를 외부 파일로 분리 실행 계획 최적화: 최소 메모리 사용량으로 연산 순서 재배열 특히 엣지 디바이스에 배포할 때, 이러한 최적화가 모델 실행 가능성을 결정짓는 중요한 요소였다.\n# 메모리 최적화된 ONNX Runtime 세션 sess_options = ort.SessionOptions() sess_options.enable_mem_pattern = True sess_options.enable_mem_reuse = True sess_options.add_session_config_entry(\u0026#34;session.save_model_format\u0026#34;, \u0026#34;ORT\u0026#34;) sess_options.optimized_model_filepath = \u0026#34;optimized_model.ort\u0026#34; session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;, sess_options) ONNX Graph 분석 도구의 세계 ONNX 모델을 더 깊이 이해하고 최적화하기 위한 다양한 도구가 있다:\n1. Netron: 그래프 시각화의 황금 표준 Netron은 ONNX 그래프를 시각적으로 탐색할 수 있는 강력한 도구다. 각 노드의 속성, 입출력 텐서 형상, 가중치 분포까지 확인할 수 있어 디버깅에 큰 도움이 된다.\n2. ONNX Runtime 프로파일링 ONNX Runtime은 각 노드의 실행 시간과 메모리 사용량을 세밀하게 프로파일링할 수 있는 도구를 제공한다:\n# ONNX Runtime 프로파일링 예시 sess_options = ort.SessionOptions() sess_options.enable_profiling = True session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;, sess_options) # 모델 실행 session.run(None, {\u0026#34;input\u0026#34;: input_data}) # 프로파일 정보 수집 profile_file = session.end_profiling() with open(profile_file, \u0026#34;r\u0026#34;) as f: profile_data = json.load(f) # 가장 시간이 많이 소요된 노드 출력 sorted_nodes = sorted(profile_data, key=lambda x: x.get(\u0026#34;dur\u0026#34;, 0) if isinstance(x, dict) else 0, reverse=True) for node in sorted_nodes[:10]: if isinstance(node, dict): print(f\u0026#34;Node: {node.get(\u0026#39;name\u0026#39;)}, Type: {node.get(\u0026#39;args\u0026#39;, {}).get(\u0026#39;op_name\u0026#39;)}, Duration: {node.get(\u0026#39;dur\u0026#39;)}us\u0026#34;) 이 프로파일링 정보는 병목 현상을 식별하고 최적화 노력을 집중해야 할 부분을 파악하는 데 필수적이다.\n3. ONNX 모델 검증과 비교 모델 변환과 최적화 과정에서 정확도 손실이 없는지 확인하는 것이 중요하다. ONNX에서는 이를 위한 유틸리티를 제공한다:\n# 원본 모델과 ONNX 모델의 출력 비교 import onnx from onnx import numpy_helper import numpy as np # 원본 PyTorch 모델 실행 with torch.no_grad(): torch_output = model(torch_input).numpy() # ONNX 모델 실행 ort_session = ort.InferenceSession(\u0026#34;model.onnx\u0026#34;) ort_inputs = {ort_session.get_inputs()[0].name: torch_input.numpy()} ort_output = ort_session.run(None, ort_inputs)[0] # 출력 비교 np.testing.assert_allclose(torch_output, ort_output, rtol=1e-3, atol=1e-3) print(\u0026#34;출력이 일치합니다!\u0026#34;) 이러한 검증은 특히 양자화나 그래프 최적화를 적용한 후 확인하는 것이 바람직하다.\n마지막으로 내가 ONNX를 사용하며 얻은 교훈은, ONNX 변환을 단순한 \u0026ldquo;마지막 버튼\u0026quot;이라고 생각하지 않게 됐다는 것이다.\n결국 우리에게 가져다 주는 성능의 이점은 쉽게 생기는 것이 아니고, ONNX graph의 세계에 대해서 감탄하게 되었다.\nONNX Graph의 세계는 깊고 풍부하다. 더 효율적이고 이식성 높은 딥러닝 모델을 향한 여정에서, ONNX를 공부해두면 좋을 것 같다.\nReferences [1] ONNX GitHub Repository: https://github.com/onnx/onnx [2] ONNX Runtime Documentation: https://onnxruntime.ai/ [3] PyTorch to ONNX Tutorial: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html [4] ONNX Operator Specifications: https://github.com/onnx/onnx/blob/master/docs/Operators.md ","permalink":"https://macsim2.github.io/posts/tech/inference_optimzation/onnx_graph/","summary":"\u003cp\u003e딥러닝 모델을 프로덕션에 배포해본 개발자라면 누구나 이런 의문을 품어봤을 것이다. \u003cstrong\u003e\u0026ldquo;내 모델이 ONNX로 변환되면 내부적으로 어떻게 표현되고 실행될까?\u0026rdquo;\u003c/strong\u003e 🤔 \u003cbr\u003e\n모델 최적화와 프로덕션 배포의 세계에서 ONNX(Open Neural Network Exchange)(오닉스라고 읽더라)는 대중적인 도구가 되었지만, 그 내부 구조와 최적화 메커니즘에 대해 깊이 이해하는 것은 어렵기도 하고, 그런 개발자가 많지도 않아보인다.\u003c/p\u003e\n\u003cp\u003e이 글에서는 단순히 \u0026ldquo;ONNX 변환 방법\u0026quot;이 아닌, ONNX Graph의 내부 구조, 최적화 과정, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 파헤쳐보려 한다.\u003c/p\u003e","title":"ONNX Graph의 내부 구조와 최적화 여정"},{"content":"요즘 딥러닝 좀 만져봤다 하는 사람치고 Pre-trained Model 안 써본 사람 없을 거다. 거대 모델들이 이미 학습해놓은 방대한 지식을 쪽쪽 빨아먹는(?) Transfer Learning(전이 학습)은 이제 자연스럽다. 그리고 그 핵심 기술 중 하나가 바로 Fine-tuning(미세 조정)이다.\n작업할 당시 생긴 궁금증이 있다. \u0026ldquo;파인튜닝할 때 모델의 모든 가중치(weight)가 다 업데이트되는 걸까?\u0026rdquo; 아니면 일부만 업데이트되는 걸까? 🤔\n이 궁금증을 해결하기 위해, 음성 인식(ASR) 분야의 대표 주자 중 하나인 wav2vec 2.0 논문[1]을 샅샅이 뒤져보며 파인튜닝의 실체를 파헤쳐 보자!\n파인튜닝? 먼저 파인튜닝이 뭔지 간단히 짚고 넘어가자. 파인튜닝(fine-tuning)은 직역하면 미세 조정이라는 말이다. 풀어서 얘기하면 이미 대규모 데이터셋(Source Dataset)으로 **사전 학습(Pre-training)**된 모델을 가져와서, 우리가 **실제로 풀고 싶은 특정 작업(Target Task)과 관련된 데이터셋(Target Dataset)**으로 추가 학습시키는 과정을 말한다.\n[2]\n비유적으로 설명하자면 영어 원어민에게 한국어 수능 문제를 풀도록 가르치는 것과 비슷하다. 기본적인 언어 능력(사전 학습된 지식)은 이미 갖춰져 있으니, 유치원 과정부터 가르치는 일은 없어도 될거다. 그러니 목표 시험(Target Task)에 맞춰 약간의 요령(추가 학습)만 알려주면 훨씬 빠르고 효과적으로 목표를 달성할 수 있다. d2l.ai[2]의 설명처럼, 사전 학습된 모델 파라미터가 가진 지식이 목표 데이터셋에도 유용할 것이라는 가정하에 진행된다.\nWe assume that these model parameters contain the knowledge learned from the source dataset and this knowledge will also be applicable to the target dataset. - d2l.ai[2]\n자, 그럼 이제 wav2vec 2.0은 파인튜닝을 어떻게 수행하는지 본격적으로 살펴보자.\nwav2vec 2.0 파인튜닝: 선택과 집중의 미학 ✨ wav2vec 2.0은 Self-Supervised Learning (자기 지도 학습) 방식으로 사전 학습된 음성 표현(Speech Representation) 모델이다. 이 모델을 특정 음성 인식 작업(예: 특정 언어 받아쓰기)에 맞게 파인튜닝하는 과정은 논문에 꽤 구체적으로 나와 있다.\n논문의 두 부분을 통해 그 비밀을 엿볼 수 있다.\n1. 간단한 모델 구조\nPre-trained models are fine-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task. - wav2vec 2.0[1]\n사전 학습된 모델(feature encoder + Transformer) 위에 새로운 linear projection 레이어를 랜덤 초기화해서 추가한다고 한다. 이 레이어는 최종적으로 우리가 인식해야 할 단어 집합(vocabulary)의 종류(C classes)만큼의 출력을 내뱉는 역할을 한다. 다음은 wav2vec의 큰 모듈들이다.\nFeature Encoder (f: X -\u0026gt; Z): 입력된 원시 음성 파형(Raw Audio, X)을 받아 잠재 음성 표현(latent speech representations, Z)으로 변환한다. 음성의 저수준 특징(음향 특징 등)을 추출하는 역할이다. Transformer (g: Z -\u0026gt; C): Feature Encoder가 만든 Z를 입력받아 문맥 정보를 고려한 표현(context representations, C)으로 변환한다. 음성 내 시간적 관계나 문맥을 파악하는 역할이다. Linear Projection (추가됨): Transformer 위에 새로 추가된 분류기(Classifier). C를 입력받아 최종 단어(vocabulary) 확률을 출력한다. 2. 파인튜닝 학습 전략 (Experimental Setup Part)\nFor the first 10k updates only the output classifier is trained, after which the Transformer is also updated. The feature encoder is not trained during fine-tuning. - wav2vec 2.0[1]\n바로 여기에 우리의 핵심 궁금증에 대한 답이 있다!\n처음 10,000 업데이트(step) 동안은 새로 추가된 output classifier(Linear Projection)만 학습시킨다. 😮 그 이후에는 Transformer 부분도 같이 학습시킨다. 놀랍게도, Feature Encoder는 파인튜닝 과정에서 전혀 학습시키지 않는다 (frozen). 🧊 결론적으로 wav2vec 2.0의 파인튜닝에서는 모든 가중치가 업데이트되는 것이 아니었다! Feature Encoder는 얼려두고, Classifier와 Transformer만 선택적으로 업데이트하는 전략을 사용한다.\n잠깐! wav2vec 2.0의 사전 학습은 어떻게 하는 걸까? 자, 그런데 한 가지 중요한 부분을 건너뛰었다. 애초에 wav2vec 2.0이 뭐길래 이렇게 파인튜닝할 때 특별한 전략이 필요한 걸까? 먼저 wav2vec 2.0의 핵심인 Self-Supervised Learning (SSL, 자기 지도 학습) 방식부터 알아보자.\n레이블 없이 어떻게 학습한다고? wav2vec 2.0의 가장 인상적인 부분은 음성 인식을 위한 학습을 레이블 없이 진행한다는 것이다. 그냥 날것의 오디오 파일만 있으면 된다! 물론 파인튜닝 단계에서는 레이블이 필요하긴 하지만 학습을 하는데 labled data가 필요하지 않다니 상당히 매력적이다.\n기존의 음성 인식 모델들은 수천 시간의 음성 데이터와 이에 대응하는 텍스트 전사본(Transcription)이 필요했다. 하지만 모든 언어에 대해 이런 데이터를 구하는 것은 매우 어렵고 비용이 많이 든다. 특히 저자원 언어(Low-resource languages)의 경우 더욱 그렇다.\nwav2vec 2.0은 이런 문제를 해결하기 위해 사람이 텍스트 레이블을 제공하지 않아도 스스로 학습할 수 있는 방법을 고안했다.\n마스킹과 예측: 음성 BERT? 한마디로 하자면 wav2vec 2.0의 사전 학습 방식은 텍스트 분야의 BERT와 개념적으로 유사하다. 입력의 일부를 가리고(마스킹), 가려진 부분을 주변 문맥으로부터 예측하는 방식이다.\n구체적인 사전 학습 과정을 자세히 설명해보고자 한다. 조금 길어도 참아주시길 하하..\n1. Feature Extraction: 음성 신호를 의미 있는 표현으로 변환 먼저 원시 오디오 파형(X)을 Feature Encoder를 통해 잠재 표현(Z)으로 변환한다.\nCNN 기반 인코더: 논문에서는 7개의 블록으로 구성된 1D 컨볼루션 네트워크를 사용한다. 첫번째 블록은 스트라이드가 5, 나머지 6개는 2 이므로 5 * (2^6) = 320이다. 다운샘플링 효과: 이러한 구성으로 인해 원본 오디오는 약 320배 정도 다운샘플링된다. 예를 들어, 16kHz로 샘플링된 10초 길이의 오디오(160,000 샘플)는 약 500개의 특징 벡터로 변환되고, 하나의 벡터는 20ms의 오디오 정보를 담는다. 레이어 정규화와 GELU: 각 CNN 블록 후에는 레이어 정규화와 GELU 활성화 함수가 적용된다. 왜 이렇게까지 Feature 를 뽑는 것일까? 내 추측이지만 아마도 전통적 특징 추출 법은 인간의 청각 시스템에 기반했고, 사전에 정의된 필터로 정보 손실의 가능성이 있어서 이지 않을까 싶다. 그러나 당시 facebook 연구진들은 이것마저 신경망에 맡기려고 한 것 같다.\n어쨌든 이렇게 추출된 특징 벡터들(Z)은 원시 파형보다 훨씬 더 의미 있는 표현을 갖게 된다. 예를 들어, 음소(phoneme)와 같은 음성 단위나 음향적 특성(피치, 음색 등)을 포착할 수 있다.\n2. Masking: 예측 과제 생성을 위한 정보 가리기 변환된 특징 벡터들(Z) 중 일정 비율을 무작위로 마스킹한다. 이는 모델이 \u0026ldquo;채워 넣기\u0026rdquo; 과제를 학습하도록 하기 위함이다.\n마스킹 비율: 논문에서는 특징 벡터의 약 ~40%를 마스킹한다. 이 비율은 경험적으로 결정된 값으로, 너무 적으면 과제가 너무 쉬워지고, 너무 많으면 너무 어려워진다. 마스킹 전략: 개별 타임스텝을 독립적으로 마스킹하는 것이 아니라, 연속된 타임스텝 그룹(span)을 마스킹한다. 기본 설정에서는 10개의 타임스텝을 그룹으로 마스킹한다. 이는 음성의 시간적 특성을 고려한 설계다. 마스킹 표현: 마스킹된 부분은 학습 가능한 벡터(마스크 임베딩)로 대체된다. 이 벡터는 \u0026ldquo;이 부분은 마스킹되었다\u0026quot;라는 표시자 역할을 한다. 예를 들어, 특징 벡터 시퀀스 [z₁, z₂, z₃, z₄, z₅, z₆, z₇, z₈, z₉, z₁₀]에서 [z₄, z₅, z₆]을 마스킹한다면, 시퀀스는 [z₁, z₂, z₃, M, M, M, z₇, z₈, z₉, z₁₀](여기서 M은 마스크 임베딩)으로 변환된다.\n3. Quantization: 연속적 표현을 이산적 코드로 변환 원래의 Z(마스킹되기 전)를 이산적인 표현 Q로 양자화한다. 이 과정은 실제로 음성의 의미 있는 단위(예: 음소)를 발견하는 데 중요하다.\n코드북(Codebook) 구성: 논문에서는 G=2개의 코드북을 사용하며, 각 코드북은 V=320개의 항목(entry)을 갖는다. 이는 총 G×V=640개의 가능한 코드 조합을 제공한다. 코드북 시스템은 쉽게 음소 사전이라고 생각하면 된다. 그러니 Q로 양자화 했다는 것은 음성 신호를 정해진 음소 entry에 매핑됐다는 의미이다. 벡터 양자화 과정: 특징 벡터 z에 선형 투영(linear projection)을 적용하여 로짓(logit) 벡터를 얻는다. 이 로짓으로부터 각 코드북에서 가장 가능성 높은 코드를 선택한다(Gumbel-Softmax 사용). 선택된 각 코드에 해당하는 임베딩을 가져와 합산한다. 효과: 이 양자화 과정을 통해 연속적인 음성 특징 공간을 유한한 수의 \u0026lsquo;음소 단위\u0026rsquo;로 매핑할 수 있다. 예를 들어, 특정 음소 \u0026ldquo;아\u0026quot;에 해당하는 특징 벡터들은 비슷한 코드 조합(예: 코드북 1에서 코드 15[실제론 256차원 벡터], 코드북 2에서 코드 42[실제론 256차원 벡터])으로 양자화될 가능성이 높다.\n다음으로 넘어가기 전에 궁금증이 든다. 왜 처음보는 Gumbel-Softmax를 사용했을까?\nGumbel-Softmax ? 양자화 과정인 이산적 과정에서 argmax 연산이 들어가게 된다. 이 argmax는 미분이 불가능하므로 학습이 되지 않는다. 일반 softmax는 미분이 가능하나 확률 분포만 출력할 뿐, 샘플을 뽑는 과정은 포함되어 있지 않다. 양자화에는 실제 이산적 선택(하나의 코드북 선택)이 필요하다. gumbel softmax는 미분 가능성을 유지하면서도 확률 분포에서 샘플을 뽑는 과정이 내장 되어 있다. gumbel noise를 통해 초기 학습 단계에서 다양한 코드북 항목을 탐색 하도록 한다. 4. Context Network: 문맥 정보 통합 마스킹된 특징 벡터들(일부가 마스크 임베딩으로 대체된 Z)을 Transformer 모델에 통과시켜 문맥화된 표현(C)을 얻는다.\nTransformer 구조: 논문에서는 12개 또는 24개의 Transformer 블록을 사용한다. 각 블록은 자기 주의(self-attention) 메커니즘과 피드포워드 네트워크로 구성된다. 상대적 위치 인코딩: 절대적 위치 대신 상대적 위치 인코딩(relative positional encoding)을 사용하여 시퀀스 내 위치 정보를 제공한다. 문맥 통합 과정: Transformer는 각 타임스텝에서 주변 정보(마스킹되지 않은 부분)를 활용하여 마스킹된 부분에 대한 예측을 가능하게 하는 문맥화된 표현을 생성한다. 예를 들어, \u0026ldquo;안녕하_요\u0026quot;에서 마스킹된 부분 \u0026ldquo;_\u0026ldquo;을 예측하기 위해, Transformer는 \u0026ldquo;안녕하\u0026quot;와 \u0026ldquo;요\u0026rdquo; 부분의 정보를 종합하여 \u0026ldquo;세\u0026quot;가 들어갈 가능성이 높다고 추론할 수 있다. transformer가 학습하는 \u0026ldquo;세\u0026quot;는 Q의 결과와 유사하게끔 학습된다.\n5. Contrastive Learning: 비교를 통한 학습 Contrastive Learning이란 \u0026ldquo;이것은 저것과 같다/다르다\u0026quot;를 구별하는 방식으로 학습하는 방법이다.\n몇번 들어본 cross entropy라든지, MSE 같은 loss 함수가 아닌 InfoNCE(Noise-Contrastive Estimation) loss 함수로 학습이 된다. 이 함수는 자기지도학습에서 주로 사용되는 요소로 \u0026ldquo;올바른 쌍과 잘못된 쌍을 구별하는 능력\u0026quot;을 학습하는 방법이라고 생각하면 된다.\n한 마디로 하자면 모델은 실제 양자화 벡터(Q)와 다양한 \u0026ldquo;false\u0026rdquo; 양자화 벡터(negative samples)를 구별하는 법을 학습한다. 좀 더 구체적으로 설명해보겠다.\nInfoNCE의 Loss 함수 핵심 아이디어는 \u0026ldquo;같은 위치의 C와 Q는 가깝게, 다른 위치의 C와 Q는 멀게\u0026rdquo; 이다. InfoNCE의 Loss 함수의 변형으로 $$L_m = -\\log\\left[ \\frac{\\exp(\\text{sim}(c_t, q_t)/\\tau)}{\\exp(\\text{sim}(c_t, q_t)/\\tau) + \\sum \\exp(\\text{sim}(c_t, \\tilde{q})/\\tau)} \\right]$$ 이런 식을 사용하며,\n$L$: 최소화하고자 하는 InfoNCE 손실 값 $c_t$: 시간 스텝 $t$에서의 Transformer 출력 벡터 (문맥 표현) 마스킹된 위치에서 모델이 생성한 표현 차원: Transformer의 은닉층 크기와 동일 (일반적으로 768 또는 1024) $q_t$: 시간 스텝 $t$에서의 실제 양자화 벡터 (정답 타겟) 마스킹되기 전 원래 오디오 부분의 양자화된 표현 차원: 모델의 은닉층 크기와 동일 $\\tilde{q}$: 부정적 샘플들 (negative samples) 다른 시간 위치나 다른 오디오 파일에서 가져온 잘못된 양자화 벡터들 wav2vec 2.0에서는 보통 100개의 부정적 샘플 사용 $\\text{sim}(\\cdot,\\cdot)$: 유사도 함수 일반적으로 코사인 유사도 사용: $\\text{sim}(a,b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}$ 두 벡터가 얼마나 유사한지 -1에서 1 사이 값으로 측정 $\\tau$: 온도 파라미터 (temperature) 일반적으로 wav2vec 2.0에서는 0.1 사용 낮을수록 차이를 더 뚜렷하게 만들고, 높을수록 차이를 부드럽게 만듦 $\\exp(\\cdot)$: 지수 함수 유사도 점수를 항상 양수로 변환하고 차이를 강조 $\\sum$: 모든 부정적 샘플에 대한 합계 분모에서 모든 부정적 샘플들의 점수를 합산 단일 대조 학습 프레임워크 Transformer의 출력 C가 Q를 직접 \u0026ldquo;출력\u0026quot;하도록 학습하는 것이 아니다. 대신, C와 Q 사이의 유사성(similarity)을 최대화하도록 학습되고, 이 과정이 InfoNCE 손실 함수를 통해 이루어진다.\n총 손실 함수 $$ L = L_m + αL_d$$ $L_m$: 주요 대조 손실 (마스크 예측용)\n$L_d$: 코드북 다양성 손실 (보조 목적) L_d = -H(q̄) = Σ_g Σ_v p(g,v) log p(g,v) $α$: 다양성 손실의 가중치\n이렇게 총 loss 함수로 학습이 되며, 부정 샘플링을 100개씩 뽑아서 loss를 계산한다고 한다.\n배치 내 부정 샘플을 뽑거나, 같은 오디오 안에서 다른 타입스텝의 양자화 벡터를 뽑거나 하는 방식을 사용한다고 한다.\n이러한 loss 함수 학습 과정을 통해, 모델은 점차적으로:\n음성의 시간적 패턴과 문맥적 관계를 이해하게 된다. 의미 있는 음향 단위(음소 등)를 스스로 발견하게 된다. 다양한 발화자, 환경, 언어적 변이에 강인한 표현을 학습하게 된다. 그리고 이것이 바로 wav2vec 2.0이 레이블 없이도 강력한 음성 표현을 학습할 수 있는 비결이다!\n파인튜닝 전략과 궁금증! 이제 위에서 설명한 사전 학습 과정과 파인튜닝 전략이 어떻게 연결되는지 이해할 수 있다.\n왜 Feature Encoder는 얼리는 걸까? Feature Encoder는 음성의 가장 기본적인 음향 특징(acoustic features)을 추출하는 역할을 한다. 이 특징은 특정 언어나 작업에 크게 구애받지 않는 범용적인(general) 정보일 가능성이 높다. 따라서 사전 학습된 상태를 유지하는 것이 오히려 다른 작업으로의 전이에 유리할 수 있다. 또한, 이 부분을 얼려두면 파인튜닝 시 업데이트해야 할 파라미터 수가 줄어들어 학습이 더 안정적이고 빨라질 수 있다. Transformer를 미세 조정하는 이유: Transformer는 문맥적 관계를 모델링하는데, 이는 특정 언어의 음소 순서나 단어 패턴 등과 연관될 수 있다. 따라서 목표 작업에 맞게 조정이 필요하다. Classifier를 먼저 학습시키는 이유: 새로 추가된 Classifier는 특정 언어의 단어 집합에 맞게 최적화되어야 하고, 이 과정에서 발생하는 큰 오차가 사전 학습된 표현을 손상시키지 않도록 먼저 안정화시킬 필요가 있다. 왜 처음 10k 스텝 동안은 Classifier만 학습시킬까? 새로 추가된 Classifier는 가중치가 랜덤하게 초기화되어 있다. 만약 처음부터 사전 학습된 Transformer와 함께 학습시키면, 이 랜덤한 가중치에서 발생하는 큰 오차(error)가 Transformer에게 잘못된 방향으로 영향을 줄 수 있다 (catastrophic forgetting 현상과 유사). 따라서 새로운 Classifier를 먼저 어느 정도 안정화시킨 후, 전체 모델을 함께 미세 조정하는 것이 더 안정적인 학습 전략일 수 있다. 그럼 10k라는 숫자는 절대적인 기준일까? 그렇지는 않다. 10k는 해당 논문의 실험 조건에서 찾은 경험적인(empirical) 값일 가능성이 높다. 실제로는 사용하는 데이터셋의 크기, 태스크의 종류, 모델 구조 등에 따라 최적의 스텝 수는 달라질 수 있다. 중요한 것은 **\u0026ldquo;새로운 레이어를 먼저 안정화시킨다\u0026rdquo;**는 전략 그 자체다. 사전 학습된 가중치는 초기값 역할만 할 뿐일까? 단순한 초기값 이상이다. 사전 학습은 모델이 데이터의 유의미한 구조나 특징을 이미 학습한 상태로 만들어준다. fine-tuning은 이 좋은 출발점에서 시작하여 목표 작업에 맞게 살짝 방향만 틀어주는 과정이다. 덕분에 적은 데이터로도 훨씬 빠르고 좋은 성능을 얻을 수 있다. 즉, 좋은 초기값을 제공할 뿐만 아니라, 학습 과정 전체에 긍정적인 영향을 미친다. 결국 wav2vec 2.0의 파인튜닝 전략은 사전 학습된 지식을 최대한 보존하면서 새로운 작업에 효과적으로 적응하기 위한 선택과 집중의 결과라고 볼 수 있다.\n실전에서의 wav2vec 2.0 의미는? wav2vec 2.0이 단순히 학술적 성과로 끝났을까? 그렇지 않다. 실제로 이 모델은 저자원 언어 음성 인식 분야에 혁명을 가져왔다.\n놀라운 성능, 적은 데이터로도! 📊 아래 논문의 실험 결과를 살펴보자.\nLibriSpeech 100h (100시간 레이블 데이터)만으로 파인튜닝했을 때, 기존 지도 학습 방식보다 훨씬 낮은 오류율을 보였다. 무려 10분(!) 분량의 레이블된 데이터만으로도 놀라울 정도로 작동하는 음성 인식기를 만들 수 있었다. 바닥부터 학습해도 이 정도 결과는 안 나온다는 게 논문의 요지다. 53개 언어에 대한 모델(XLSR-53)을 사전 학습하여 언어 간 전이가 가능함을 보여주었다. | 훈련 데이터량 | wav2vec 2.0 | 기존 지도 학습 | |--------------|------------|--------------| | 10분 | 4.8% WER | 분석 불가능 | | 1시간 | 3.5% WER | 17.0% WER | | 10시간 | 2.9% WER | 11.1% WER | | 100시간 | 2.2% WER | 6.5% WER | | 960시간 | 1.9% WER | 2.5% WER | 특히 저자원 상황(10분~10시간)에서의 성능 차이는 정말 극적이다. 이는 현실에서 학습 데이터를 구하기 어려운 많은 언어들에게 실질적인 희망이 되었다.\nwav2vec 2.0의 미래: 후속 모델들과 발전 방향 wav2vec 2.0 이후 음성 자기 지도 학습은 계속 발전하고 있다. 주요 후속 모델들을 간단히 살펴보자:\n1. wav2vec-U: 완전한 비지도 학습의 꿈 wav2vec-U(Unsupervised)는 한 걸음 더 나아가 파인튜닝 단계에서도 레이블이 전혀 필요 없는 완전한 비지도 학습 방식을 제안했다. 음성 데이터와 텍스트 데이터만 있고, 이들의 쌍(pair)은 없어도 모델을 학습시킬 수 있다는 놀라운 접근법이다.\n2. HuBERT: 마스킹에서 클러스터링으로 HuBERT(Hidden-Unit BERT)는 wav2vec 2.0의 양자화 방식 대신 클러스터링 기반 목표를 사용하여 더 안정적인 학습을 가능하게 했다. 또한 사전 학습과 파인튜닝의 차이를 줄여 더 좋은 성능을 얻을 수 있었다. 나중엔 HuBERT에 대해서도 포스팅을 해봐야 겠다.\n3. WavLM: 음향 이벤트까지 고려한 표현 학습 WavLM은 음성뿐만 아니라 다양한 음향 이벤트(배경 소음, 화자 중첩 등)까지 고려한 표현 학습을 제안했다. 이는 실제 환경에서의 강인한 음성 인식을 가능하게 한다.\n4. 다양한 도메인으로의 확장 이러한 자기 지도 학습 방식은 음성 인식(ASR)을 넘어 다양한 영역으로 확장되고 있다:\n음성 감정 인식: 화자의 감정 상태 파악 화자 식별: 누가 말하는지 구분 음성 합성: 자연스러운 음성 생성 다국어 음성 번역: 한 언어에서 다른 언어로 직접 번역 다시 파인튜닝으로: 우리가 배울 수 있는 교훈 지금까지 wav2vec 2.0의 사전 학습 방식, 파인튜닝 전략, 실제 응용, 그리고 후속 연구까지 살펴보았다. 이제 처음 던졌던 질문으로 돌아가보자: 파인튜닝에서 모든 가중치가 업데이트되어야 할까?\nwav2vec 2.0의 사례는 \u0026ldquo;아니오\u0026quot;라고 명확히 답해준다. 오히려 더 중요한 질문은:\n어떤 레이어가 일반적인 지식을 담고 있어 고정해도 될까? 어떤 레이어가 태스크별로 특화된 정보를 학습해야 할까? 학습 과정에서 안정성과 효율성을 위한 최적의 전략은 무엇일까? 이러한 질문들은 wav2vec 2.0에만 국한되지 않고, 최근 급증하고 있는 모든 거대 사전 학습 모델들(GPT, BERT, CLIP 등)의 효과적인 파인튜닝에도 적용될 수 있는 보편적인 고민이다.\n결론: 파인튜닝, 아는 만큼 보인다! fine-tuning 작업을 할 때 모든 가중치가 업데이트될 것이라고 막연히 생각하기 쉽지만, wav2vec 2.0의 사례처럼 실제로는 어떤 레이어를 얼리고, 어떤 레이어를 학습시킬지 선택하는 전략이 매우 중요하다는 것을 알 수 있다.\n어떤 부분을 얼리고 어떤 부분을 학습시킬지는 모델의 구조, 사전 학습 방식, 목표 작업의 특성 등 다양한 요소를 고려하여 결정된다. 단순히 코드를 돌리는 것을 넘어, 이러한 전략적 선택의 이유를 이해하는 것이 모델의 성능을 최대한 끌어올리는 열쇠가 될 것이다.\n앞으로 fine-tuning을 할 때는 \u0026ldquo;이 모델은 어떤 부분을 업데이트하고 있을까?\u0026rdquo; 하고 한 번쯤 더 고민해보는 습관을 들여보는 것은 어떨까?\nReferences [1] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations: https://arxiv.org/abs/2006.11477 [2] Dive into Deep Learning - Fine-Tuning: https://d2l.ai/chapter_computer-vision/fine-tuning.html ","permalink":"https://macsim2.github.io/posts/tech/deeplearning/fine-tune_curious/","summary":"\u003cp\u003e요즘 딥러닝 좀 만져봤다 하는 사람치고 \u003ccode\u003ePre-trained Model\u003c/code\u003e 안 써본 사람 없을 거다. 거대 모델들이 이미 학습해놓은 방대한 지식을 쪽쪽 빨아먹는(?) \u003ccode\u003eTransfer Learning\u003c/code\u003e(전이 학습)은 이제 자연스럽다. 그리고 그 핵심 기술 중 하나가 바로 \u003ccode\u003eFine-tuning\u003c/code\u003e(미세 조정)이다.\u003c/p\u003e\n\u003cp\u003e작업할 당시 생긴 궁금증이 있다. \u003cstrong\u003e\u0026ldquo;파인튜닝할 때 모델의 모든 \u003ccode\u003e가중치(weight)\u003c/code\u003e가 다 업데이트되는 걸까?\u0026rdquo;\u003c/strong\u003e 아니면 일부만 업데이트되는 걸까? 🤔\u003c/p\u003e\n\u003cp\u003e이 궁금증을 해결하기 위해, 음성 인식(ASR) 분야의 대표 주자 중 하나인 \u003ccode\u003ewav2vec 2.0\u003c/code\u003e 논문[1]을 샅샅이 뒤져보며 파인튜닝의 실체를 파헤쳐 보자!\u003c/p\u003e","title":"가중치 업데이트에 관해서 with wav2vec 2.0"},{"content":"ASR(Automatic Speech Recognition) 분야에서 모델 성능을 평가할 때 WER(Word Error Rate), CER(Character Error Rate) 같은 지표를 쓴다. Edit Distance 알고리즘을 이용해서 모델이 내놓은 결과(hypothesis)가 실제 정답과 얼마나 비슷한지 점수를 매기는 방식이다.\n그런데 인식(Recognition), 분류(Classification), 탐지(Detection) 같은 다른 머신러닝 task에서는 Accuracy, Precision, Recall 같은 또 다른 지표들이 자주 등장한다. 얘네들은 이름도 비슷비슷해서 헷갈리기 딱 좋다. 특히 Precision이랑 Recall은 뭐가 다른 건지, TP, FP, FN, TN은 또 뭔지\u0026hellip; @_@\n오늘은 이 용어들의 의미를 명확히 하고, 특히 Binary Classification(이진 분류) 상황에서 얘네들을 어떻게 써먹고 해석해야 하는지, 데이터 분석가의 마음(?)으로 한번 정리해보려 한다.\n혼동 행렬 (Confusion Matrix): 모든 것은 여기서 시작된다 분류 모델의 성능을 이야기할 때 빼놓을 수 없는 것이 바로 Confusion Matrix(혼동 행렬)이다. 이름 한번 거창하지만, 알고 보면 그냥 모델의 예측 결과랑 실제 정답을 표로 정리한 거다. 이 표 하나로 모델이 뭘 잘하고 뭘 못하는지 한눈에 파악할 수 있다.\nBinary Classification에서는 보통 Positive(긍정, 1) / Negative(부정, 0) 두 클래스로 나눈다. 예를 들어 \u0026lsquo;스팸 메일 분류\u0026rsquo;라면 스팸 메일이 Positive, 정상 메일이 Negative가 될 수 있다. 이걸 기준으로 혼동 행렬은 다음과 같이 4가지 경우의 수를 보여준다.\nTP (True Positive): 실제 Positive인 것을 Positive라고 정확히 예측한 경우. (정답!) - \u0026ldquo;이건 스팸이야!\u0026rdquo; 라고 했는데 진짜 스팸. TN (True Negative): 실제 Negative인 것을 Negative라고 정확히 예측한 경우. (정답!) - \u0026ldquo;이건 스팸 아니야!\u0026rdquo; 라고 했는데 진짜 정상 메일. FP (False Positive): 실제 Negative인 것을 Positive라고 잘못 예측한 경우. (1종 오류, Type I Error) - \u0026ldquo;이건 스팸이야!\u0026rdquo; 라고 했는데\u0026hellip; 억울한 정상 메일. FN (False Negative): 실제 Positive인 것을 Negative라고 잘못 예측한 경우. (2종 오류, Type II Error) - \u0026ldquo;이건 스팸 아니야!\u0026rdquo; 라고 했는데\u0026hellip; 놓쳐버린 스팸. 이 네 가지 값(TP, TN, FP, FN)만 알면 지금부터 설명할 모든 지표를 계산할 수 있다. 그야말로 분류 모델 성능 평가의 알파이자 오메가!\nAccuracy (정확도): 가장 직관적, 하지만 함정이? 자, 이제 본격적으로 성능 지표들을 살펴보자. 가장 먼저 만나볼 친구는 Accuracy(정확도)다.\n$$ ACCURACY = \\frac{TP+TN}{TP + TN + FP + FN} = \\frac{맞춘 것}{전체} $$\n수식을 보면 알겠지만, 그냥 전체 예측 건수 중에서 정답을 맞춘 비율이다. TP와 TN을 더해서 전체(TP+TN+FP+FN)로 나눈 값. 100문제 중에 90문제 맞히면 정확도 90%인 것처럼 아주 직관적이다. 뭔가 시험 성적표 받는 느낌이랄까?\nAccuracy = (잘 맞춘 스팸 개수 + 잘 맞춘 정상 메일 개수) / (전체 메일 개수) 여기까지만 보면 Accuracy가 무난하고 괜찮아 보인다. 높으면 무조건 좋은 모델 아닌가? 하지만 세상은 그렇게 단순하지 않다. Accuracy에는 치명적인 단점이 숨어있다. 바로 데이터가 불균형(imbalanced)할 때 성능을 제대로 반영하지 못한다는 점이다.\n예를 들어, 1000개의 메일 중 스팸 메일(Positive)이 단 10개뿐이고 나머지 990개가 정상 메일(Negative)인 데이터셋이 있다고 치자. 이때 모델이 모든 메일을 그냥 \u0026ldquo;정상 메일이야!\u0026quot;(Negative)라고 예측해버리면 어떻게 될까?\nTP = 0 (스팸을 스팸이라 예측 못함) TN = 990 (정상 메일을 정상이라 예측함) FP = 0 (정상 메일을 스팸이라 예측 안함) FN = 10 (스팸을 정상이라 예측함) 이 모델의 Accuracy를 계산해보면?\n$$ Accuracy = \\frac{0 + 990}{0 + 990 + 0 + 10} = \\frac{990}{1000} = 99% $$\n무려 99%의 정확도! 와, 엄청난 모델이다..! 최강의 모델인가 보다! 라고 생각하면 큰일 난다. 이 모델은 스팸 메일을 단 하나도 걸러내지 못하는, 사실상 쓸모없는 모델이다.\n이처럼 데이터 클래스 비율이 한쪽으로 크게 치우쳐 있으면, 다수 클래스만 잘 맞춰도 Accuracy는 높게 나올 수 있다. 그래서 Accuracy만 보고 모델 성능을 판단하는 것은 매우 위험하다.\nPrecision (정밀도) \u0026amp; Recall (재현율): Accuracy의 빈틈을 메우다 Accuracy의 함정을 피하기 위해 등장한 구원투수들이 바로 Precision(정밀도)과 Recall(재현율)이다. 이 둘은 모델의 예측 결과를 좀 더 다른 관점에서 바라본다.\nPrecision (정밀도): \u0026ldquo;내가 Positive라고 한 것 중에 진짜는?\u0026rdquo; $$ PRECISION = \\frac{TP}{TP + FP} = \\frac{진짜 Positive}{Positive라고 예측한 것들} $$\nPrecision은 모델이 Positive라고 예측한 것들 중에서, 실제로 Positive인 것의 비율이다. 즉, 모델의 Positive 예측이 얼마나 정확한지를 나타낸다.\nPrecision = (잘 맞춘 스팸 개수) / (모델이 스팸이라고 예측한 전체 메일 개수) Precision이 높다는 것은 모델이 \u0026ldquo;이건 Positive야!\u0026ldquo;라고 했을 때, 그 예측을 믿을 만하다는 뜻이다. 스팸 메일 분류 예시에서는, 모델이 스팸이라고 분류한 메일들 중 실제 스팸일 확률이 높다는 의미다. FP(정상 메일을 스팸으로 잘못 예측)가 낮을수록 Precision은 높아진다. 괜히 억울하게 스팸함으로 가는 정상 메일을 줄이고 싶을 때 중요한 지표다.\nRecall (재현율): \u0026ldquo;실제 Positive 중에 내가 얼마나 찾아냈지?\u0026rdquo; $$ RECALL = \\frac{TP}{TP + FN} = \\frac{진짜 Positive}{실제 Positive인 것들} $$\nRecall은 실제 Positive인 것들 중에서, 모델이 Positive라고 예측한 것의 비율이다. 즉, 모델이 실제 Positive 샘플을 얼마나 놓치지 않고 잘 찾아내는 지를 나타낸다. Sensitivity(민감도)라고도 불린다.\nRecall = (잘 맞춘 스팸 개수) / (실제 스팸 메일 전체 개수) Recall이 높다는 것은 모델이 실제 Positive 샘플들을 놓치지 않고 잘 잡아낸다는 뜻이다. 스팸 메일 분류 예시에서는, 실제 스팸 메일들을 빠짐없이 잘 걸러내는 능력을 의미한다. FN(스팸을 정상 메일로 잘못 예측)이 낮을수록 Recall은 높아진다. 단 하나의 스팸 메일도 놓치고 싶지 않을 때 중요한 지표다.\n그래서 뭘 써야 하냐고? Task 나름! Accuracy가 항상 능사는 아니라는 것, Precision과 Recall의 계산법과 의미가 다르다는 것은 이제 알겠다. 그래서 어쩌라는 건가..? 어디서 주워듣기로 통계는 해석의 학문이라고 한다. 여기서도 비슷하다. 어떤 지표를 중요하게 볼지는 풀고자 하는 문제(Task)의 특성에 따라 달라진다. 왜냐하면 FP와 FN 중 어떤 실수가 더 치명적인가가 Task마다 다르기 때문이다.\n다시 한번 두 가지 Task를 예로 들어보자.\n스팸 메일 필터링:\nFP (정상 메일을 스팸으로): 중요한 메일을 놓칠 수 있다. (짜증 유발) FN (스팸 메일을 정상으로): 귀찮은 스팸 메일을 받게 된다. (귀찮음 유발) 상대적으로 FP가 더 치명적일 수 있다. 중요한 메일을 놓치는 것보다는 스팸 몇 개 더 받는 게 나을 수도? 이 경우 Precision이 중요해진다. 암 진단 모델:\nFP (정상인을 암 환자로): 환자는 불필요한 검사와 스트레스를 받는다. (비용, 심리적 부담) FN (암 환자를 정상인으로): 치료 시기를 놓쳐 생명이 위험해질 수 있다! (매우 치명적!) 이 경우, FN을 줄이는 것이 압도적으로 중요하다. 즉, 실제 암 환자를 한 명이라도 놓치지 않아야 하므로 Recall이 매우 중요해진다. 차라리 정상인을 암 환자로 오진(FP)하더라도, 실제 환자를 놓치는(FN) 것보다는 낫다고 판단할 수 있다. 이처럼 Task의 맥락에서 어떤 오류(FP or FN)를 더 피하고 싶은지에 따라 Precision과 Recall의 중요도가 달라진다. 모델을 평가할 때는 내가 풀려는 문제가 무엇인지, 어떤 실수가 더 큰 문제를 일으키는지 먼저 고민해야 한다. 데이터가 주어졌을 때, 이 상황을 잘 분석해서 어떤 지표를 중점적으로 볼지 결정하는 것이 중요하다.\nF1-Score: Precision과 Recall, 둘 다 놓치지 않을 거예요! \u0026ldquo;아니, Precision이랑 Recall 둘 다 중요하면 어떡하라고? 하나만 고르기 너무 어려운데?\u0026rdquo; 라는 생각이 들 수 있다. 실제로 많은 경우 Precision과 Recall은 서로 반비례(Trade-off) 관계에 있다. 하나를 높이려고 하면 다른 하나가 낮아지는 경향이 있다. (모델의 예측 임계값(threshold)을 조정해보면 이 관계를 확인할 수 있다.)\n그래서 등장한 것이 F1-Score다. F1-Score는 Precision과 Recall의 조화 평균(Harmonic Mean)을 계산하여, 두 지표를 균형 있게 고려하려는 지표다.\n$$ F1 = 2 \\times \\frac{PRECISION \\times RECALL}{PRECISION + RECALL} $$\n왜 그냥 산술 평균((Precision + Recall) / 2)이 아니라 조화 평균을 쓸까? 조화 평균은 두 값 중 낮은 쪽에 더 큰 가중치를 주는 특징이 있다. 따라서 Precision과 Recall 어느 한쪽이라도 값이 매우 낮으면, F1-Score 역시 낮게 나온다. 즉, F1-Score가 높다는 것은 Precision과 Recall이 모두 어느 정도 높은 수준을 유지하고 있다는 의미로 해석할 수 있다. 둘 중 하나만 극단적으로 높고 다른 하나는 매우 낮은 경우, F1-Score는 낮게 평가하여 불균형을 잡아준다.\nF1-Score는 특히 클래스 불균형이 심하거나, Precision과 Recall 모두 중요한 Task에서 유용하게 사용된다.\n(참고: F-measure는 F1-Score를 일반화한 것으로, β 값을 조절하여 Precision 또는 Recall에 더 가중치를 줄 수 있다. F1-Score는 β=1인 경우로, Precision과 Recall을 동일한 비중으로 고려한다.)\n결론: 상황에 맞는 지표 선택과 종합적인 판단이 핵심 정리해보자.\nAccuracy: 가장 직관적이지만, 데이터 불균형에 취약하다. Precision: Positive 예측의 정확성. FP를 줄이는 것이 중요할 때. (예: 스팸 필터) Recall: 실제 Positive를 놓치지 않는 능력. FN을 줄이는 것이 중요할 때. (예: 암 진단) F1-Score: Precision과 Recall의 조화 평균. 둘 다 중요할 때 균형 있게 평가. 어떤 지표가 절대적으로 좋다고 말할 수는 없다. 가장 중요한 것은 Task의 특성과 목적을 이해하고, 어떤 종류의 오류가 더 치명적인지 판단하여 적절한 평가지표를 선택하거나 여러 지표를 종합적으로 고려하는 것이다. 모델의 성능 숫자에만 매몰되지 말고, 그 숫자가 실제 상황에서 어떤 의미를 가지는지 해석하는 능력이 중요하다 하겠다.\nReferences [1] Wikipedia - Precision and recall: https://en.wikipedia.org/wiki/Precision_and_recall [2] Google Developers - Classification: Accuracy: https://developers.google.com/machine-learning/crash-course/classification/accuracy [3] Scikit-learn Documentation - Precision, recall and F-measures: https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics ","permalink":"https://macsim2.github.io/posts/tech/machinelearning/classification_metric/","summary":"ASR(Automatic Speech Recognition) 분야에서 모델 성능을 평가할 때 WER(Word Error Rate), CER(Character Error Rate) 같은 지표를 쓴다. Edit Distance 알고리즘을 이용해서 모델이 내놓은 결과(hypothesis)가 실제 정답과 얼마나 비슷한지 점수를 매기는 방식이다. 그런데 인식(Recognition), 분류(Classification), 탐지(Detection) 같은 다른","title":"분류 모델 성능 측정? Accuracy만 믿으면 큰일나는 이유 (Precision, Recall, F1)"},{"content":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\nscan总共有这几种命令：scan、sscan、hscan、zscan，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]，cursor表示游标，指查询开始的位置，count默认为10，查询完后会返回下一个开始的游标，当返回0的时候表示所有键查询完了\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; MATCH可以采用模糊匹配找出自己想要查找的键，这里的逻辑是先查出20个，再匹配，而不是先匹配再查询，这里加上count 20是因为默认查出的10个数中可能不能包含所有的相关项，所以把范围扩大到查20个，我这里测试的键总共有15个\n127.0.0.1:6379[2]\u0026gt; scan 0 match mylist* count 20 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; TYPE可以根据具体的结构类型来匹配该类型的键\n127.0.0.1:6379[2]\u0026gt; scan 0 count 20 type list 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;mylist\u0026#34; 2) \u0026#34;mylist2\u0026#34; 3) \u0026#34;mylist1\u0026#34; 3. sscan sscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是集合类型的key\n127.0.0.1:6379[2]\u0026gt; sadd myset1 a b c d (integer) 4 127.0.0.1:6379[2]\u0026gt; smembers myset1 1) \u0026#34;d\u0026#34; 2) \u0026#34;a\u0026#34; 3) \u0026#34;c\u0026#34; 4) \u0026#34;b\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;d\u0026#34; 2) \u0026#34;c\u0026#34; 3) \u0026#34;b\u0026#34; 4) \u0026#34;a\u0026#34; 127.0.0.1:6379[2]\u0026gt; sscan myset1 0 match a 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;a\u0026#34; 4. hscan hscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是哈希类型的key\n127.0.0.1:6379[2]\u0026gt; hset myhset1 kk1 vv1 kk2 vv2 kk3 vv3 (integer) 3 127.0.0.1:6379[2]\u0026gt; hgetall myhset1 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 127.0.0.1:6379[2]\u0026gt; hscan myhset1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;kk1\u0026#34; 2) \u0026#34;vv1\u0026#34; 3) \u0026#34;kk2\u0026#34; 4) \u0026#34;vv2\u0026#34; 5) \u0026#34;kk3\u0026#34; 6) \u0026#34;vv3\u0026#34; 5. zscan zscan key cursor [MATCH pattern] [COUNT count]，sscan的第一个参数总是有序集合类型的key\n127.0.0.1:6379[2]\u0026gt; zadd myzadd1 1 zz1 2 zz2 3 zz3 (integer) 3 127.0.0.1:6379[2]\u0026gt; zrange myzadd1 0 -1 withscores 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; 127.0.0.1:6379[2]\u0026gt; zscan myzadd1 0 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;zz1\u0026#34; 2) \u0026#34;1\u0026#34; 3) \u0026#34;zz2\u0026#34; 4) \u0026#34;2\u0026#34; 5) \u0026#34;zz3\u0026#34; 6) \u0026#34;3\u0026#34; ","permalink":"https://macsim2.github.io/posts/tech/tmp/","summary":"1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找 scan总共有这几种命令：sca","title":"Redis scan命令学习"},{"content":"","permalink":"https://macsim2.github.io/posts/blog/blog/","summary":"","title":"Blog"},{"content":"","permalink":"https://macsim2.github.io/posts/life/life/","summary":"","title":"Life"},{"content":"얼마전 친구와 카레를 먹으러 간날 생긴 일이다. 카레를 먹던 친구는 나에게 물었다.\n카카오톡도 통신과정에서 데이터가 1bit이라도 틀릴 수 있지 않나..?\n모든 경우에서 100%란 없을 텐데 어떻게 데이터 신뢰성을 보장할 수 있나?\n전산에 기본 지식이 있는 친구였기에 더 자세히 설명해주고 싶었지만, 못내 제대로 설명을 해주지 못한 점이 마음에 걸렸다. 게다가 그런 얘기를 들으니 무선 통신이라는 녀석이 더 신기하게 다가오는게 아닌가..! 그래서! 공부겸 정리겸 원리만 간단하게 나마 정리해두려 한다.\n데이터 전송의 기본: 변조와 복조 데이터를 한 곳에서 다른 곳으로, 특히 무선으로 보내려면 몇 가지 단계를 거쳐야 한다. 가장 기본적인 원리는 보내려는 데이터(정보) 를 전송 매체(채널) 에 적합한 신호 형태로 바꾸어 보내고(변조), 받은 쪽에서는 이 신호를 다시 원래 데이터로 복원하는(복조) 것이다.\n1. 전송할 데이터 우리가 보내려는 데이터는 크게 아날로그(연속적인 값, 예: 예전 라디오 음성)와 디지털(이산적인 값, 예: 컴퓨터 파일, 카톡 메시지)로 나눌 수 있다. 현대 통신에서는 대부분의 정보가 디지털 데이터(0과 1의 조합) 형태로 처리된다.\n2. 변조 (Modulation) 디지털 데이터(0과 1)는 그 자체로는 멀리 전송하기 어렵다. 특히 무선 환경에서는 더욱 그렇다. 변조는 이 디지털 데이터를 특정 주파수를 가진 반송파(Carrier Wave)에 실어 보내기 위한 과정이다. 즉, 데이터를 전송 매체(공기, 광케이블 등)의 특성에 맞는 전기적 신호 파형으로 변환하는 것이다.\n위 그림처럼, \u0026lsquo;0\u0026rsquo;과 \u0026lsquo;1\u0026rsquo;이라는 디지털 데이터를 특정 규칙에 따라 아날로그 신호 파형의 진폭(Amplitude), 주파수(Frequency), 또는 위상(Phase)을 변화시키는 방식으로 표현한다. 대표적인 디지털 변조 방식으로는 다음과 같은 것들이 있다.\nASK (Amplitude Shift Keying): 진폭의 크고 작음으로 0과 1을 구분한다. FSK (Frequency Shift Keying): 주파수의 높고 낮음으로 0과 1을 구분한다. PSK (Phase Shift Keying): 위상의 변화(꺾임)로 0과 1을 구분한다. (위 그림 예시) 변조는 데이터를 전송 가능한 형태로 만들 뿐 아니라, 한정된 주파수 자원을 여러 사용자가 효율적으로 나누어 사용할 수 있게(다중 접속) 하는 역할도 한다.\n3. 복조 (Demodulation) 수신 측에서는 변조된 신호를 받아 원래의 디지털 데이터(0과 1)를 복원해야 한다. 이 과정을 복조라고 하며, 기본적으로 변조의 역과정을 수행한다. 수신된 신호의 진폭, 주파수, 위상 변화를 감지하여 원래의 디지털 데이터 스트림을 알아낸다.\n데이터 신뢰성 확보의 핵심: 오류 제어 (Error Control) 변조와 복조만으로는 친구가 질문했던 \u0026lsquo;데이터 신뢰성\u0026rsquo; 문제를 해결할 수 없다. 현실의 통신 채널(특히 무선 채널)은 잡음(Noise), 간섭(Interference), 신호 감쇠(Attenuation) 등 다양한 요인으로 인해 전송 중인 신호에 오류가 발생할 가능성이 항상 존재한다. 1이 0으로 바뀌거나, 0이 1로 바뀌는 등의 오류가 생길 수 있다는 뜻이다.\n이러한 오류를 극복하고 데이터의 정확성을 보장하기 위해 오류 제어 기술이 사용된다. 데이터 전송의 전체 과정을 좀 더 자세히 보면 다음과 같다. (송신 과정 기준)\n원본 데이터 → 소스 코딩 → 채널 코딩 → 변조 → 전송\n(수신 과정은 이 역순으로 진행된다: 수신 → 복조 → 채널 디코딩 → 소스 디코딩 → 복원된 데이터)\n여기서 데이터 신뢰성과 직접적으로 관련된 단계는 채널 코딩(Channel Coding)이다.\n채널 코딩 (Channel Coding) 이란? 채널 코딩은 전송할 원래 데이터에 의도적으로 추가 정보(잉여 비트, Redundancy)를 덧붙이는 과정이다. 이 추가 정보는 수신 측에서 데이터 전송 중 발생한 오류를 검출(Detection)하거나 심지어 정정(Correction)하는 데 사용된다.\n잡음이 많은 환경에서 중요한 내용을 전달할 때, 우리가 같은 말을 여러 번 반복하거나 핵심 단어를 강조해서 말하는 것과 비슷하다고 생각할 수 있다. 채널 코딩은 이런 \u0026lsquo;반복\u0026rsquo;과 \u0026lsquo;강조\u0026rsquo;를 수학적으로 정교하게 설계하여, 오류가 발생하더라도 원래 정보를 복원할 수 있는 가능성을 높이는 기술이다.\n채널 코딩 기법은 크게 오류 검출과 오류 정정으로 나눌 수 있다.\n1. 오류 검출 (Error Detection) 기법 오류 검출은 수신된 데이터에 오류가 발생했는지 \u0026lsquo;알아채는\u0026rsquo; 데 중점을 둔다. 오류가 검출되면, 보통 수신 측은 송신 측에 데이터 재전송을 요청한다 (ARQ: Automatic Repeat reQuest 방식).\n패리티 검사 (Parity Check): 가장 간단한 방법이다. 데이터 블록 끝에 1비트(패리티 비트)를 추가하여, 전체 비트 중 \u0026lsquo;1\u0026rsquo;의 개수가 항상 짝수(짝수 패리티) 또는 홀수(홀수 패리티)가 되도록 만든다. 수신 측에서 \u0026lsquo;1\u0026rsquo;의 개수를 세어 약속된 규칙(짝수/홀수)을 만족하지 않으면 오류가 발생했다고 판단한다. 장점: 구현이 매우 간단하다. 단점: 홀수 개의 비트 오류만 검출할 수 있다 (2개 비트 오류 등 짝수 개 오류는 검출 불가). 오류 정정은 불가능하다. 체크섬 (Checksum): 데이터를 일정 크기의 블록으로 나누고, 각 블록의 값을 더한 후 그 합의 보수(complement) 등을 계산하여 데이터와 함께 보낸다. 수신 측에서 동일한 계산을 수행하여 결과가 일치하는지 확인한다. 패리티보다는 강력하지만, 여전히 일부 오류 패턴을 놓칠 수 있다. CRC (Cyclic Redundancy Check, 순환 중복 검사): 현재 가장 널리 사용되는 강력한 오류 검출 기법 중 하나이다. 전송할 데이터를 특정 다항식(Polynomial)으로 나누어 그 나머지를 계산하고, 이 나머지 값(CRC 코드)을 데이터 뒤에 붙여 전송한다. 수신 측에서 받은 데이터와 CRC 코드를 이용해 동일한 다항식으로 나누어 나머지가 0인지 확인한다. 나머지가 0이 아니면 오류가 발생한 것으로 간주한다. 장점: 하드웨어 구현이 용이하고, 특히 여러 비트가 연속적으로 깨지는 버스트 오류(Burst Error) 검출에 매우 효과적이다. 이더넷(LAN), 와이파이(Wi-Fi), 하드 디스크, 압축 파일(ZIP) 등 다양한 곳에서 사용된다. 2. 오류 정정 (Forward Error Correction, FEC) 기법 오류 정정은 수신 측에서 재전송 요청 없이 스스로 오류를 찾아 수정하는 것을 목표로 한다. 이를 위해 오류 검출 기법보다 더 많은 잉여 비트를 추가한다. 실시간 통신(전화, 화상 회의)이나 재전송이 어려운 환경(우주 통신)에서 필수적이다.\n해밍 코드 (Hamming Code): 비교적 간단한 FEC 코드 중 하나이다. 1비트 오류를 정정하고 2비트 오류를 검출할 수 있다. 메모리(RAM) 오류 검출 등에 사용되기도 했다. 리드-솔로몬 코드 (Reed-Solomon Code): 여러 개의 비트가 묶인 심볼(Symbol) 단위로 오류를 정정하는 강력한 블록 코드이다. 특히 버스트 오류 정정에 효과적이어서 CD, DVD, QR 코드, 디지털 방송 등 다양한 분야에서 핵심적으로 사용된다. 컨볼루션 코드 (Convolutional Code): 데이터 스트림을 처리하며 잉여 비트를 생성하는 방식으로, 비터비 알고리즘(Viterbi Algorithm) 등 효율적인 디코딩 방법과 함께 사용된다. 위성 통신, 이동 통신(GSM, CDMA) 등에 널리 사용되었다. 터보 코드 (Turbo Code) / LDPC 코드 (Low-Density Parity-Check Code): 현대 통신의 핵심 기술로, 이론적 한계(섀넌 한계)에 근접하는 매우 뛰어난 오류 정정 성능을 보여준다. 3G, 4G LTE, 5G 이동 통신, Wi-Fi (802.11n 이상), 디지털 비디오 방송(DVB-S2) 등 고속/고신뢰 통신 시스템에 필수적으로 사용된다. 오류 제어의 트레이드오프: 오류 검출 및 정정 능력을 높이려면 더 많은 잉여 비트를 추가해야 한다. 이는 전체 전송 데이터 양을 늘리고, 실제 데이터 전송률(Throughput)을 감소시키는 결과를 낳는다. 따라서 통신 시스템 설계 시에는 채널 환경, 요구되는 신뢰도 수준, 허용 가능한 전송 속도 등을 고려하여 적절한 오류 제어 기법과 부호화율(원본 데이터 대비 전체 데이터 비율)을 선택하는 것이 중요하다.\n결론: 신뢰성은 그냥 얻어지는 것이 아니다 다시 처음 질문으로 돌아가 보자. 카카오톡 메시지가 과연 1비트의 오류도 없이 전달될 수 있을까? 물리적인 전송 과정만 보면 오류 발생 가능성은 항상 존재한다. 하지만 우리가 일상에서 데이터 전송 오류를 거의 체감하지 못하는 이유는, 바로 앞에서 설명한 다양한 계층의 오류 제어 기술 덕분이다.\n물리 계층(Physical Layer)에서의 채널 코딩(오류 검출 및 정정)뿐만 아니라, 그 상위 계층인 데이터 링크 계층(Data Link Layer)이나 전송 계층(Transport Layer, 예: TCP 프로토콜)에서도 CRC, 체크섬, 재전송 메커니즘 등을 통해 추가적인 오류 검사와 복구를 수행한다.\n이처럼 여러 단계에 걸친 정교한 오류 제어 매커니즘이 있기 때문에, 우리는 잡음 많고 불안정한 통신 환경 속에서도 친구에게 보낸 카톡 메시지나 중요한 파일이 정확하게 전달될 것이라고 신뢰할 수 있는 것이다. 100% 완벽한 전송이란 물리적으로 불가능할지 몰라도, 현실적으로는 거의 100%에 가까운 신뢰도를 달성하도록 설계된 것이 현대의 통신 시스템이다.\n","permalink":"https://macsim2.github.io/posts/life/data-transfer-confidence-explained/","summary":"얼마전 친구와 카레를 먹으러 간날 생긴 일이다. 카레를 먹던 친구는 나에게 물었다. 카카오톡도 통신과정에서 데이터가 1bit이라도 틀릴 수 있지 않나..? 모든 경우에서 100%란 없을 텐데 어떻게 데이터 신뢰성을 보장할 수 있나? 전산에 기본 지식이 있는 친구였기에 더 자세히 설명해주고 싶었지만, 못내 제대로 설명을 해주지 못","title":"데이터 전송, 어떻게 신뢰성을 보장할까?"},{"content":"","permalink":"https://macsim2.github.io/posts/read/read/","summary":"","title":"Read"},{"content":" Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n👉Hugo博客交流群 787018782\n","permalink":"https://macsim2.github.io/links/","summary":"Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内 👉Hugo博客交流群 787018782","title":"🤝友链"},{"content":"Signal Processing Sound [WIP]Fourier Tansform [Sampling Theorem] [Filtering] [Convolution] [DFT and FFT] [라플라스 변환 and Z-변환] Linear Algebra [WIP]Eigenvalues \u0026amp; Eigenvectors \u0026amp; SVD Deep Learning [linear regression] [SGD] [WIP]Momentum [WIP]Backpropagation [WIP]Cross Entropy [BCE loss] [MLE] [Universal Approximation Theorem] [flash attention] [skip-connection] [Convoluition] Machine Learning Linear Regression MLE \u0026amp; Bayesian Series (1/3) - Maximum Likelihood Estimation (MLE) Principle Component Analysis (PCA) AutoEncoder (AE) [WIP]Precision, Recall and F1 Score L1 \u0026amp; L2 Regularization Inference and Optimization [WIP]ONNX [WIP]TensorRT [init]Triton ASR Beam search Viterbi CTC Loss [WIP]RNN-T Loss [WIP]Wav2vec 2.0 TLG HLG? Algorithms BOJ LCS2 용액 summarize papers [init]Mamba Quantum Strokes Immune system ","permalink":"https://macsim2.github.io/archives/","summary":"Signal Processing Sound [WIP]Fourier Tansform [Sampling Theorem] [Filtering] [Convolution] [DFT and FFT] [라플라스 변환 and Z-변환] Linear Algebra [WIP]Eigenvalues \u0026amp; Eigenvectors \u0026amp; SVD Deep Learning [linear regression] [SGD] [WIP]Momentum [WIP]Backpropagation [WIP]Cross Entropy [BCE loss] [MLE] [Universal Approximation Theorem] [flash attention] [skip-connection] [Convoluition] Machine Learning Linear Regression MLE \u0026amp; Bayesian Series (1/3) - Maximum Likelihood Estimation (MLE) Principle Component Analysis (PCA) AutoEncoder (AE) [WIP]Precision, Recall and F1 Score L1 \u0026amp; L2 Regularization Inference and Optimization [WIP]ONNX [WIP]TensorRT [init]Triton ASR Beam search Viterbi CTC Loss [WIP]RNN-T Loss [WIP]Wav2vec 2.0 TLG HLG? Algorithms BOJ LCS2 용액 summarize papers [init]Mamba Quantum Strokes Immune system","title":"archives"},{"content":"이 블로그는 저의 머릿속에 있는 파편화된 지식을 정리하고, 쌓여가는 기술부채를 조금이나마 해소하기 위해 만들어졌습니다.\n특히 기술 포스트는 이미 알려져 있는 내용을 조합하거나 간단히 확인하는 것에 그칠 수 있습니다만, 저를 포함해 누군가에게는 도움이 되었으면 좋겠다는 바람은 가지고 글을 적습니다.\n혹시 제 글의 오류나 저작권 문제가 있는 경우, 댓글로 저에게 알려주시면 감사하겠습니다.\n","permalink":"https://macsim2.github.io/about/","summary":"이 블로그는 저의 머릿속에 있는 파편화된 지식을 정리하고, 쌓여가는 기술부채를 조금이나마 해소하기 위해 만들어졌습니다. 특히 기술 포스트는 이미 알려져 있는 내용을 조합하거나 간단히 확인하는 것에 그칠 수 있습니다만, 저를 포함해 누군가에게는 도움이 되었으면 좋겠다는 바람은 가지고 글을 적습니다. 혹시 제 글의 오류나 저작권","title":"Introduction"},{"content":"웹 개발 좀 해봤다 하는 사람치고 Nginx(엔진엑스라고 읽는다) 이름을 못 들어본 사람은 없을 거다. 마치 웹 서버계의 아이돌 같은 존재랄까? 근데 Nginx가 정확히 뭐 하는 친구고, 왜 이렇게 인기가 많은 걸까? 그리고 항상 같이 언급되는 Apache(아파치)와는 도대체 뭐가 다른 걸까?\n오늘은 웹 서버계의 양대 산맥, Nginx와 Apache를 비교하며 Nginx의 매력을 집중 탐구해보는 시간을 가져보려 한다.\nNginx, 넌 대체 누구니? (만능 재주꾼 소개) Nginx는 단순히 웹 서버 역할만 하는 것이 아니라, 다양한 재주를 가진 만능 엔터테이너다. 주요 역할만 해도 이 정도다:\n웹 서버 (Web Server): 기본적인 역할. HTML, CSS, Javascript, 이미지 파일 같은 정적 콘텐츠(Static Contents)를 클라이언트(웹 브라우저)에게 슝슝 보내준다. 리버스 프록시 (Reverse Proxy): 클라이언트 요청을 직접 처리하지 않고, 뒤에 숨어있는 실제 서버(WAS 등)들에게 전달해주고 응답을 받아 다시 클라이언트에게 전달하는 중간 대리인 역할. 보안 강화, 캐싱, 로드 밸런싱 등 다양한 부가 효과는 덤! 로드 밸런서 (Load Balancer): 여러 대의 서버에게 들어오는 요청(트래픽)을 골고루 나눠주는 교통정리 역할. 특정 서버에 과부하가 걸리는 것을 막아 서비스 안정성을 높인다. HTTP 캐시 (HTTP Cache): 자주 요청되는 콘텐츠를 미리 저장해두었다가 빠르게 응답해주는 똑똑한 창고 역할. 서버 부담을 줄이고 응답 속도를 높인다. 메일 프록시 (Mail Proxy): 이메일 관련 프로토콜(SMTP, POP3, IMAP)을 위한 프록시 기능도 제공한다. (이건 좀 덜 유명하지만\u0026hellip;) 한마디로 웹 서비스를 위한 온갖 궂은일(?)을 도맡아 하는 멀티플레이어라고 할 수 있다.\nApache vs Nginx: 세기의 라이벌, 뭐가 다를까? Nginx 이야기를 할 때 절대 빼놓을 수 없는 존재가 바로 Apache다. 오랫동안 웹 서버 시장의 절대 강자였던 Apache와 신흥 강자 Nginx는 처리 방식에서 근본적인 차이를 보인다. 이게 바로 성능 차이로 이어지는 핵심 포인트다!\nApache: 전통의 강호, \u0026ldquo;한 요청엔 한 일꾼!\u0026rdquo; (프로세스/스레드 기반) Apache는 전통적으로 요청(Request) 하나당 프로세스 또는 스레드 하나를 할당하는 방식으로 동작한다. 마치 손님 한 명당 전담 직원 한 명을 붙여주는 식당 같다고 할까?\n작동 방식: 클라이언트 요청이 들어오면 새로운 프로세스나 스레드를 생성해서 처리한다. (MPM 방식에 따라 조금씩 다르지만 기본 원리는 비슷) Prefork MPM: 요청마다 프로세스를 복제. 안정적이지만 메모리 소모가 크다. Worker MPM: 여러 스레드가 요청을 나눠 처리. 메모리 효율성은 좋지만 스레드 간 동기화 문제가 있을 수 있다. Event MPM: Worker 방식에 비동기 처리 개념을 더함. (Nginx와 비슷해지려는 노력!) 장점: 오랜 역사만큼 다양한 기능과 모듈(Module)을 지원한다. 호환성이 좋다. 기능 추가나 설정 변경이 비교적 쉽다. 요청 처리가 완료될 때까지 연결을 유지하는 방식에 적합하다. 단점: 동시 접속자 수가 많아지면 성능이 급격히 저하된다. 요청마다 프로세스/스레드를 생성하는 비용(메모리, CPU)이 만만치 않기 때문이다. 소위 C10K 문제(동시 접속 1만 개 처리 문제)에 취약하다. Nginx: 떠오르는 신성, \u0026ldquo;혼자서도 열 요청 거뜬!\u0026rdquo; (이벤트 기반 비동기) Nginx는 Apache와 전혀 다른 접근 방식을 택했다. 바로 Event-Driven(이벤트 기반) 아키텍처다. 마치 혼자서 여러 테이블 손님 주문을 동시에 능숙하게 받는 베테랑 직원 같다고 할까?\n작동 방식: 미리 정해진 **적은 수의 프로세스(Worker Process)**만 생성한다. 각 프로세스는 하나의 스레드로 동작하며, **여러 개의 연결(Connection)**을 동시에 처리한다. 실제 작업(I/O 등)은 비동기 논블로킹(Asynchronous Non-blocking) 방식으로 처리한다. 즉, 작업이 완료될 때까지 마냥 기다리는 게 아니라, 작업 요청만 해놓고 바로 다른 이벤트를 처리하러 간다. 작업이 완료되면 그때 알려달라고(Callback) 하는 식이다. 장점: 매우 적은 자원(메모리, CPU)으로 높은 동시 접속 처리 성능을 보여준다. C10K 문제 해결사! 비동기 방식으로 동작하여 I/O 작업이 많은 웹 환경에 최적화되어 있다. 정적 파일 처리 속도가 매우 빠르다. 단점: Apache만큼 다양한 모듈을 지원하지는 않는다. (물론 핵심 기능은 충분!) 동적으로 기능을 추가하거나 설정을 변경하는 것이 Apache보다 조금 더 복잡할 수 있다. 비동기 논블로킹 I/O, 그게 뭔데? Nginx 성능의 핵심인 Event-Driven과 비동기 논블로킹 I/O를 좀 더 쉽게 이해해보자. Node.js도 이 방식을 사용해서 유명해졌다.\n이벤트 발생 (Event Emit): 클라이언트 요청(Request), 파일 읽기 완료 등 다양한 이벤트가 발생한다. 마치 식당에 손님이 들어오거나, 주방에서 요리가 완성되는 것과 같다.\n이벤트 루프 (Event Loop) 출동: 이벤트 루프는 끊임없이 \u0026ldquo;뭔 일 없나?\u0026rdquo; 하고 지켜보다가 이벤트가 발생하면 재빨리 감지한다. 그리고 이 이벤트를 처리할 담당자(Handler)에게 작업을 넘긴다.\n논블로킹 (Non-blocking) 방식의 작업 요청: 만약 처리해야 할 작업이 시간이 오래 걸리는 I/O 작업(파일 읽기/쓰기, 네트워크 통신, 데이터베이스 조회 등)이라면 더욱 Nginx의 진가가 드러날 거다.\n기다리지 않는다! (Non-blocking): Nginx(담당 Handler)는 운영체제에게 \u0026ldquo;이 파일 좀 읽어줘!\u0026rdquo; 또는 \u0026ldquo;저 서버에 데이터 좀 보내줘!\u0026rdquo; 라고 요청만 하고, 그 작업이 끝날 때까지 기다리지 않는다. 마치 주방에 \u0026ldquo;파스타 하나요!\u0026rdquo; 라고 주문만 넣고 바로 다른 테이블 주문 받으러 가는 웨이터처럼 이것이 바로 논블로킹이다. 즉, 함수(작업 요청)를 호출했을 때, 작업 완료 여부와 상관없이 즉시 반환되어 다음 코드를 실행할 수 있게 된다. (vs 블로킹 (Blocking)): 만약 블로킹 방식이었다면, 파일 읽기가 다 끝나거나 데이터 전송이 완료될 때까지 그 자리에서 하염없이 기다려야 한다. 그동안 다른 손님(요청)은 아무도 못 받는 거다. 제어권 반환 \u0026amp; 다른 이벤트 처리: 논블로킹 방식으로 I/O 작업을 요청한 Nginx(담당 Handler)는 프로그램의 실행 흐름(Control, 제어권)을 즉시 이벤트 루프에게 돌려준다. \u0026ldquo;나는 요청했으니, 이제 다른 일 할게!\u0026rdquo; 라는 뜻이다.\n제어권이란? 프로그램 코드가 순차적으로 실행되는 \u0026lsquo;흐름\u0026rsquo;을 의미한다. 싱글 스레드 환경에서는 이 실행 흐름이 하나뿐이다. 블로킹 작업은 이 실행 흐름을 멈추고 기다리게 만들지만, 논블로킹 작업은 실행 흐름을 멈추지 않고 계속 이어갈 수 있게 해준다. 제어권을 돌려받은 이벤트 루프는 쉬지 않고 다음 대기 중인 다른 이벤트가 있는지 확인하고 처리한다. 덕분에 하나의 스레드만으로도 여러 요청을 동시에 처리하는 것처럼 보이는 것이 가능하다. 비동기 (Asynchronous) 방식의 결과 처리: 아까 요청했던 I/O 작업이 드디어 완료되면 어떻게 될까?\n알아서 알려준다! (Asynchronous): 운영체제가 \u0026ldquo;아까 시킨 파일 다 읽었어!\u0026rdquo; 라고 이벤트 루프에게 알려준다. Nginx가 계속 \u0026ldquo;다 됐니?\u0026rdquo; 하고 물어볼 필요가 없다. 이것이 바로 비동기 방식이다. 작업 완료 시점을 Nginx가 신경 쓰지 않고, 완료되면 나중에 통보받는 구조다. 콜백 실행 (Callback): 작업 완료 알림을 받은 이벤트 루프는 해당 작업과 연결된 콜백 함수를 실행 대기열(Task Queue)에 넣는다. 그리고 이벤트 루프는 적절한 시점에 이 콜백 함수를 실행시켜 작업 결과를 처리하거나 다음 작업을 진행한다. \u0026ldquo;파스타 나왔습니다~\u0026rdquo; 하고 알려주면, 웨이터가 서빙하는 것과 같다. 결론: 논블로킹은 작업을 기다리지 않고 바로 다음 일을 할 수 있게 해주고, 비동기는 작업 완료를 나중에 통보받아 처리하는 방식이다. 이 둘의 조합 덕분에 Nginx는 CPU가 직접 일하지 않는 I/O 대기 시간 동안 다른 요청들을 효율적으로 처리할 수 있고, 적은 스레드로도 수많은 동시 요청을 감당할 수 있게 되는 것이다. Apache처럼 요청마다 일꾼(프로세스/스레드)을 늘릴 필요가 없으니 자원도 아끼고 성능도 좋아진다.\n(참고로 CPU 자원을 많이 사용하는 계산 위주의 작업(CPU-bound)은 이런 비동기 방식의 이점을 크게 누리기 어렵다. Nginx는 웹 서버 특성상 네트워크 I/O가 대부분이므로 이 방식이 매우 효과적이다.)\n블로킹? 논블로킹? 동기? 비동기? 헷갈리는 개념 정리! 앞서 논블로킹-비동기 조합이 Nginx의 핵심이라고 했는데, 여기서 잠깐! 블로킹이니 동기니 하는 용어들이 머릿속을 뒤죽박죽 만들 수 있다. 특히 논블로킹과 비동기가 찰떡궁합처럼 같이 쓰이다 보니 더더욱 비슷하게 느껴지기도 한다.\n하지만 이 용어들은 서로 다른 관점에서 나온 개념이다. 이 기회에 네 가지 조합이 어떻게 다른지, 그리고 왜 특정 조합이 잘 안 쓰이는지 식당 웨이터 비유를 통해 확실히 정리하고 가자!\n블로킹 (Blocking) vs 논블로킹 (Non-blocking): 이 둘은 함수(작업)를 호출한 쪽(호출자, Caller)의 관점이다.\n블로킹: 호출한 함수가 작업을 끝낼 때까지 호출자가 그 자리에서 기다리는 것 (실행 흐름 멈춤). 논블로킹: 호출한 함수가 작업 완료 여부와 관계없이 일단 바로 리턴되는 것 (실행 흐름 안 멈춤). \u0026ldquo;접수 완료!\u0026ldquo;하고 바로 다음 일 하러 감. 동기 (Synchronous) vs 비동기 (Asynchronous): 이 둘은 호출된 함수(작업)의 결과나 완료를 어떻게 신경 쓰고 통지받는지에 대한 관점이다.\n동기: 호출자가 스스로 작업 완료 여부를 계속 확인하거나, 작업이 끝나면 그 순서에 맞춰 결과가 바로 리턴되는 방식. 나가 직접 챙겨야 함. 비동기: 호출자는 작업 완료를 신경 끄고, 작업이 끝나면 다른 누군가(OS, 이벤트 루프 등)가 알아서 콜백이나 이벤트로 알려주는 방식. 나중에 통보받음. 자, 이제 웨이터 등판! 네 가지 조합을 살펴보자.\n1. 블로킹 (Blocking) - 동기 (Synchronous): (가장 흔하고 직관적)\n동작: 웨이터가 주문(함수 호출)을 받고 주방(호출된 함수)에 전달. 웨이터는 주방 앞에서 요리 나올 때까지 멍때리며 기다림 (블로킹). 요리 완성! 웨이터가 직접 받아서(동기적 결과 반환) 서빙. 설명: 함수 호출하면 그 함수 작업(특히 I/O) 끝날 때까지 스레드가 멈춘다. 작업 끝나면 결과 리턴되고 멈췄던 흐름이 다시 이어진다. 장단점: 코드 짜기 쉽고 이해도 쉽다. 하지만 스레드가 노는 시간이 많아 비효율적 (특히 손님 많을 때). 2. 논블로킹 (Non-blocking) - 동기 (Synchronous):\n동작: 웨이터가 주문 전달. 주방에 \u0026ldquo;다 됐어요?\u0026rdquo; 바로 물어봄 (논블로킹 호출). 아직이면 일단 다른 테이블 잠깐 봄. 근데 요리 나올 때까지 계속 주방 가서 \u0026ldquo;다 됐어요??\u0026rdquo; 들락날락 확인해야 함 (동기적 확인). 요리 완성! 그때 받아옴. 설명: 함수 호출하면 일단 바로 리턴 (예: \u0026ldquo;아직이요~\u0026rdquo;). 근데 호출자가 결과 받으려면 계속 함수 다시 호출해서 \u0026ldquo;다 됐니?\u0026rdquo; 물어봐야 함 (Polling). 실행 흐름이 멈추진 않지만 계속 물어보느라 바쁨. 장단점: 스레드가 멈추진 않지만, 계속 상태 확인(Polling)하느라 CPU 자원 낭비. 코드도 복잡해짐. 3. 비동기 (Asynchronous) - 블로킹 (Blocking): (이론상으론 가능\u0026hellip;?)\n동작: 웨이터가 주문 넣고 \u0026ldquo;다 되면 알려주세요 (비동기 알림 요청)\u0026rdquo;. 근데 다른 일 안 하고 식당 입구에서 알림 벨만 뚫어져라 쳐다보며 기다림 (블로킹). 사람의 일로 비유하니 이게 뭐지 싶다.. 설명: 일반적으론 잘 없다. 함수 호출(이벤트 등록)은 바로 리턴될 수 있는데, 그 결과(이벤트 통지)를 기다리는 다른 특정 지점에서 블로킹될 수 있다 (select, poll 등). 작업 완료는 비동기로 통지되지만, 그걸 기다리는 과정에서 블로킹이 끼어드는 애매한 상황. \u0026lsquo;비동기\u0026rsquo;의 장점인 \u0026lsquo;기다리지 않음\u0026rsquo;을 활용 못 함. 장단점: 비동기 장점 상실. 복잡한데 효율은 별로라 거의 안 씀. 4. 논블로킹 (Non-blocking) - 비동기 (Asynchronous): (Nginx, Node.js 최애 방식)\n동작: 웨이터가 주문 넣고 \u0026ldquo;다 되면 알려주세요 (비동기 알림 요청)\u0026rdquo;. 그리고 바로 다른 테이블 주문받으러 감 (논블로킹). 나중에 주방에서 \u0026ldquo;딩동! 요리 완료!\u0026rdquo; 알림(콜백) 오면, 그때 가서 요리 받아 서빙. 설명: 함수 호출하면 일단 바로 리턴 (논블로킹). 호출자는 작업 완료 신경 끄고 다른 일 함. 작업 완료되면 나중에 OS나 이벤트 루프가 알아서 콜백 함수 호출 등을 통해 알려줌 (비동기). 장단점: 스레드 대기 시간 최소화! 자원 효율 끝판왕. 동시 요청 처리에 최적. 단점은 콜백 지옥 가능성 (Promise, async/await 등으로 극복 시도 중). 이제 논블로킹-비동기 조합이 왜 Nginx의 빠른 성능 비결인지 감이 좀 더 올 것이다!\n그래서 뭘 써야 할까? (정답은 없다!) \u0026ldquo;와, Nginx 짱인데? 그럼 무조건 Nginx 쓰면 되는 거 아냐?\u0026rdquo; 라고 생각할 수 있지만, 세상만사 그렇듯 정답은 없다.\nNginx가 빛을 발하는 경우:\n높은 동시 접속 처리가 필요한 서비스 (대규모 트래픽 예상) 정적 파일 서빙이 많은 경우 (이미지, CSS, JS 등) 리버스 프록시, 로드 밸런서 역할이 중요한 경우 제한된 서버 자원으로 최대한의 성능을 뽑아내야 할 때 Apache가 여전히 매력적인 경우:\n다양한 서드파티 모듈을 활용해야 하는 복잡한 기능 구현 .htaccess 파일을 통한 유연한 설정 변경이 필요한 경우 (웹 호스팅 등) 개발 편의성이나 기존 시스템과의 호환성이 더 중요할 때 동시 접속자 수가 많지 않고 안정성이 더 중요할 때 물론 요즘 대세는 Nginx를 리버스 프록시로 앞단에 두고, 실제 애플리케이션 로직은 Apache나 다른 WAS(Tomcat, Node.js 등)가 처리하는 조합이다. 각자의 장점을 살리는 현명한 방법이다.\n대세는 Nginx? (점유율 변화) 예전에는 Apache가 웹 서버 시장을 압도했지만, Netcraft 같은 웹 서버 조사 기관 자료를 보면 최근 몇 년간 Nginx의 점유율이 가파르게 상승하여 Apache를 앞지르거나 팽팽한 경쟁을 벌이고 있다. (2020년 데이터는 조금 오래되었으니 최신 자료를 찾아보는 것도 좋다!) 이는 대규모 트래픽 처리 능력과 효율성이 중요한 현대 웹 환경의 요구와 Nginx의 특징이 잘 맞아떨어졌기 때문으로 보인다.\n결론: 상황에 맞는 현명한 선택이 중요! Nginx는 Event-Driven 비동기 방식으로 동작하여 적은 자원으로 높은 동시 접속 처리 성능을 내는 매력적인 웹 서버다. 특히 리버스 프록시, 로드 밸런서 등으로 활약하며 현대 웹 서비스 아키텍처의 핵심 요소로 자리 잡았다.\n하지만 Apache 역시 오랜 역사와 풍부한 기능, 높은 호환성이라는 장점을 가지고 있다. 결국 어떤 기술이든 만능은 없다(No silver bullet!). 내가 만들려는 서비스의 특징, 예상되는 트래픽 규모, 서버 자원, 개발 편의성 등을 종합적으로 고려하여 상황에 맞는 최적의 도구를 선택하는 것이 중요하다 하겠다.\nReferences [1] Nginx 공식 사이트: https://nginx.org/ [2] Apache HTTP Server Project: https://httpd.apache.org/ [3] Apache vs Nginx (DigitalOcean): https://www.digitalocean.com/community/tutorials/apache-vs-nginx-practical-considerations [4] C10k problem (Wikipedia): https://en.wikipedia.org/wiki/C10k_problem [5] (기존 참고) 네이버 블로그 - Node.js 이벤트 기반 비동기 처리 방식: https://blog.naver.com/jhc9639/221108496101 (개념 이해에 도움) ","permalink":"https://macsim2.github.io/posts/tech/dev/web/nginx/","summary":"\u003cp\u003e웹 개발 좀 해봤다 하는 사람치고 \u003ccode\u003eNginx\u003c/code\u003e(엔진엑스라고 읽는다) 이름을 못 들어본 사람은 없을 거다. 마치 웹 서버계의 아이돌 같은 존재랄까? 근데 Nginx가 정확히 뭐 하는 친구고, 왜 이렇게 인기가 많은 걸까? 그리고 항상 같이 언급되는 \u003ccode\u003eApache\u003c/code\u003e(아파치)와는 도대체 뭐가 다른 걸까?\u003c/p\u003e\n\u003cp\u003e오늘은 웹 서버계의 양대 산맥, Nginx와 Apache를 비교하며 Nginx의 매력을 집중 탐구해보는 시간을 가져보려 한다.\u003c/p\u003e","title":"Nginx와 blocking, sync"},{"content":"요청은 ASR(Automatic Speech Recognition, 자동 음성 인식)에서 사용되는 CTC Loss와 RNN-T Loss에 대한 자세한 설명을 제공하는 것입니다. 이 두 손실 함수는 음성 인식 시스템에서 널리 사용되며, 각각 다른 특성과 장단점을 가지고 있습니다.\n계획:\nASR 개요 및 시퀀스 모델링 문제 설명 CTC Loss 설명 수학적 정의 작동 원리 장단점 예제 RNN-T Loss 설명 수학적 정의 작동 원리 CTC와의 차이점 장단점 예제 두 손실 함수의 비교 실제 구현 및 사용 사례 이 응답에서는 두 손실 함수의 수학적 정의, 작동 원리, 장단점, 구현 방식 등을 설명하고, 가능한 경우 코드 예제와 시각적 설명을 포함하겠습니다.# ASR에서의 CTC Loss와 RNN-T Loss 상세 설명\n음성 인식(ASR)에서 가장 중요한 두 손실 함수인 CTC(Connectionist Temporal Classification)와 RNN-T(Recurrent Neural Network Transducer)에 대해 자세히 설명하겠습니다.\n1. ASR의 기본 문제와 도전 과제 ASR의 핵심 문제:\n가변 길이 입력(음성 신호)을 가변 길이 출력(텍스트)으로 변환 입력과 출력 사이의 명확한 정렬(alignment)이 없음 입력이 출력보다 훨씬 더 길고 중복됨 (예: 30ms 프레임마다 특징 추출) 2. CTC Loss (Connectionist Temporal Classification) 2.1 기본 개념 CTC는 명시적인 정렬 없이 시퀀스 데이터를 학습하기 위한 알고리즘입니다.\n핵심 아이디어: 모든 가능한 정렬을 고려하고, 올바른 출력을 생성하는 모든 정렬의 확률 합을 최대화 특수 기호 blank(보통 \u0026lsquo;-\u0026rsquo; 또는 \u0026lsquo;ε\u0026rsquo;로 표기)를 도입하여 반복 문자 처리와 정렬 유연성 확보 2.2 수학적 정의 입력 시퀀스 $X = (x_1, x_2, \u0026hellip;, x_T)$와 출력 레이블 $Y = (y_1, y_2, \u0026hellip;, y_U)$가 있을 때:\n모델은 각 타임스텝 $t$마다 모든 가능한 레이블 $k$ (레이블 집합 + blank)에 대한 확률 $P(k|t)$ 출력 경로(path) $\\pi = (\\pi_1, \\pi_2, \u0026hellip;, \\pi_T)$는 각 타임스텝마다의 레이블 예측 경로에서 레이블 시퀀스로의 매핑 함수 $\\mathcal{B}$를 정의: blank 제거 및 반복 문자 병합 CTC Loss는 다음과 같이 정의됩니다:\n$L_{CTC} = -\\log P(Y|X) = -\\log \\sum_{\\pi \\in \\mathcal{B}^{-1}(Y)} \\prod_{t=1}^{T} P(\\pi_t|t)$\n여기서 $\\mathcal{B}^{-1}(Y)$는 $Y$로 매핑되는 모든 가능한 경로의 집합입니다.\n2.3 계산 과정 (Forward-Backward 알고리즘) 직접적인 계산은 조합 폭발 문제가 있어 동적 프로그래밍을 활용합니다:\nForward 변수 $\\alpha(t,s)$: 시간 $t$까지 레이블의 $s$번째 위치까지 생성할 확률 Backward 변수 $\\beta(t,s)$: 시간 $t$부터 끝까지 레이블의 $s$번째 위치부터 끝까지 생성할 확률 최종 확률: $P(Y|X) = \\sum_{s} \\alpha(T,s) = \\sum_{s} \\beta(1,s)$\n2.4 CTC의 장단점 장점:\n정렬이 필요 없음 (end-to-end 학습 가능) 계산 효율성이 상대적으로 높음 구현이 비교적 간단함 단점:\n조건부 독립 가정: 각 타임스텝의 출력이 이전 출력과 독립적 언어 모델링 능력 부족 빈 출력(전체가 blank) 문제 가능성 3. RNN-T Loss (RNN Transducer) 3.1 기본 개념 RNN-T는 CTC의 한계를 극복하기 위해 개발된 알고리즘입니다.\n핵심 아이디어: 음향 모델(인코더)과 예측 네트워크(언어 모델)의 결합 이전 출력에 조건부로 현재 출력을 생성 (자기회귀 특성) 3.2 모델 구조 RNN-T는 세 가지 주요 컴포넌트로 구성됩니다:\n인코더(Encoder): 음향 특징을 처리하는 RNN (시간 $t$에 대한 함수) 예측 네트워크(Prediction Network): 이전 출력을 처리하는 RNN (출력 인덱스 $u$에 대한 함수) 조인트 네트워크(Joint Network): 인코더와 예측 네트워크의 출력을 결합 3.3 수학적 정의 RNN-T는 CTC와 유사하게 모든 가능한 정렬을 고려하지만, 조건부 확률이 다릅니다:\n$L_{RNN-T} = -\\log P(Y|X) = -\\log \\sum_{\\pi \\in \\mathcal{A}(Y)} P(\\pi|X)$\n여기서 $\\mathcal{A}(Y)$는 $Y$로 매핑되는 모든 유효한 정렬의 집합입니다.\n경로 확률은 다음과 같이 계산됩니다:\n$P(\\pi|X) = \\prod_{i=1}^{|\\pi|} P(\\pi_i|X, y_{0:u-1})$\n여기서 $y_{0:u-1}$는 현재까지 생성된 출력 시퀀스입니다.\n3.4 계산 과정 RNN-T도 Forward-Backward 알고리즘을 사용하지만, 격자(grid)에서 동작합니다:\nForward 변수 $\\alpha(t,u)$: 시간 $t$까지 $u$개의 레이블을 생성할 확률 Backward 변수 $\\beta(t,u)$: 시간 $t$부터 끝까지 나머지 레이블을 생성할 확률 각 셀에서 두 가지 전이가 가능합니다:\nblank 출력 (수평 이동) 레이블 출력 (대각선 이동) 3.5 RNN-T의 장단점 장점:\n자기회귀 특성으로 이전 출력을 고려 (언어 모델링 가능) CTC보다 일반적으로 더 나은 성능 스트리밍 인식에 적합 단점:\n계산 복잡도가 높음 훈련이 더 어려움 추론 시 더 많은 계산 필요 4. CTC와 RNN-T 비교 4.1 핵심 차이점 특성 CTC RNN-T 조건부 독립성 각 타임스텝 독립적 이전 출력에 조건부 언어 모델링 제한적 내장 언어 모델링 계산 복잡도 낮음 높음 스트리밍 가능하나 제한적 자연스럽게 지원 빈 출력 문제 발생 가능 덜 발생 4.2 구체적인 예시 \u0026ldquo;HELLO\u0026quot;라는 단어를 인식하는 경우:\nCTC:\n가능한 경로: \u0026ldquo;H-EE-L-L-O\u0026rdquo;, \u0026ldquo;HH-E-LLL\u0026ndash;O\u0026rdquo;, 등 모든 가능한 경로의 합 계산 RNN-T:\n각 스텝에서 이전 출력을 고려 \u0026ldquo;H\u0026rdquo; 다음에는 \u0026ldquo;E\u0026quot;가 나올 확률이 높다는 언어 모델 정보 활용 5. 구현 예제 (PyTorch 기반) 5.1 CTC Loss 구현 사용 예제 import torch import torch.nn as nn # 로그 소프트맥스 출력을 가진 모델 (배치 크기 2, 시퀀스 길이 50, 클래스 수 28) log_probs = torch.randn(2, 50, 28).log_softmax(2).detach().requires_grad_() # 타겟 시퀀스 (배치 크기 2, 각각 다른 길이) targets = torch.tensor([[1, 3, 5], [2, 4, 6, 8]], dtype=torch.long) # 입력 길이와 타겟 길이 input_lengths = torch.tensor([50, 50], dtype=torch.long) target_lengths = torch.tensor([3, 4], dtype=torch.long) # CTC 손실 계산 ctc_loss = nn.CTCLoss() loss = ctc_loss(log_probs, targets, input_lengths, target_lengths) print(f\u0026#34;CTC Loss: {loss.item()}\u0026#34;) 5.2 RNN-T 구현 개념 (단순화) class RNNTModel(nn.Module): def __init__(self, input_dim, hidden_dim, vocab_size): super().__init__() # 인코더 (음향 모델) self.encoder = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True) # 예측 네트워크 (언어 모델) self.prediction = nn.LSTM(hidden_dim, hidden_dim, batch_first=True) self.embedding = nn.Embedding(vocab_size, hidden_dim) # 조인트 네트워크 self.joint = nn.Sequential( nn.Linear(2*hidden_dim + hidden_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, vocab_size) ) def forward(self, x, y_prev): # 인코더 출력 enc_out, _ = self.encoder(x) # 예측 네트워크 출력 y_embed = self.embedding(y_prev) pred_out, _ = self.prediction(y_embed) # 두 출력 확장 및 조합 enc_out = enc_out.unsqueeze(2) # [B, T, 1, H] pred_out = pred_out.unsqueeze(1) # [B, 1, U, H] # 조인트 네트워크에 피드 joint_out = self.joint( torch.cat([enc_out.expand(-1, -1, pred_out.size(2), -1), pred_out.expand(-1, enc_out.size(1), -1, -1)], dim=-1) ) # [B, T, U, V] return joint_out 6. 실제 활용 사례 Google: RNN-T를 사용하여 디바이스 상의 실시간 음성 인식 구현 Mozilla DeepSpeech: CTC 기반 오픈소스 음성 인식 시스템 Hybrid 시스템: CTC와 RNN-T의 장점을 결합한 시스템도 연구 중 두 손실 함수 모두 ASR 분야에서 중요한 발전을 이끌었으며, 각각의 특성에 따라 다양한 응용 분야에서 활용되고 있습니다.\n","permalink":"https://macsim2.github.io/posts/tech/asr/ctc_loss/","summary":"요청은 ASR(Automatic Speech Recognition, 자동 음성 인식)에서 사용되는 CTC Loss와 RNN-T Loss에 대한 자세한 설명을 제공하는 것입니다. 이 두 손실 함수는 음성 인식 시스템에서 널리 사용되며, 각각 다른 특성과 장단점을 가지고 있습니다. 계획: ASR 개요 및 시퀀스 모델링 문제 설명 CTC Loss 설명 수학적 정의 작동 원리 장단점 예제 RNN-T Loss 설명 수학적 정의 작동 원리 CT","title":""}]