<!DOCTYPE html>
<html lang="ko" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 | macsim&#39;s Blog</title>
<meta name="keywords" content="deeplearning, ASR, speech recognition, CTC, RNN-T, sequence modeling">
<meta name="description" content="자동 음성 인식에서 핵심적인 CTC Loss와 RNN-T Loss의 내부 메커니즘, 수학적 유도, 최적화 기법 및 실전 적용 경험에 대한 심도 있는 기술적 분석">
<meta name="author" content="macsim">
<link rel="canonical" href="https://macsim2.github.io/posts/tech/asr/ctc_loss/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://macsim2.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://macsim2.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://macsim2.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://macsim2.github.io/img/Q.gif">
<link rel="mask-icon" href="https://macsim2.github.io/img/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="ko" href="https://macsim2.github.io/posts/tech/asr/ctc_loss/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  

<meta property="og:title" content="CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리" />
<meta property="og:description" content="자동 음성 인식에서 핵심적인 CTC Loss와 RNN-T Loss의 내부 메커니즘, 수학적 유도, 최적화 기법 및 실전 적용 경험에 대한 심도 있는 기술적 분석" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://macsim2.github.io/posts/tech/asr/ctc_loss/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-08-10T10:30:00+09:00" />
<meta property="article:modified_time" content="2024-08-10T10:30:00+09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리"/>
<meta name="twitter:description" content="자동 음성 인식에서 핵심적인 CTC Loss와 RNN-T Loss의 내부 메커니즘, 수학적 유도, 최적화 기법 및 실전 적용 경험에 대한 심도 있는 기술적 분석"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "posts",
          "item": "https://macsim2.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "👨🏻‍💻 tech",
          "item": "https://macsim2.github.io/posts/tech/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리",
      "item": "https://macsim2.github.io/posts/tech/asr/ctc_loss/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리",
  "name": "CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리",
  "description": "자동 음성 인식에서 핵심적인 CTC Loss와 RNN-T Loss의 내부 메커니즘, 수학적 유도, 최적화 기법 및 실전 적용 경험에 대한 심도 있는 기술적 분석",
  "keywords": [
    "deeplearning", "ASR", "speech recognition", "CTC", "RNN-T", "sequence modeling"
  ],
  "articleBody": "“CTC Loss와 RNN-T Loss의 수학적 기반과 구현 최적화 기법은 무엇인가?” 음성 인식(ASR) 분야에서 이 두 손실 함수는 핵심적인 역할을 하지만, 그 내부 동작 원리와 수학적 기반을 완전히 이해하는 것은 쉽지 않다.\n이 글에서는 단순히 “CTC와 RNN-T 사용법\"이 아닌, 두 손실 함수의 수학적 유도 과정, 알고리즘 동작 원리, 그리고 실제 프로젝트에서의 최적화 기법과 구현 세부사항에 대해 심도 있게 파헤쳐보려 한다.\nASR의 수학적 정형화와 도전 과제 음성 인식은 가변 길이 시퀀스 $\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T) \\in \\mathbb{R}^{T \\times D}$를 다른 가변 길이 시퀀스 $\\mathbf{Y} = (y_1, y_2, \\ldots, y_U) \\in \\mathcal{V}^U$로 변환하는 문제다. 여기서:\n$T$는 음향 특징 벡터의 시퀀스 길이 $D$는 각 특징 벡터의 차원 $U$는 출력 텍스트 토큰의 길이 $\\mathcal{V}$는 출력 토큰의 어휘 집합 (알파벳, 음소, 단어 등) 이 문제의 가장 근본적인 도전은 정렬(alignment) 불확실성이다. 음성 신호와 텍스트 사이의 정렬은 일대일 대응도 아니고, 명확한 경계도 없다. 이는 수학적으로 다음과 같은 문제를 야기한다:\n길이 불일치 문제: 일반적으로 $T \\gg U$ (음향 프레임 수가 텍스트 토큰 수보다 훨씬 많음) 명시적 정렬의 부재: 특정 음향 프레임 $\\mathbf{x}_t$가 어떤 텍스트 토큰 $y_u$에 대응하는지 알 수 없음 가변 속도 문제: 동일한 발화도 발화 속도에 따라 다른 길이의 음향 특징 시퀀스를 생성 전통적인 HMM-GMM 시스템은 이 문제를 명시적인 정렬(Viterbi alignments)을 통해 해결했다. 하지만 이 방법은 전문적인 언어학 지식이 필요하고, 수작업 라벨링의 의존도가 높았다.\nCTC와 RNN-T는 이러한 정렬 문제를 확률론적 관점에서 해결하는 엔드투엔드 접근법이다.\nCTC Loss의 수학적 형식화와 알고리즘 1. 확률 모델과 수학적 정의 CTC(Connectionist Temporal Classification)는 가능한 모든 정렬 경로의 확률을 합산함으로써 정렬 없이 시퀀스 라벨링을 가능하게 하는 방법이다. 먼저 기본 수학적 정의를 살펴보자.\n입력 시퀀스 $\\mathbf{X} \\in \\mathbb{R}^{T \\times D}$와 타겟 시퀀스 $\\mathbf{Y} \\in \\mathcal{V}^U$에 대해, CTC는 다음과 같이 $P(\\mathbf{Y}|\\mathbf{X})$를 정의한다:\n$$ 예측 네트워크(Prediction N(\\mathbf{Y})} P(\\pi|\\mathbf{X}) $$ 여기서:\n$\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_T)$는 길이 $T$의 가능한 정렬 경로 (path) $\\pi_t \\in \\mathcal{V} \\cup {blank}$은 각 시간 스텝에서의 예측 토큰 $\\mathcal{B}$는 경로 $\\pi$를 라벨 시퀀스 $\\mathbf{Y}$로 매핑하는 함수 $\\mathcal{B}^{-1}(\\mathbf{Y})$는 라벨 $\\mathbf{Y}$로 매핑되는 모든 경로의 집합 $\\mathcal{B}$는 두 가지 규칙을 적용한다:\n모든 blank 토큰(’-’)을 제거 연속된 중복 토큰을 하나로 병합 (예: “A-A-” → “A”) 각 경로 $\\pi$의 확률은 조건부 독립 가정하에 다음과 같이 계산된다:\n$$P(\\pi|\\mathbf{X}) = \\prod_{t=1}^{T} P(\\pi_t|\\mathbf{X}, t)$$\n여기서 $P(\\pi_t|\\mathbf{X}, t)$는 시간 $t$에서 모델이 출력하는 토큰 $\\pi_t$의 확률이다. 실제 구현에서는 이 확률은 Neural Network의 출력에 softmax를 적용하여 얻는다:\n$$P(\\pi_t = k|\\mathbf{X}, t) = \\frac{\\exp(z_{t,k})}{\\sum_{k’} \\exp(z_{t,k’})}$$\nCTC Loss는 음의 로그 우도(Negative Log-Likelihood)로 정의된다:\n$$\\mathcal{L}_{CTC} = -\\log P(\\mathbf{Y}|\\mathbf{X})$$\n2. Forward-Backward 알고리즘의 수학적 유도 CTC Loss를 계산하기 위해서는 모든 가능한 정렬 경로 $\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})$의 확률 합을 계산해야 한다. 가능한 경로의 수는 기하급수적으로 증가하므로, Dynamic Programing(동적 프로그래밍) 기법인 Forward-Backward 알고리즘을 사용한다.\n먼저, 확장된 라벨 시퀀스 $\\mathbf{l} = (l_1, l_2, \\ldots, l_{2U+1})$를 정의한다:\n$l_{2u-1} = y_u$ (홀수 인덱스에 원래 라벨) $l_{2u} = blank$ (짝수 인덱스에 blank 토큰) 예를 들어, “CAT\"는 “C-A-T-“로 확장된다.\nForward 알고리즘 Forward 변수 $\\alpha(t,s)$는 시간 $t$까지 확장된 라벨 $\\mathbf{l}$의 첫 $s$ 요소에 대응하는 모든 경로의 확률 합을 나타낸다:\n$$\\alpha(t,s) = \\sum_{\\pi_{1,2,\\ldots,t}: \\mathcal{B}(\\pi_{1,2,\\ldots,t}) = \\mathbf{l}{1,2,\\ldots,s}} \\prod{i=1}^{t} P(\\pi_i|\\mathbf{X}, i)$$\n다음과 같이 재귀적으로 계산할 수 있다:\n초기화:\n$\\alpha(1,1) = P(l_1|\\mathbf{X}, 1)$ $\\alpha(1,2) = P(l_2|\\mathbf{X}, 1)$ $\\alpha(1,s) = 0$ for $s \u003e 2$ 재귀식:\n$l_s \\neq blank$ 이고 $l_s \\neq l_{s-2}$ 인 경우: $$\\alpha(t,s) = \\left[ \\alpha(t-1,s) + \\alpha(t-1,s-1) + \\alpha(t-1,s-2) \\right] \\cdot P(l_s|\\mathbf{X}, t)$$\n$l_s \\neq blank$ 이고 $l_s = l_{s-2}$ 인 경우: $$\\alpha(t,s) = \\left[ \\alpha(t-1,s) + \\alpha(t-1,s-1) \\right] \\cdot P(l_s|\\mathbf{X}, t)$$\n$l_s = blank$ 인 경우: $$\\alpha(t,s) = \\left[ \\alpha(t-1,s) + \\alpha(t-1,s-1) \\right] \\cdot P(blank|\\mathbf{X}, t)$$\nBackward 알고리즘 Backward 변수 $\\beta(t,s)$는 시간 $t$부터 $T$까지, 확장된 라벨 $\\mathbf{l}$의 $s$번째 요소부터 끝까지 대응하는 모든 경로의 확률 합을 나타낸다:\n다음과 같이 재귀적으로 계산할 수 있다:\n초기화:\n$\\beta(T,2U+1) = 1$ $\\beta(T,2U) = P(blank|\\mathbf{X}, T)$ $\\beta(T,s) = 0$ for $s \u003c 2U$ 재귀식:\n$l_s \\neq blank$ 이고 $l_s \\neq l_{s+2}$ 인 경우: $$\\beta(t,s) = \\beta(t+1,s) \\cdot P(l_s|\\mathbf{X}, t+1) + \\beta(t+1,s+1) \\cdot P(l_{s+1}|\\mathbf{X}, t+1) + \\beta(t+1,s+2) \\cdot P(l_{s+2}|\\mathbf{X}, t+1)$$\n$l_s \\neq blank$ 이고 $l_s = l_{s+2}$ 인 경우: $$\\beta(t,s) = \\beta(t+1,s) \\cdot P(l_s|\\mathbf{X}, t+1) + \\beta(t+1,s+1) \\cdot P(l_{s+1}|\\mathbf{X}, t+1)$$\n$l_s = blank$ 인 경우: $$\\beta(t,s) = \\beta(t+1,s) \\cdot P(blank|\\mathbf{X}, t+1) + \\beta(t+1,s+1) \\cdot P(l_{s+1}|\\mathbf{X}, t+1)$$\n최종적으로, 타겟 시퀀스의 확률은 다음과 같이 계산된다: $$P(\\mathbf{Y}|\\mathbf{X}) = \\alpha(T, 2U+1) = \\beta(1, 1)$$\n3. 미분과 기울기 계산의 정밀 유도 CTC Loss의 기울기는 모델의 파라미터 최적화에 필수적이다. 각 시간 $t$와 클래스 $k$에 대한 로짓 $z_{t,k}$의 기울기를 다음과 같이 계산할 수 있다:\n$$ \\frac{\\partial \\mathcal{L}_{CTC}}{\\partial z_{t,k}}= \\frac{\\partial (-\\log P(\\mathbf{Y}|\\mathbf{X}))}{\\partial z_{t,k}} = -\\frac{1}{P(\\mathbf{Y}|\\mathbf{X})} \\cdot \\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} $$ $P(\\mathbf{Y}|\\mathbf{X})$의 $z_{t,k}$에 대한 편미분은 다음과 같이 전개된다:\n$$\\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} = \\frac{\\partial}{\\partial z_{t,k}} \\sum_{\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})} \\prod_{i=1}^{T} P(\\pi_i|\\mathbf{X}, i)$$\n소프트맥스 함수의 미분 성질을 이용하면:\n$$\\frac{\\partial P(j|\\mathbf{X}, t)}{\\partial z_{t,k}} = \\begin{cases} P(j|\\mathbf{X}, t) \\cdot (1 - P(k|\\mathbf{X}, t)) \u0026 \\text{if } j = k \\ -P(j|\\mathbf{X}, t) \\cdot P(k|\\mathbf{X}, t) \u0026 \\text{if } j \\neq k \\end{cases}$$\n이를 이용하여, 다음과 같이 기울기를 표현할 수 있다:\n$$ \\frac{\\partial \\mathcal{L}_{CTC}}{\\partial z_{t,k}} = P(k|\\mathbf{X}, t) - \\frac{1}{P(\\mathbf{Y}|\\mathbf{X})} \\sum_{s: l_s = k} \\alpha(t-1, s) \\cdot \\beta(t, s) $$ 여기서 $\\sum_{s: l_s = k}$는 확장된 라벨 시퀀스 $\\mathbf{l}$에서 값이 $k$인 모든 위치 $s$에 대한 합을 의미한다.\n4. 수치적 안정성과 계산 최적화 CTC Loss 계산 과정에서 여러 확률의 곱이 연속적으로 이루어지기 때문에, 수치적 언더플로우(numerical underflow) 문제가 발생할 수 있다. 이를 해결하기 위해 로그 도메인에서 연산을 수행한다.\n로그 스케일 Forward-Backward 알고리즘 로그 스케일에서 Forward 변수 $\\log \\alpha(t,s)$를 계산할 때, 덧셈 연산은 LogSumExp 함수로 대체된다:\n$$\\log(a + b) = \\log(a) + \\log(1 + \\exp(\\log(b) - \\log(a)))$$\n더 일반적으로, $\\log \\sum_i \\exp(x_i)$를 계산할 때는 다음 기법을 사용한다:\n$$\\log \\sum_i \\exp(x_i) = m + \\log \\sum_i \\exp(x_i - m) \\quad \\text{where } m = \\max_i x_i$$\n이를 통해 언더플로우와 오버플로우 문제를 모두 완화할 수 있다.\n병렬화 및 벡터화 최적화 실제 구현에서는 GPU와 같은 병렬 처리 하드웨어에서 효율적으로 계산하기 위해 다음과 같은 최적화 기법을 사용한다:\n배치 병렬화: 여러 샘플을 동시에 처리 행렬 연산 활용: 벡터화된 연산으로 Forward-Backward 계산 고속화 희소 행렬 표현: 많은 전이 확률이 0인 점을 활용한 계산 효율화 # 로그 도메인에서 안정적인 CTC 손실 계산 def log_sum_exp(x, axis=None): \"\"\"로그 도메인에서의 안정적인 덧셈\"\"\" x_max = np.max(x, axis=axis, keepdims=True) return x_max + np.log(np.sum(np.exp(x - x_max), axis=axis, keepdims=True)) def stable_log_forward(log_probs, labels, blank=0): \"\"\"로그 도메인에서의 Forward 알고리즘\"\"\" T, V = log_probs.shape # 시간 단계, 어휘 크기 extended_labels = np.concatenate([[blank], np.repeat(labels, 2)]) extended_labels = np.append(extended_labels, [blank]) S = len(extended_labels) # 로그 도메인에서의 알파 변수 초기화 log_alpha = np.ones((T, S)) * -np.inf log_alpha[0, 0] = log_probs[0, blank] if S \u003e 1: log_alpha[0, 1] = log_probs[0, extended_labels[1]] # 로그 도메인에서의 Forward 알고리즘 for t in range(1, T): for s in range(S): current_label = extended_labels[s] # 가능한 전이들을 고려 possible_moves = [] # 현재 상태 유지 possible_moves.append(log_alpha[t-1, s]) # 이전 라벨에서 전이 if s \u003e 0: possible_moves.append(log_alpha[t-1, s-1]) # 이전-이전 라벨에서 전이 (비blank→blank→비blank 제약 고려) if s \u003e 1 and extended_labels[s] != blank and extended_labels[s] != extended_labels[s-2]: possible_moves.append(log_alpha[t-1, s-2]) # 로그 도메인에서의 확률 합산 if possible_moves: log_alpha[t, s] = log_sum_exp(np.array(possible_moves)) + log_probs[t, current_label] # 최종 확률 계산 (마지막 라벨 또는 마지막 블랭크) return log_sum_exp(np.array([log_alpha[T-1, S-1], log_alpha[T-1, S-2]]) if S \u003e 1 else log_alpha[T-1, S-1]) 5. Prefix-Search 디코딩과 빔 서치 CTC 모델의 디코딩은 최대 사후 확률(MAP) 추정을 통해 이루어진다:\n$$\\hat{\\mathbf{Y}} = \\arg\\max_{\\mathbf{Y}} P(\\mathbf{Y}|\\mathbf{X})$$\n그러나 가능한 모든 출력 시퀀스를 탐색하는 것은 현실적으로 불가능하다. 따라서 여러 효율적인 디코딩 알고리즘이 사용된다:\nGreedy 디코딩 가장 단순한 방법은 각 시간 단계에서 가장 확률이 높은 토큰을 선택하고 CTC 병합 규칙을 적용하는 것이다:\n$$\\pi^* = \\arg\\max_{\\pi} \\prod_{t=1}^{T} P(\\pi_t|\\mathbf{X}, t)$$ $$\\hat{\\mathbf{Y}} = \\mathcal{B}(\\pi^*)$$\n빔 서치 디코딩 빔 서치는 여러 유망한 후보 경로를 동시에 유지하는 방법이다. CTC의 경우, 특수한 prefix 빔 서치가 사용된다:\n각 시간 단계 $t$에서, 확률이 높은 상위 $k$개의 prefix를 유지\n각 prefix에 대해 두 확률 성분을 계산:\n$P^b(prefix, t)$: $t$까지의 경로 중 blank로 끝나는 경로의 확률 $P^{nb}(prefix, t)$: $t$까지의 경로 중 non-blank로 끝나는 경로의 확률 확률 업데이트 규칙:\n$P^b(prefix, t) = P^b(prefix, t-1) \\cdot P(blank|\\mathbf{X}, t) + P^{nb}(prefix, t-1) \\cdot P(blank|\\mathbf{X}, t)$ $P^{nb}(prefix+c, t) = P^b(prefix, t-1) \\cdot P(c|\\mathbf{X}, t) + P^{nb}(prefix, t-1) \\cdot P(c|\\mathbf{X}, t)$ if $c \\neq \\text{last of prefix}$ $P^{nb}(prefix+c, t) = P^b(prefix, t-1) \\cdot P(c|\\mathbf{X}, t)$ if $c = \\text{last of prefix}$ 각 시간 단계 이후, prefix의 총 확률은 $P(prefix, t) = P^b(prefix, t) + P^{nb}(prefix, t)$\n이러한 빔 서치 알고리즘은 복잡한 실제 시나리오에서 Greedy 디코딩보다 우수한 결과를 제공한다.\n6. CTC Loss의 이론적 분석과 한계 CTC Loss는 수학적으로 특별한 성질과 몇 가지 한계를 가지고 있다:\n조건부 독립 가정 CTC의 핵심 가정은 각 시간 단계의 출력이 다른 시간 단계와 조건부 독립이라는 것이다:\n$$P(\\pi|\\mathbf{X}) = \\prod_{t=1}^{T} P(\\pi_t|\\mathbf{X}, t)$$\n이 가정은 모델이 시간적 의존성을 충분히 포착하지 못하게 한다. 특히 언어 모델링 관점에서, 이전 출력 토큰들의 정보를 활용하지 못한다.\nPeak 분포 성질 CTC 학습의 이론적 분석에 따르면, 모델은 정확한 정렬에 높은 확률을 집중시키는 “Peak Distribution\"을 형성하는 경향이 있다. 이로 인해 특정 정렬 패턴에 과도하게 집중하여 일반화 능력이 제한될 수 있다.\n가중치 불균형 문제 blank 토큰과 non-blank 토큰 간의 확률 불균형 문제가 존재한다. 일반적으로 blank 토큰은 더 자주 출현하므로, 학습 과정에서 더 큰 영향력을 갖게 된다. 이는 학습 초기에 모델이 대부분 blank를 예측하는 “blank 함정(blank trap)” 현상으로 이어질 수 있다.\n이러한 문제를 해결하기 위해 다양한 확장 및 변형 연구가 진행되었으며, 이 중 하나가 RNN-T이다.\n7. CTC Loss의 심도 깊은 수학적 분석 7.1 확률론적 프레임워크와 정보이론적 해석 CTC Loss는 정보이론적 관점에서 크로스 엔트로피 최소화 문제로 볼 수 있다. 구체적으로, 다음과 같은 확률 분포 간의 크로스 엔트로피를 최소화한다:\n$$H(P, Q) = -\\sum_{y} P(y) \\log Q(y)$$\n여기서 $P$는 실제 라벨의 확률 분포이고, $Q$는 모델이 예측한 분포이다. 실제 라벨 $\\mathbf{Y}$에 대해 $P(\\mathbf{Y}) = 1$이고 다른 모든 라벨에 대해 $P = 0$이므로, 이는 결국 $-\\log Q(\\mathbf{Y})$를 최소화하는 문제가 된다. 이것이 바로 CTC Loss 함수이다:\n$$\\mathcal{L}_{CTC} = -\\log P(\\mathbf{Y}|\\mathbf{X})$$\n정보이론적 관점에서, 이 손실 함수는 모델이 타겟 시퀀스를 정확히 예측하는 데 필요한 “놀람(surprise)“의 양, 즉 정보량을 측정한다. 손실이 낮을수록, 모델은 타겟 시퀀스를 더 높은 확률로 예측한다.\n7.2 심층 미분 유도 과정 CTC Loss의 미분을 더 상세히 살펴보자. 출력 로짓 $z_{t,k}$에 대한 CTC Loss의 편미분은 다음과 같이 유도된다:\n$$ \\frac{\\partial \\mathcal{L}_{CTC}}{\\partial z_{t,k}} = \\frac{\\partial}{\\partial z_{t,k}}\\left(-\\log \\sum_{\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})} \\prod_{i=1}^{T} P(\\pi_i|\\mathbf{X}, i)\\right) $$ 연쇄 법칙(chain rule)을 적용하면:\n$$ \\frac{\\partial \\mathcal{L}_{CTC}}{\\partial z_{t,k}} = -\\frac{1}{P(\\mathbf{Y}|\\mathbf{X})} \\cdot \\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} $$ 그리고:\n$$\\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} = \\frac{\\partial}{\\partial z_{t,k}} \\sum_{\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})} \\prod_{i=1}^{T} P(\\pi_i|\\mathbf{X}, i)$$\n시간 단계 $t$에서만 $z_{t,k}$가 영향을 미치므로:\n$$\\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} = \\sum_{\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})} \\frac{\\partial P(\\pi_t|\\mathbf{X}, t)}{\\partial z_{t,k}} \\prod_{i \\neq t} P(\\pi_i|\\mathbf{X}, i)$$\n소프트맥스 도함수를 적용하면:\n$$\\frac{\\partial P(\\pi_t = j|\\mathbf{X}, t)}{\\partial z_{t,k}} = \\begin{cases} P(\\pi_t = j|\\mathbf{X}, t)(1 - P(\\pi_t = j|\\mathbf{X}, t)) \u0026 \\text{if } j = k \\ -P(\\pi_t = j|\\mathbf{X}, t)P(\\pi_t = k|\\mathbf{X}, t) \u0026 \\text{if } j \\neq k \\end{cases}$$\n이를 정리하면:\n$$\\frac{\\partial P(\\mathbf{Y}|\\mathbf{X})}{\\partial z_{t,k}} = P(k|\\mathbf{X}, t) \\sum_{\\pi \\in \\mathcal{B}^{-1}(\\mathbf{Y})} \\prod_{i \\neq t} P(\\pi_i|\\mathbf{X}, i) \\left[ \\mathbf{1}_{\\pi_t = k} - P(k|\\mathbf{X}, t) \\right]$$\n여기서 $\\mathbf{1}_{\\pi_t = k}$는 $\\pi_t = k$일 때 1이고 그렇지 않으면 0인 지시 함수이다.\n최종적으로, Forward-Backward 변수를 활용하면:\n$$ \\frac{\\partial \\mathcal{L}_{CTC}}{\\partial z_{t,k}} = P(k|\\mathbf{X}, t) - \\frac{1}{P(\\mathbf{Y}|\\mathbf{X})} \\sum_{s: l_s = k} \\alpha(t-1, s) \\beta(t, s) $$ 이 식은 두 항의 차로 구성된다:\n첫 번째 항 $P(k|\\mathbf{X}, t)$는 현재 모델이 토큰 $k$에 할당하는 확률 두 번째 항은 모든 가능한 정렬 경로에서 시간 $t$에 토큰 $k$가 나타날 확률의 기대값 훈련이 진행됨에 따라, 이 두 항은 점점 더 가까워지며, 이는 모델이 올바른 정렬을 학습하고 있음을 의미한다.\n7.3 Blank 토큰의 역할에 대한 심층 이해 Blank 토큰은 CTC 학습에서 핵심적인 역할을 한다. 그 역할과 영향을 더 심층적으로 분석해보자.\nBlank 토큰의 확률적 역학 Blank 토큰이 높은 확률을 갖는 몇 가지 상황이 있다:\n반복 토큰 사이: 동일한 토큰이 연속적으로 등장해야 할 때 (예: “HELLO\"에서 “L” 토큰 반복) 연음 구간(transition regions): 한 발음에서 다른 발음으로 전환되는 중간 지점 무음 구간(silence regions): 발화 사이의 조용한 부분 학습 초기에는 모델이 정확한 정렬을 아직 학습하지 못했기 때문에, blank 토큰에 높은 확률을 할당하는 것이 “안전한 전략\"이 된다. 이는 “blank 함정(blank trap)” 현상을 초래할 수 있다.\nBlank 비율 통계 분석 실제 ASR 데이터셋에 대한 통계 분석에 따르면, 학습된 CTC 모델은 일반적으로 다음과 같은 blank 비율을 보인다:\n음성 프레임의 약 45-65%가 blank로 예측됨 단어 간 경계와 문장 시작/끝 부분에서 blank 비율이 증가 (최대 80-90%) 빠른 발음이나 명확한 조음에서는 blank 비율이 감소 (최소 30-40%) 이러한 통계는 blank 토큰이 단순한 “필러(filler)” 이상의 역할을 한다는 것을 시사한다. Blank는 음성의 시간적 구조와 단어 경계에 대한 중요한 정보를 인코딩한다.\nBlank 함정 극복을 위한 고급 기법 Blank 함정 문제를 완화하기 위한 몇 가지 고급 기법:\nBlank 감소 손실(Blank Reduction Loss): CTC Loss에 blank 확률을 명시적으로 페널티화하는 정규화 항 추가 $$ \\mathcal{L}_{BR} = \\mathcal{L}_{CTC} + \\lambda \\sum_{t=1}^{T} P(blank|\\mathbf{X}, t) $$ 스케줄링된 Blank 조정(Scheduled Blank Scaling): 학습 초기에는 blank 로짓에 작은 스케일 팩터를 적용하고, 학습이 진행됨에 따라 이를 점진적으로 1로 증가 $$ z'_{t,blank} = \\alpha_t \\cdot z_{t,blank} $$ 여기서 $\\alpha_t$는 시간에 따라 증가하는 스케일 팩터이다.\nCurriculum Learning: 쉬운 샘플(짧은 발화, 명확한 발음)부터 시작하여 점진적으로 어려운 샘플로 이동\n초기화 전략: 초기에 blank 로짓을 약간 낮은 값으로 초기화하여 학습 시작 시 non-blank 토큰에 더 많은 기회 제공\n7.4 수치적 안정성의 이론적 근거 CTC 계산의 수치적 안정성은 주로 로그 도메인 연산을 통해 달성된다. 그 이론적 근거를 심층적으로 살펴보자.\n로그 도메인 연산의 정밀도 분석 컴퓨터의 부동 소수점 표현에서, 값 $x$의 상대 오차는 대략 $\\epsilon \\cdot |x|$이다 (여기서 $\\epsilon$은 기계 입실론). 따라서 작은 확률값을 직접 곱하면 큰 상대 오차가 누적될 수 있다.\n로그 도메인에서는:\n곱셈이 덧셈으로 변환됨: $\\log(a \\cdot b) = \\log a + \\log b$ 덧셈이 로그-합-지수(LogSumExp) 연산으로 변환됨: $\\log(a + b) = \\log a + \\log(1 + \\exp(\\log b - \\log a))$ (만약 $a \u003e b$) 이러한 변환은 수치적 언더플로우를 방지할 뿐만 아니라, 수치 정밀도도 향상시킨다. 로그 스케일에서의 덧셈은 원래 값에 비례하는 오차가 아닌, 절대적 오차를 가진다.\n로그-합-지수(LogSumExp) 트릭의 엄밀한 증명 LogSumExp 트릭은 다음과 같이 표현된다:\n$$\\log \\sum_{i=1}^{n} \\exp(x_i) = m + \\log \\sum_{i=1}^{n} \\exp(x_i - m)$$\n이것이 올바르다는 것을 증명하자:\n$$\\begin{align} m + \\log \\sum_{i=1}^{n} \\exp(x_i - m) \u0026= m + \\log \\sum_{i=1}^{n} \\frac{\\exp(x_i)}{\\exp(m)} \\ \u0026= m + \\log \\left( \\frac{1}{\\exp(m)} \\sum_{i=1}^{n} \\exp(x_i) \\right) \\ \u0026= m + \\log \\sum_{i=1}^{n} \\exp(x_i) - m \\ \u0026= \\log \\sum_{i=1}^{n} \\exp(x_i) \\end{align}$$\n일반적으로 $m = \\max_i x_i$로 설정하면, $\\exp(x_i - m) \\leq 1$이 보장되어 오버플로우가 방지된다. 또한, 최소한 하나의 항은 정확히 1이 되어 언더플로우도 방지된다.\n7.5 CTC 손실의 확률적 해석과 최적화 이론 CTC 손실은 최대 우도 추정(Maximum Likelihood Estimation, MLE)의 프레임워크 내에서 이해할 수 있다. 이는 다음과 같은 최적화 문제로 표현된다:\n$$\\theta^* = \\arg\\max_{\\theta} \\prod_{i=1}^{N} P(\\mathbf{Y}_i|\\mathbf{X}_i; \\theta)$$\n로그를 취하면:\n$$\\theta^* = \\arg\\max_{\\theta} \\sum_{i=1}^{N} \\log P(\\mathbf{Y}_i|\\mathbf{X}_i; \\theta)$$\n이는 경사 상승법(gradient ascent)으로 최적화될 수 있으며, 이는 CTC 손실의 최소화와 동등하다:\n$$\\theta^* = \\arg\\min_{\\theta} \\sum_{i=1}^{N} -\\log P(\\mathbf{Y}_i|\\mathbf{X}_i; \\theta)$$\n최적화 알고리즘과 수렴 특성 CTC 손실 경사면(loss landscape)은 일반적으로 다음과 같은 특성을 가진다:\n초기 학습 단계에서의 불안정성: 손실 함수가 가파르게 감소하며, 그레디언트의 분산이 크다 Plateau 지역: 학습 중간 단계에서 손실이 느리게 감소하는 구간이 나타날 수 있다 복잡한 국소 최적점 구조: 여러 국소 최적점이 존재할 수 있으며, 이는 다양한 가능한 정렬에 해당한다 이러한 특성을 고려하여, CTC 모델 훈련에 효과적인 최적화 전략:\n학습률 스케줄링: 초기에는 낮은 학습률로 시작하여 점차 증가시키고, 이후 점진적으로 감소 그레디언트 클리핑: 특히 초기 학습 단계에서 그레디언트 폭발을 방지하기 위해 그레디언트 노름에 상한을 설정 모멘텀 기반 최적화: Adam이나 RMSprop와 같은 적응적 모멘텀 기반 최적화 알고리즘을 사용하여 plateau 지역을 효과적으로 탈출 배치 정규화: 훈련 안정성을 향상시키고 더 빠른 수렴을 촉진 학습률 워밍업과 스케줄링의 이론적 근거 CTC 학습의 초기 단계에서는 그레디언트가 불안정하고 크기가 클 수 있다. 이러한 이유로, 학습률 워밍업이 효과적이다:\n매우 낮은 학습률(예: 1e-6)로 시작 $N_{warmup}$ 스텝 동안 학습률을 목표값(예: 1e-3 또는 1e-4)까지 선형적으로 증가 그 후, 코사인 또는 지수 감소 스케줄링을 적용 이러한 접근법의 이론적 근거는 다음과 같다:\n초기 낮은 학습률: 무작위 초기화된 모델의 불안정한 그레디언트로 인한 발산 방지 점진적 증가: 모델이 더 안정적인 그레디언트를 생성하게 되면서 학습 속도 향상 후기 감소: 미세 조정 단계에서 국소 최적점 주변에서의 진동 감소 실증적 연구에 따르면, 이러한 스케줄링 전략은 CTC 모델의 최종 성능을 1-2% 개선할 수 있다.\n8. RNN-T: CTC의 한계를 넘어선 수학적 접근 8.1. RNN-T의 수학적 형식화 RNN-T(Recurrent Neural Network Transducer)는 CTC의 조건부 독립 가정을 극복하기 위해 개발된 방법이다. RNN-T는 이전 출력 토큰들의 정보를 활용하여 다음과 같이 확률 모델을 정의한다:\n$$P(\\mathbf{Y}|\\mathbf{X}) = \\sum_{\\pi \\in \\mathcal{A}(\\mathbf{Y})} P(\\pi|\\mathbf{X})$$\n$$P(\\pi|\\mathbf{X}) = \\prod_{i=1}^{|\\pi|} P(\\pi_i|\\mathbf{X}, \\mathbf{y}_{",
  "wordCount" : "8039",
  "inLanguage": "ko",
  "datePublished": "2024-08-10T10:30:00+09:00",
  "dateModified": "2024-08-10T10:30:00+09:00",
  "author":{
    "@type": "Person",
    "name": "macsim"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://macsim2.github.io/posts/tech/asr/ctc_loss/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "macsim's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://macsim2.github.io/img/Q.gif"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://macsim2.github.io/" accesskey="h" title="Macsim&#39;s Blog (Alt + H)">Macsim&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://macsim2.github.io/" title="🏠 home">
                <span>🏠 home</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/about/" title="🙋 about">
                <span>🙋 about</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/archives/" title="archives">
                <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/posts/" title="📚 posts">
                <span>📚 posts</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/tags/" title="🧩 tags">
                <span>🧩 tags</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/search/" title="⏱️ search (Alt &#43; /)" accesskey=/>
                <span>⏱️ search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://macsim2.github.io/">🏠 홈</a>&nbsp;»&nbsp;<a href="https://macsim2.github.io/posts/">posts</a>&nbsp;»&nbsp;<a href="https://macsim2.github.io/posts/tech/">👨🏻‍💻 tech</a></div>
            <h1 class="post-title">
                CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리
            </h1>
            <div class="post-description">
                자동 음성 인식에서 핵심적인 CTC Loss와 RNN-T Loss의 내부 메커니즘, 수학적 유도, 최적화 기법 및 실전 적용 경험에 대한 심도 있는 기술적 분석
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2024-08-10
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>8039 word
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>17 min
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>macsim
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://macsim2.github.io/tags/deeplearning/" style="color: var(--secondary)!important;">Deeplearning</a>
                &nbsp;<a href="https://macsim2.github.io/tags/asr/" style="color: var(--secondary)!important;">ASR</a>
                &nbsp;<a href="https://macsim2.github.io/tags/speech-recognition/" style="color: var(--secondary)!important;">Speech Recognition</a>
                &nbsp;<a href="https://macsim2.github.io/tags/ctc/" style="color: var(--secondary)!important;">CTC</a>
                &nbsp;<a href="https://macsim2.github.io/tags/rnn-t/" style="color: var(--secondary)!important;">RNN-T</a>
                &nbsp;<a href="https://macsim2.github.io/tags/sequence-modeling/" style="color: var(--secondary)!important;">Sequence Modeling</a>
            </span>
        </span>
    </span>
</span>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

                <span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://macsim2.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">목차</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#asr%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ec%a0%95%ed%98%95%ed%99%94%ec%99%80-%eb%8f%84%ec%a0%84-%ea%b3%bc%ec%a0%9c" aria-label="ASR의 수학적 정형화와 도전 과제">ASR의 수학적 정형화와 도전 과제</a></li>
                <li>
                    <a href="#ctc-loss%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ed%98%95%ec%8b%9d%ed%99%94%ec%99%80-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98" aria-label="CTC Loss의 수학적 형식화와 알고리즘">CTC Loss의 수학적 형식화와 알고리즘</a><ul>
                        
                <li>
                    <a href="#1-%ed%99%95%eb%a5%a0-%eb%aa%a8%eb%8d%b8%ea%b3%bc-%ec%88%98%ed%95%99%ec%a0%81-%ec%a0%95%ec%9d%98" aria-label="1. 확률 모델과 수학적 정의">1. 확률 모델과 수학적 정의</a></li>
                <li>
                    <a href="#2-forward-backward-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ec%9c%a0%eb%8f%84" aria-label="2. Forward-Backward 알고리즘의 수학적 유도">2. Forward-Backward 알고리즘의 수학적 유도</a><ul>
                        
                <li>
                    <a href="#forward-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98" aria-label="Forward 알고리즘">Forward 알고리즘</a></li>
                <li>
                    <a href="#backward-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98" aria-label="Backward 알고리즘">Backward 알고리즘</a></li></ul>
                </li>
                <li>
                    <a href="#3-%eb%af%b8%eb%b6%84%ea%b3%bc-%ea%b8%b0%ec%9a%b8%ea%b8%b0-%ea%b3%84%ec%82%b0%ec%9d%98-%ec%a0%95%eb%b0%80-%ec%9c%a0%eb%8f%84" aria-label="3. 미분과 기울기 계산의 정밀 유도">3. 미분과 기울기 계산의 정밀 유도</a></li>
                <li>
                    <a href="#4-%ec%88%98%ec%b9%98%ec%a0%81-%ec%95%88%ec%a0%95%ec%84%b1%ea%b3%bc-%ea%b3%84%ec%82%b0-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="4. 수치적 안정성과 계산 최적화">4. 수치적 안정성과 계산 최적화</a><ul>
                        
                <li>
                    <a href="#%eb%a1%9c%ea%b7%b8-%ec%8a%a4%ec%bc%80%ec%9d%bc-forward-backward-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98" aria-label="로그 스케일 Forward-Backward 알고리즘">로그 스케일 Forward-Backward 알고리즘</a></li>
                <li>
                    <a href="#%eb%b3%91%eb%a0%ac%ed%99%94-%eb%b0%8f-%eb%b2%a1%ed%84%b0%ed%99%94-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="병렬화 및 벡터화 최적화">병렬화 및 벡터화 최적화</a></li></ul>
                </li>
                <li>
                    <a href="#5-prefix-search-%eb%94%94%ec%bd%94%eb%94%a9%ea%b3%bc-%eb%b9%94-%ec%84%9c%ec%b9%98" aria-label="5. Prefix-Search 디코딩과 빔 서치">5. Prefix-Search 디코딩과 빔 서치</a><ul>
                        
                <li>
                    <a href="#greedy-%eb%94%94%ec%bd%94%eb%94%a9" aria-label="Greedy 디코딩">Greedy 디코딩</a></li>
                <li>
                    <a href="#%eb%b9%94-%ec%84%9c%ec%b9%98-%eb%94%94%ec%bd%94%eb%94%a9" aria-label="빔 서치 디코딩">빔 서치 디코딩</a></li></ul>
                </li>
                <li>
                    <a href="#6-ctc-loss%ec%9d%98-%ec%9d%b4%eb%a1%a0%ec%a0%81-%eb%b6%84%ec%84%9d%ea%b3%bc-%ed%95%9c%ea%b3%84" aria-label="6. CTC Loss의 이론적 분석과 한계">6. CTC Loss의 이론적 분석과 한계</a><ul>
                        
                <li>
                    <a href="#%ec%a1%b0%ea%b1%b4%eb%b6%80-%eb%8f%85%eb%a6%bd-%ea%b0%80%ec%a0%95" aria-label="조건부 독립 가정">조건부 독립 가정</a></li>
                <li>
                    <a href="#peak-%eb%b6%84%ed%8f%ac-%ec%84%b1%ec%a7%88" aria-label="Peak 분포 성질">Peak 분포 성질</a></li>
                <li>
                    <a href="#%ea%b0%80%ec%a4%91%ec%b9%98-%eb%b6%88%ea%b7%a0%ed%98%95-%eb%ac%b8%ec%a0%9c" aria-label="가중치 불균형 문제">가중치 불균형 문제</a></li></ul>
                </li>
                <li>
                    <a href="#7-ctc-loss%ec%9d%98-%ec%8b%ac%eb%8f%84-%ea%b9%8a%ec%9d%80-%ec%88%98%ed%95%99%ec%a0%81-%eb%b6%84%ec%84%9d" aria-label="7. CTC Loss의 심도 깊은 수학적 분석">7. CTC Loss의 심도 깊은 수학적 분석</a><ul>
                        
                <li>
                    <a href="#71-%ed%99%95%eb%a5%a0%eb%a1%a0%ec%a0%81-%ed%94%84%eb%a0%88%ec%9e%84%ec%9b%8c%ed%81%ac%ec%99%80-%ec%a0%95%eb%b3%b4%ec%9d%b4%eb%a1%a0%ec%a0%81-%ed%95%b4%ec%84%9d" aria-label="7.1 확률론적 프레임워크와 정보이론적 해석">7.1 확률론적 프레임워크와 정보이론적 해석</a></li>
                <li>
                    <a href="#72-%ec%8b%ac%ec%b8%b5-%eb%af%b8%eb%b6%84-%ec%9c%a0%eb%8f%84-%ea%b3%bc%ec%a0%95" aria-label="7.2 심층 미분 유도 과정">7.2 심층 미분 유도 과정</a></li>
                <li>
                    <a href="#73-blank-%ed%86%a0%ed%81%b0%ec%9d%98-%ec%97%ad%ed%95%a0%ec%97%90-%eb%8c%80%ed%95%9c-%ec%8b%ac%ec%b8%b5-%ec%9d%b4%ed%95%b4" aria-label="7.3 Blank 토큰의 역할에 대한 심층 이해">7.3 Blank 토큰의 역할에 대한 심층 이해</a><ul>
                        
                <li>
                    <a href="#blank-%ed%86%a0%ed%81%b0%ec%9d%98-%ed%99%95%eb%a5%a0%ec%a0%81-%ec%97%ad%ed%95%99" aria-label="Blank 토큰의 확률적 역학">Blank 토큰의 확률적 역학</a></li>
                <li>
                    <a href="#blank-%eb%b9%84%ec%9c%a8-%ed%86%b5%ea%b3%84-%eb%b6%84%ec%84%9d" aria-label="Blank 비율 통계 분석">Blank 비율 통계 분석</a></li>
                <li>
                    <a href="#blank-%ed%95%a8%ec%a0%95-%ea%b7%b9%eb%b3%b5%ec%9d%84-%ec%9c%84%ed%95%9c-%ea%b3%a0%ea%b8%89-%ea%b8%b0%eb%b2%95" aria-label="Blank 함정 극복을 위한 고급 기법">Blank 함정 극복을 위한 고급 기법</a></li></ul>
                </li>
                <li>
                    <a href="#74-%ec%88%98%ec%b9%98%ec%a0%81-%ec%95%88%ec%a0%95%ec%84%b1%ec%9d%98-%ec%9d%b4%eb%a1%a0%ec%a0%81-%ea%b7%bc%ea%b1%b0" aria-label="7.4 수치적 안정성의 이론적 근거">7.4 수치적 안정성의 이론적 근거</a><ul>
                        
                <li>
                    <a href="#%eb%a1%9c%ea%b7%b8-%eb%8f%84%eb%a9%94%ec%9d%b8-%ec%97%b0%ec%82%b0%ec%9d%98-%ec%a0%95%eb%b0%80%eb%8f%84-%eb%b6%84%ec%84%9d" aria-label="로그 도메인 연산의 정밀도 분석">로그 도메인 연산의 정밀도 분석</a></li>
                <li>
                    <a href="#%eb%a1%9c%ea%b7%b8-%ed%95%a9-%ec%a7%80%ec%88%98logsumexp-%ed%8a%b8%eb%a6%ad%ec%9d%98-%ec%97%84%eb%b0%80%ed%95%9c-%ec%a6%9d%eb%aa%85" aria-label="로그-합-지수(LogSumExp) 트릭의 엄밀한 증명">로그-합-지수(LogSumExp) 트릭의 엄밀한 증명</a></li></ul>
                </li>
                <li>
                    <a href="#75-ctc-%ec%86%90%ec%8b%a4%ec%9d%98-%ed%99%95%eb%a5%a0%ec%a0%81-%ed%95%b4%ec%84%9d%ea%b3%bc-%ec%b5%9c%ec%a0%81%ed%99%94-%ec%9d%b4%eb%a1%a0" aria-label="7.5 CTC 손실의 확률적 해석과 최적화 이론">7.5 CTC 손실의 확률적 해석과 최적화 이론</a><ul>
                        
                <li>
                    <a href="#%ec%b5%9c%ec%a0%81%ed%99%94-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98%ea%b3%bc-%ec%88%98%eb%a0%b4-%ed%8a%b9%ec%84%b1" aria-label="최적화 알고리즘과 수렴 특성">최적화 알고리즘과 수렴 특성</a></li>
                <li>
                    <a href="#%ed%95%99%ec%8a%b5%eb%a5%a0-%ec%9b%8c%eb%b0%8d%ec%97%85%ea%b3%bc-%ec%8a%a4%ec%bc%80%ec%a4%84%eb%a7%81%ec%9d%98-%ec%9d%b4%eb%a1%a0%ec%a0%81-%ea%b7%bc%ea%b1%b0" aria-label="학습률 워밍업과 스케줄링의 이론적 근거">학습률 워밍업과 스케줄링의 이론적 근거</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#8-rnn-t-ctc%ec%9d%98-%ed%95%9c%ea%b3%84%eb%a5%bc-%eb%84%98%ec%96%b4%ec%84%a0-%ec%88%98%ed%95%99%ec%a0%81-%ec%a0%91%ea%b7%bc" aria-label="8. RNN-T: CTC의 한계를 넘어선 수학적 접근">8. RNN-T: CTC의 한계를 넘어선 수학적 접근</a><ul>
                        
                <li>
                    <a href="#81-rnn-t%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%ed%98%95%ec%8b%9d%ed%99%94" aria-label="8.1. RNN-T의 수학적 형식화">8.1. RNN-T의 수학적 형식화</a></li>
                <li>
                    <a href="#82-rnn-t%ec%9d%98-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98%ec%a0%81-%ea%b5%ac%ec%84%b1" aria-label="8.2. RNN-T의 아키텍처적 구성">8.2. RNN-T의 아키텍처적 구성</a></li>
                <li>
                    <a href="#83-%eb%b3%b5%ec%9e%a1%ed%95%9c-forward-backward-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98" aria-label="8.3. 복잡한 Forward-Backward 알고리즘">8.3. 복잡한 Forward-Backward 알고리즘</a></li>
                <li>
                    <a href="#84-rnn-t%ec%99%80-ctc%ec%9d%98-%ec%88%98%ed%95%99%ec%a0%81-%eb%b9%84%ea%b5%90" aria-label="8.4 RNN-T와 CTC의 수학적 비교">8.4 RNN-T와 CTC의 수학적 비교</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%ea%b2%b0%eb%a1%a0" aria-label="결론">결론</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p><strong>&ldquo;CTC Loss와 RNN-T Loss의 수학적 기반과 구현 최적화 기법은 무엇인가?&rdquo;</strong> <br>
음성 인식(ASR) 분야에서 이 두 손실 함수는 핵심적인 역할을 하지만, 그 내부 동작 원리와 수학적 기반을 완전히 이해하는 것은 쉽지 않다.</p>
<p>이 글에서는 단순히 &ldquo;CTC와 RNN-T 사용법&quot;이 아닌, 두 손실 함수의 수학적 유도 과정, 알고리즘 동작 원리, 그리고 실제 프로젝트에서의 최적화 기법과 구현 세부사항에 대해 심도 있게 파헤쳐보려 한다.</p>
<h2 id="asr의-수학적-정형화와-도전-과제">ASR의 수학적 정형화와 도전 과제<a hidden class="anchor" aria-hidden="true" href="#asr의-수학적-정형화와-도전-과제">#</a></h2>
<p>음성 인식은 가변 길이 시퀀스 $\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T) \in \mathbb{R}^{T \times D}$를 다른 가변 길이 시퀀스 $\mathbf{Y} = (y_1, y_2, \ldots, y_U) \in \mathcal{V}^U$로 변환하는 문제다. 여기서:</p>
<ul>
<li>$T$는 음향 특징 벡터의 시퀀스 길이</li>
<li>$D$는 각 특징 벡터의 차원</li>
<li>$U$는 출력 텍스트 토큰의 길이</li>
<li>$\mathcal{V}$는 출력 토큰의 어휘 집합 (알파벳, 음소, 단어 등)</li>
</ul>
<p>이 문제의 가장 근본적인 도전은 <strong>정렬(alignment) 불확실성</strong>이다. 음성 신호와 텍스트 사이의 정렬은 일대일 대응도 아니고, 명확한 경계도 없다. 이는 수학적으로 다음과 같은 문제를 야기한다:</p>
<ol>
<li>길이 불일치 문제: 일반적으로 $T \gg U$ (음향 프레임 수가 텍스트 토큰 수보다 훨씬 많음)</li>
<li>명시적 정렬의 부재: 특정 음향 프레임 $\mathbf{x}_t$가 어떤 텍스트 토큰 $y_u$에 대응하는지 알 수 없음</li>
<li>가변 속도 문제: 동일한 발화도 발화 속도에 따라 다른 길이의 음향 특징 시퀀스를 생성</li>
</ol>
<p>전통적인 HMM-GMM 시스템은 이 문제를 명시적인 정렬(Viterbi alignments)을 통해 해결했다. 하지만 이 방법은 전문적인 언어학 지식이 필요하고, 수작업 라벨링의 의존도가 높았다.</p>
<p>CTC와 RNN-T는 이러한 정렬 문제를 확률론적 관점에서 해결하는 엔드투엔드 접근법이다.</p>
<h2 id="ctc-loss의-수학적-형식화와-알고리즘">CTC Loss의 수학적 형식화와 알고리즘<a hidden class="anchor" aria-hidden="true" href="#ctc-loss의-수학적-형식화와-알고리즘">#</a></h2>
<h3 id="1-확률-모델과-수학적-정의">1. 확률 모델과 수학적 정의<a hidden class="anchor" aria-hidden="true" href="#1-확률-모델과-수학적-정의">#</a></h3>
<p>CTC(Connectionist Temporal Classification)는 가능한 모든 정렬 경로의 확률을 합산함으로써 정렬 없이 시퀀스 라벨링을 가능하게 하는 방법이다. 먼저 기본 수학적 정의를 살펴보자.</p>
<p>입력 시퀀스 $\mathbf{X} \in \mathbb{R}^{T \times D}$와 타겟 시퀀스 $\mathbf{Y} \in \mathcal{V}^U$에 대해, CTC는 다음과 같이 $P(\mathbf{Y}|\mathbf{X})$를 정의한다:</p>
   <div>
   $$
   예측 네트워크(Prediction N(\mathbf{Y})} P(\pi|\mathbf{X})
   $$
   </div>
<p>여기서:</p>
<ul>
<li>$\pi = (\pi_1, \pi_2, \ldots, \pi_T)$는 길이 $T$의 가능한 정렬 경로 (path)</li>
<li>$\pi_t \in \mathcal{V} \cup {blank}$은 각 시간 스텝에서의 예측 토큰</li>
<li>$\mathcal{B}$는 경로 $\pi$를 라벨 시퀀스 $\mathbf{Y}$로 매핑하는 함수</li>
<li>$\mathcal{B}^{-1}(\mathbf{Y})$는 라벨 $\mathbf{Y}$로 매핑되는 모든 경로의 집합</li>
</ul>
<p>$\mathcal{B}$는 두 가지 규칙을 적용한다:</p>
<ol>
<li>모든 blank 토큰(&rsquo;-&rsquo;)을 제거</li>
<li>연속된 중복 토큰을 하나로 병합 (예: &ldquo;A-A-&rdquo; → &ldquo;A&rdquo;)</li>
</ol>
<p>각 경로 $\pi$의 확률은 조건부 독립 가정하에 다음과 같이 계산된다:</p>
<p>$$P(\pi|\mathbf{X}) = \prod_{t=1}^{T} P(\pi_t|\mathbf{X}, t)$$</p>
<p>여기서 $P(\pi_t|\mathbf{X}, t)$는 시간 $t$에서 모델이 출력하는 토큰 $\pi_t$의 확률이다. 실제 구현에서는 이 확률은 Neural Network의 출력에 softmax를 적용하여 얻는다:</p>
<p>$$P(\pi_t = k|\mathbf{X}, t) = \frac{\exp(z_{t,k})}{\sum_{k&rsquo;} \exp(z_{t,k&rsquo;})}$$</p>
<p>CTC Loss는 음의 로그 우도(Negative Log-Likelihood)로 정의된다:</p>
<p>$$\mathcal{L}_{CTC} = -\log P(\mathbf{Y}|\mathbf{X})$$</p>
<h3 id="2-forward-backward-알고리즘의-수학적-유도">2. Forward-Backward 알고리즘의 수학적 유도<a hidden class="anchor" aria-hidden="true" href="#2-forward-backward-알고리즘의-수학적-유도">#</a></h3>
<p>CTC Loss를 계산하기 위해서는 모든 가능한 정렬 경로 $\pi \in \mathcal{B}^{-1}(\mathbf{Y})$의 확률 합을 계산해야 한다. 가능한 경로의 수는 기하급수적으로 증가하므로, Dynamic Programing(동적 프로그래밍) 기법인 Forward-Backward 알고리즘을 사용한다.</p>
<p>먼저, 확장된 라벨 시퀀스 $\mathbf{l} = (l_1, l_2, \ldots, l_{2U+1})$를 정의한다:</p>
<ul>
<li>$l_{2u-1} = y_u$ (홀수 인덱스에 원래 라벨)</li>
<li>$l_{2u} = blank$ (짝수 인덱스에 blank 토큰)</li>
</ul>
<p>예를 들어, &ldquo;CAT&quot;는 &ldquo;C-A-T-&ldquo;로 확장된다.</p>
<h4 id="forward-알고리즘">Forward 알고리즘<a hidden class="anchor" aria-hidden="true" href="#forward-알고리즘">#</a></h4>
<p>Forward 변수 $\alpha(t,s)$는 시간 $t$까지 확장된 라벨 $\mathbf{l}$의 첫 $s$ 요소에 대응하는 모든 경로의 확률 합을 나타낸다:</p>
<p>$$\alpha(t,s) = \sum_{\pi_{1,2,\ldots,t}: \mathcal{B}(\pi_{1,2,\ldots,t}) = \mathbf{l}<em>{1,2,\ldots,s}} \prod</em>{i=1}^{t} P(\pi_i|\mathbf{X}, i)$$</p>
<p>다음과 같이 재귀적으로 계산할 수 있다:</p>
<p><strong>초기화:</strong></p>
<ul>
<li>$\alpha(1,1) = P(l_1|\mathbf{X}, 1)$</li>
<li>$\alpha(1,2) = P(l_2|\mathbf{X}, 1)$</li>
<li>$\alpha(1,s) = 0$ for $s &gt; 2$</li>
</ul>
<p><strong>재귀식:</strong></p>
<ul>
<li>
<p>$l_s \neq blank$ 이고 $l_s \neq l_{s-2}$ 인 경우:
$$\alpha(t,s) = \left[ \alpha(t-1,s) + \alpha(t-1,s-1) + \alpha(t-1,s-2) \right] \cdot P(l_s|\mathbf{X}, t)$$</p>
</li>
<li>
<p>$l_s \neq blank$ 이고 $l_s = l_{s-2}$ 인 경우:
$$\alpha(t,s) = \left[ \alpha(t-1,s) + \alpha(t-1,s-1) \right] \cdot P(l_s|\mathbf{X}, t)$$</p>
</li>
<li>
<p>$l_s = blank$ 인 경우:
$$\alpha(t,s) = \left[ \alpha(t-1,s) + \alpha(t-1,s-1) \right] \cdot P(blank|\mathbf{X}, t)$$</p>
</li>
</ul>
<h4 id="backward-알고리즘">Backward 알고리즘<a hidden class="anchor" aria-hidden="true" href="#backward-알고리즘">#</a></h4>
<p>Backward 변수 $\beta(t,s)$는 시간 $t$부터 $T$까지, 확장된 라벨 $\mathbf{l}$의 $s$번째 요소부터 끝까지 대응하는 모든 경로의 확률 합을 나타낸다:</p>
<p>다음과 같이 재귀적으로 계산할 수 있다:</p>
<p><strong>초기화:</strong></p>
<ul>
<li>$\beta(T,2U+1) = 1$</li>
<li>$\beta(T,2U) = P(blank|\mathbf{X}, T)$</li>
<li>$\beta(T,s) = 0$ for $s &lt; 2U$</li>
</ul>
<p><strong>재귀식:</strong></p>
<ul>
<li>
<p>$l_s \neq blank$ 이고 $l_s \neq l_{s+2}$ 인 경우:
$$\beta(t,s) = \beta(t+1,s) \cdot P(l_s|\mathbf{X}, t+1) + \beta(t+1,s+1) \cdot P(l_{s+1}|\mathbf{X}, t+1) + \beta(t+1,s+2) \cdot P(l_{s+2}|\mathbf{X}, t+1)$$</p>
</li>
<li>
<p>$l_s \neq blank$ 이고 $l_s = l_{s+2}$ 인 경우:
$$\beta(t,s) = \beta(t+1,s) \cdot P(l_s|\mathbf{X}, t+1) + \beta(t+1,s+1) \cdot P(l_{s+1}|\mathbf{X}, t+1)$$</p>
</li>
<li>
<p>$l_s = blank$ 인 경우:
$$\beta(t,s) = \beta(t+1,s) \cdot P(blank|\mathbf{X}, t+1) + \beta(t+1,s+1) \cdot P(l_{s+1}|\mathbf{X}, t+1)$$</p>
</li>
</ul>
<p>최종적으로, 타겟 시퀀스의 확률은 다음과 같이 계산된다:
$$P(\mathbf{Y}|\mathbf{X}) = \alpha(T, 2U+1) = \beta(1, 1)$$</p>
<h3 id="3-미분과-기울기-계산의-정밀-유도">3. 미분과 기울기 계산의 정밀 유도<a hidden class="anchor" aria-hidden="true" href="#3-미분과-기울기-계산의-정밀-유도">#</a></h3>
<p>CTC Loss의 기울기는 모델의 파라미터 최적화에 필수적이다. 각 시간 $t$와 클래스 $k$에 대한 로짓 $z_{t,k}$의 기울기를 다음과 같이 계산할 수 있다:</p>
<div>
$$
\frac{\partial \mathcal{L}_{CTC}}{\partial z_{t,k}}= \frac{\partial (-\log P(\mathbf{Y}|\mathbf{X}))}{\partial z_{t,k}} = -\frac{1}{P(\mathbf{Y}|\mathbf{X})} \cdot \frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}}
$$
</div>
<p>$P(\mathbf{Y}|\mathbf{X})$의 $z_{t,k}$에 대한 편미분은 다음과 같이 전개된다:</p>
<p>$$\frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}} = \frac{\partial}{\partial z_{t,k}} \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{Y})} \prod_{i=1}^{T} P(\pi_i|\mathbf{X}, i)$$</p>
<p>소프트맥스 함수의 미분 성질을 이용하면:</p>
<p>$$\frac{\partial P(j|\mathbf{X}, t)}{\partial z_{t,k}} =
\begin{cases}
P(j|\mathbf{X}, t) \cdot (1 - P(k|\mathbf{X}, t)) &amp; \text{if } j = k \
-P(j|\mathbf{X}, t) \cdot P(k|\mathbf{X}, t) &amp; \text{if } j \neq k
\end{cases}$$</p>
<p>이를 이용하여, 다음과 같이 기울기를 표현할 수 있다:</p>
<div>
$$
\frac{\partial \mathcal{L}_{CTC}}{\partial z_{t,k}} = P(k|\mathbf{X}, t) - \frac{1}{P(\mathbf{Y}|\mathbf{X})} \sum_{s: l_s = k} \alpha(t-1, s) \cdot \beta(t, s)
$$
</div>
<p>여기서 $\sum_{s: l_s = k}$는 확장된 라벨 시퀀스 $\mathbf{l}$에서 값이 $k$인 모든 위치 $s$에 대한 합을 의미한다.</p>
<h3 id="4-수치적-안정성과-계산-최적화">4. 수치적 안정성과 계산 최적화<a hidden class="anchor" aria-hidden="true" href="#4-수치적-안정성과-계산-최적화">#</a></h3>
<p>CTC Loss 계산 과정에서 여러 확률의 곱이 연속적으로 이루어지기 때문에, 수치적 언더플로우(numerical underflow) 문제가 발생할 수 있다. 이를 해결하기 위해 로그 도메인에서 연산을 수행한다.</p>
<h4 id="로그-스케일-forward-backward-알고리즘">로그 스케일 Forward-Backward 알고리즘<a hidden class="anchor" aria-hidden="true" href="#로그-스케일-forward-backward-알고리즘">#</a></h4>
<p>로그 스케일에서 Forward 변수 $\log \alpha(t,s)$를 계산할 때, 덧셈 연산은 LogSumExp 함수로 대체된다:</p>
<p>$$\log(a + b) = \log(a) + \log(1 + \exp(\log(b) - \log(a)))$$</p>
<p>더 일반적으로, $\log \sum_i \exp(x_i)$를 계산할 때는 다음 기법을 사용한다:</p>
<p>$$\log \sum_i \exp(x_i) = m + \log \sum_i \exp(x_i - m) \quad \text{where } m = \max_i x_i$$</p>
<p>이를 통해 언더플로우와 오버플로우 문제를 모두 완화할 수 있다.</p>
<h4 id="병렬화-및-벡터화-최적화">병렬화 및 벡터화 최적화<a hidden class="anchor" aria-hidden="true" href="#병렬화-및-벡터화-최적화">#</a></h4>
<p>실제 구현에서는 GPU와 같은 병렬 처리 하드웨어에서 효율적으로 계산하기 위해 다음과 같은 최적화 기법을 사용한다:</p>
<ol>
<li><strong>배치 병렬화</strong>: 여러 샘플을 동시에 처리</li>
<li><strong>행렬 연산 활용</strong>: 벡터화된 연산으로 Forward-Backward 계산 고속화</li>
<li><strong>희소 행렬 표현</strong>: 많은 전이 확률이 0인 점을 활용한 계산 효율화</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 로그 도메인에서 안정적인 CTC 손실 계산</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">log_sum_exp</span>(x, axis<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;로그 도메인에서의 안정적인 덧셈&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    x_max <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(x, axis<span style="color:#f92672">=</span>axis, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x_max <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>log(np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>exp(x <span style="color:#f92672">-</span> x_max), axis<span style="color:#f92672">=</span>axis, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stable_log_forward</span>(log_probs, labels, blank<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;로그 도메인에서의 Forward 알고리즘&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    T, V <span style="color:#f92672">=</span> log_probs<span style="color:#f92672">.</span>shape  <span style="color:#75715e"># 시간 단계, 어휘 크기</span>
</span></span><span style="display:flex;"><span>    extended_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate([[blank], np<span style="color:#f92672">.</span>repeat(labels, <span style="color:#ae81ff">2</span>)])
</span></span><span style="display:flex;"><span>    extended_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append(extended_labels, [blank])
</span></span><span style="display:flex;"><span>    S <span style="color:#f92672">=</span> len(extended_labels)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 로그 도메인에서의 알파 변수 초기화</span>
</span></span><span style="display:flex;"><span>    log_alpha <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones((T, S)) <span style="color:#f92672">*</span> <span style="color:#f92672">-</span>np<span style="color:#f92672">.</span>inf
</span></span><span style="display:flex;"><span>    log_alpha[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> log_probs[<span style="color:#ae81ff">0</span>, blank]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> S <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>        log_alpha[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> log_probs[<span style="color:#ae81ff">0</span>, extended_labels[<span style="color:#ae81ff">1</span>]]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 로그 도메인에서의 Forward 알고리즘</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, T):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> range(S):
</span></span><span style="display:flex;"><span>            current_label <span style="color:#f92672">=</span> extended_labels[s]
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 가능한 전이들을 고려</span>
</span></span><span style="display:flex;"><span>            possible_moves <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 현재 상태 유지</span>
</span></span><span style="display:flex;"><span>            possible_moves<span style="color:#f92672">.</span>append(log_alpha[t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, s])
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 이전 라벨에서 전이</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> s <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                possible_moves<span style="color:#f92672">.</span>append(log_alpha[t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, s<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 이전-이전 라벨에서 전이 (비blank→blank→비blank 제약 고려)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> s <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">and</span> extended_labels[s] <span style="color:#f92672">!=</span> blank <span style="color:#f92672">and</span> extended_labels[s] <span style="color:#f92672">!=</span> extended_labels[s<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>]:
</span></span><span style="display:flex;"><span>                possible_moves<span style="color:#f92672">.</span>append(log_alpha[t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, s<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 로그 도메인에서의 확률 합산</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> possible_moves:
</span></span><span style="display:flex;"><span>                log_alpha[t, s] <span style="color:#f92672">=</span> log_sum_exp(np<span style="color:#f92672">.</span>array(possible_moves)) <span style="color:#f92672">+</span> log_probs[t, current_label]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 최종 확률 계산 (마지막 라벨 또는 마지막 블랭크)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> log_sum_exp(np<span style="color:#f92672">.</span>array([log_alpha[T<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, S<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], log_alpha[T<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, S<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>]]) <span style="color:#66d9ef">if</span> S <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> log_alpha[T<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, S<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span></code></pre></div><h3 id="5-prefix-search-디코딩과-빔-서치">5. Prefix-Search 디코딩과 빔 서치<a hidden class="anchor" aria-hidden="true" href="#5-prefix-search-디코딩과-빔-서치">#</a></h3>
<p>CTC 모델의 디코딩은 최대 사후 확률(MAP) 추정을 통해 이루어진다:</p>
<p>$$\hat{\mathbf{Y}} = \arg\max_{\mathbf{Y}} P(\mathbf{Y}|\mathbf{X})$$</p>
<p>그러나 가능한 모든 출력 시퀀스를 탐색하는 것은 현실적으로 불가능하다. 따라서 여러 효율적인 디코딩 알고리즘이 사용된다:</p>
<h4 id="greedy-디코딩">Greedy 디코딩<a hidden class="anchor" aria-hidden="true" href="#greedy-디코딩">#</a></h4>
<p>가장 단순한 방법은 각 시간 단계에서 가장 확률이 높은 토큰을 선택하고 CTC 병합 규칙을 적용하는 것이다:</p>
<p>$$\pi^* = \arg\max_{\pi} \prod_{t=1}^{T} P(\pi_t|\mathbf{X}, t)$$
$$\hat{\mathbf{Y}} = \mathcal{B}(\pi^*)$$</p>
<h4 id="빔-서치-디코딩">빔 서치 디코딩<a hidden class="anchor" aria-hidden="true" href="#빔-서치-디코딩">#</a></h4>
<p>빔 서치는 여러 유망한 후보 경로를 동시에 유지하는 방법이다. CTC의 경우, 특수한 prefix 빔 서치가 사용된다:</p>
<ol>
<li>
<p>각 시간 단계 $t$에서, 확률이 높은 상위 $k$개의 prefix를 유지</p>
</li>
<li>
<p>각 prefix에 대해 두 확률 성분을 계산:</p>
<ul>
<li>$P^b(prefix, t)$: $t$까지의 경로 중 blank로 끝나는 경로의 확률</li>
<li>$P^{nb}(prefix, t)$: $t$까지의 경로 중 non-blank로 끝나는 경로의 확률</li>
</ul>
</li>
<li>
<p>확률 업데이트 규칙:</p>
<ul>
<li>$P^b(prefix, t) = P^b(prefix, t-1) \cdot P(blank|\mathbf{X}, t) + P^{nb}(prefix, t-1) \cdot P(blank|\mathbf{X}, t)$</li>
<li>$P^{nb}(prefix+c, t) = P^b(prefix, t-1) \cdot P(c|\mathbf{X}, t) + P^{nb}(prefix, t-1) \cdot P(c|\mathbf{X}, t)$ if $c \neq \text{last of prefix}$</li>
<li>$P^{nb}(prefix+c, t) = P^b(prefix, t-1) \cdot P(c|\mathbf{X}, t)$ if $c = \text{last of prefix}$</li>
</ul>
</li>
<li>
<p>각 시간 단계 이후, prefix의 총 확률은 $P(prefix, t) = P^b(prefix, t) + P^{nb}(prefix, t)$</p>
</li>
</ol>
<p>이러한 빔 서치 알고리즘은 복잡한 실제 시나리오에서 Greedy 디코딩보다 우수한 결과를 제공한다.</p>
<h3 id="6-ctc-loss의-이론적-분석과-한계">6. CTC Loss의 이론적 분석과 한계<a hidden class="anchor" aria-hidden="true" href="#6-ctc-loss의-이론적-분석과-한계">#</a></h3>
<p>CTC Loss는 수학적으로 특별한 성질과 몇 가지 한계를 가지고 있다:</p>
<h4 id="조건부-독립-가정">조건부 독립 가정<a hidden class="anchor" aria-hidden="true" href="#조건부-독립-가정">#</a></h4>
<p>CTC의 핵심 가정은 각 시간 단계의 출력이 다른 시간 단계와 조건부 독립이라는 것이다:</p>
<p>$$P(\pi|\mathbf{X}) = \prod_{t=1}^{T} P(\pi_t|\mathbf{X}, t)$$</p>
<p>이 가정은 모델이 시간적 의존성을 충분히 포착하지 못하게 한다. 특히 언어 모델링 관점에서, 이전 출력 토큰들의 정보를 활용하지 못한다.</p>
<h4 id="peak-분포-성질">Peak 분포 성질<a hidden class="anchor" aria-hidden="true" href="#peak-분포-성질">#</a></h4>
<p>CTC 학습의 이론적 분석에 따르면, 모델은 정확한 정렬에 높은 확률을 집중시키는 &ldquo;Peak Distribution&quot;을 형성하는 경향이 있다. 이로 인해 특정 정렬 패턴에 과도하게 집중하여 일반화 능력이 제한될 수 있다.</p>
<h4 id="가중치-불균형-문제">가중치 불균형 문제<a hidden class="anchor" aria-hidden="true" href="#가중치-불균형-문제">#</a></h4>
<p>blank 토큰과 non-blank 토큰 간의 확률 불균형 문제가 존재한다. 일반적으로 blank 토큰은 더 자주 출현하므로, 학습 과정에서 더 큰 영향력을 갖게 된다. 이는 학습 초기에 모델이 대부분 blank를 예측하는 &ldquo;blank 함정(blank trap)&rdquo; 현상으로 이어질 수 있다.</p>
<p>이러한 문제를 해결하기 위해 다양한 확장 및 변형 연구가 진행되었으며, 이 중 하나가 RNN-T이다.</p>
<h3 id="7-ctc-loss의-심도-깊은-수학적-분석">7. CTC Loss의 심도 깊은 수학적 분석<a hidden class="anchor" aria-hidden="true" href="#7-ctc-loss의-심도-깊은-수학적-분석">#</a></h3>
<h4 id="71-확률론적-프레임워크와-정보이론적-해석">7.1 확률론적 프레임워크와 정보이론적 해석<a hidden class="anchor" aria-hidden="true" href="#71-확률론적-프레임워크와-정보이론적-해석">#</a></h4>
<p>CTC Loss는 정보이론적 관점에서 크로스 엔트로피 최소화 문제로 볼 수 있다. 구체적으로, 다음과 같은 확률 분포 간의 크로스 엔트로피를 최소화한다:</p>
<p>$$H(P, Q) = -\sum_{y} P(y) \log Q(y)$$</p>
<p>여기서 $P$는 실제 라벨의 확률 분포이고, $Q$는 모델이 예측한 분포이다. 실제 라벨 $\mathbf{Y}$에 대해 $P(\mathbf{Y}) = 1$이고 다른 모든 라벨에 대해 $P = 0$이므로, 이는 결국 $-\log Q(\mathbf{Y})$를 최소화하는 문제가 된다. 이것이 바로 CTC Loss 함수이다:</p>
<p>$$\mathcal{L}_{CTC} = -\log P(\mathbf{Y}|\mathbf{X})$$</p>
<p>정보이론적 관점에서, 이 손실 함수는 모델이 타겟 시퀀스를 정확히 예측하는 데 필요한 &ldquo;놀람(surprise)&ldquo;의 양, 즉 정보량을 측정한다. 손실이 낮을수록, 모델은 타겟 시퀀스를 더 높은 확률로 예측한다.</p>
<h4 id="72-심층-미분-유도-과정">7.2 심층 미분 유도 과정<a hidden class="anchor" aria-hidden="true" href="#72-심층-미분-유도-과정">#</a></h4>
<p>CTC Loss의 미분을 더 상세히 살펴보자. 출력 로짓 $z_{t,k}$에 대한 CTC Loss의 편미분은 다음과 같이 유도된다:</p>
<div>
$$
\frac{\partial \mathcal{L}_{CTC}}{\partial z_{t,k}} = \frac{\partial}{\partial z_{t,k}}\left(-\log \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{Y})} \prod_{i=1}^{T} P(\pi_i|\mathbf{X}, i)\right)
$$
</div>
<p>연쇄 법칙(chain rule)을 적용하면:</p>
<div>
$$
\frac{\partial \mathcal{L}_{CTC}}{\partial z_{t,k}} = -\frac{1}{P(\mathbf{Y}|\mathbf{X})} \cdot \frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}}
$$
</div>
<p>그리고:</p>
<p>$$\frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}} = \frac{\partial}{\partial z_{t,k}} \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{Y})} \prod_{i=1}^{T} P(\pi_i|\mathbf{X}, i)$$</p>
<p>시간 단계 $t$에서만 $z_{t,k}$가 영향을 미치므로:</p>
<p>$$\frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}} = \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{Y})} \frac{\partial P(\pi_t|\mathbf{X}, t)}{\partial z_{t,k}} \prod_{i \neq t} P(\pi_i|\mathbf{X}, i)$$</p>
<p>소프트맥스 도함수를 적용하면:</p>
<p>$$\frac{\partial P(\pi_t = j|\mathbf{X}, t)}{\partial z_{t,k}} =
\begin{cases}
P(\pi_t = j|\mathbf{X}, t)(1 - P(\pi_t = j|\mathbf{X}, t)) &amp; \text{if } j = k \
-P(\pi_t = j|\mathbf{X}, t)P(\pi_t = k|\mathbf{X}, t) &amp; \text{if } j \neq k
\end{cases}$$</p>
<p>이를 정리하면:</p>
<p>$$\frac{\partial P(\mathbf{Y}|\mathbf{X})}{\partial z_{t,k}} = P(k|\mathbf{X}, t) \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{Y})} \prod_{i \neq t} P(\pi_i|\mathbf{X}, i) \left[ \mathbf{1}_{\pi_t = k} - P(k|\mathbf{X}, t) \right]$$</p>
<p>여기서 $\mathbf{1}_{\pi_t = k}$는 $\pi_t = k$일 때 1이고 그렇지 않으면 0인 지시 함수이다.</p>
<p>최종적으로, Forward-Backward 변수를 활용하면:</p>
<div>
$$
\frac{\partial \mathcal{L}_{CTC}}{\partial z_{t,k}} = P(k|\mathbf{X}, t) - \frac{1}{P(\mathbf{Y}|\mathbf{X})} \sum_{s: l_s = k} \alpha(t-1, s) \beta(t, s)
$$
</div>
<p>이 식은 두 항의 차로 구성된다:</p>
<ol>
<li>첫 번째 항 $P(k|\mathbf{X}, t)$는 현재 모델이 토큰 $k$에 할당하는 확률</li>
<li>두 번째 항은 모든 가능한 정렬 경로에서 시간 $t$에 토큰 $k$가 나타날 확률의 기대값</li>
</ol>
<p>훈련이 진행됨에 따라, 이 두 항은 점점 더 가까워지며, 이는 모델이 올바른 정렬을 학습하고 있음을 의미한다.</p>
<h4 id="73-blank-토큰의-역할에-대한-심층-이해">7.3 Blank 토큰의 역할에 대한 심층 이해<a hidden class="anchor" aria-hidden="true" href="#73-blank-토큰의-역할에-대한-심층-이해">#</a></h4>
<p>Blank 토큰은 CTC 학습에서 핵심적인 역할을 한다. 그 역할과 영향을 더 심층적으로 분석해보자.</p>
<h5 id="blank-토큰의-확률적-역학">Blank 토큰의 확률적 역학<a hidden class="anchor" aria-hidden="true" href="#blank-토큰의-확률적-역학">#</a></h5>
<p>Blank 토큰이 높은 확률을 갖는 몇 가지 상황이 있다:</p>
<ol>
<li><strong>반복 토큰 사이</strong>: 동일한 토큰이 연속적으로 등장해야 할 때 (예: &ldquo;HELLO&quot;에서 &ldquo;L&rdquo; 토큰 반복)</li>
<li><strong>연음 구간(transition regions)</strong>: 한 발음에서 다른 발음으로 전환되는 중간 지점</li>
<li><strong>무음 구간(silence regions)</strong>: 발화 사이의 조용한 부분</li>
</ol>
<p>학습 초기에는 모델이 정확한 정렬을 아직 학습하지 못했기 때문에, blank 토큰에 높은 확률을 할당하는 것이 &ldquo;안전한 전략&quot;이 된다. 이는 &ldquo;blank 함정(blank trap)&rdquo; 현상을 초래할 수 있다.</p>
<h5 id="blank-비율-통계-분석">Blank 비율 통계 분석<a hidden class="anchor" aria-hidden="true" href="#blank-비율-통계-분석">#</a></h5>
<p>실제 ASR 데이터셋에 대한 통계 분석에 따르면, 학습된 CTC 모델은 일반적으로 다음과 같은 blank 비율을 보인다:</p>
<ul>
<li>음성 프레임의 약 45-65%가 blank로 예측됨</li>
<li>단어 간 경계와 문장 시작/끝 부분에서 blank 비율이 증가 (최대 80-90%)</li>
<li>빠른 발음이나 명확한 조음에서는 blank 비율이 감소 (최소 30-40%)</li>
</ul>
<p>이러한 통계는 blank 토큰이 단순한 &ldquo;필러(filler)&rdquo; 이상의 역할을 한다는 것을 시사한다. Blank는 음성의 시간적 구조와 단어 경계에 대한 중요한 정보를 인코딩한다.</p>
<h5 id="blank-함정-극복을-위한-고급-기법">Blank 함정 극복을 위한 고급 기법<a hidden class="anchor" aria-hidden="true" href="#blank-함정-극복을-위한-고급-기법">#</a></h5>
<p>Blank 함정 문제를 완화하기 위한 몇 가지 고급 기법:</p>
<ol>
<li><strong>Blank 감소 손실(Blank Reduction Loss)</strong>: CTC Loss에 blank 확률을 명시적으로 페널티화하는 정규화 항 추가</li>
</ol>
<div>
   $$
   \mathcal{L}_{BR} = \mathcal{L}_{CTC} + \lambda \sum_{t=1}^{T} P(blank|\mathbf{X}, t)
   $$
</div>
<ol start="2">
<li><strong>스케줄링된 Blank 조정(Scheduled Blank Scaling)</strong>: 학습 초기에는 blank 로짓에 작은 스케일 팩터를 적용하고, 학습이 진행됨에 따라 이를 점진적으로 1로 증가</li>
</ol>
<div>
   $$
   z'_{t,blank} = \alpha_t \cdot z_{t,blank}
   $$
</div>
<p>여기서 $\alpha_t$는 시간에 따라 증가하는 스케일 팩터이다.</p>
<ol start="3">
<li>
<p><strong>Curriculum Learning</strong>: 쉬운 샘플(짧은 발화, 명확한 발음)부터 시작하여 점진적으로 어려운 샘플로 이동</p>
</li>
<li>
<p><strong>초기화 전략</strong>: 초기에 blank 로짓을 약간 낮은 값으로 초기화하여 학습 시작 시 non-blank 토큰에 더 많은 기회 제공</p>
</li>
</ol>
<h4 id="74-수치적-안정성의-이론적-근거">7.4 수치적 안정성의 이론적 근거<a hidden class="anchor" aria-hidden="true" href="#74-수치적-안정성의-이론적-근거">#</a></h4>
<p>CTC 계산의 수치적 안정성은 주로 로그 도메인 연산을 통해 달성된다. 그 이론적 근거를 심층적으로 살펴보자.</p>
<h5 id="로그-도메인-연산의-정밀도-분석">로그 도메인 연산의 정밀도 분석<a hidden class="anchor" aria-hidden="true" href="#로그-도메인-연산의-정밀도-분석">#</a></h5>
<p>컴퓨터의 부동 소수점 표현에서, 값 $x$의 상대 오차는 대략 $\epsilon \cdot |x|$이다 (여기서 $\epsilon$은 기계 입실론). 따라서 작은 확률값을 직접 곱하면 큰 상대 오차가 누적될 수 있다.</p>
<p>로그 도메인에서는:</p>
<ul>
<li>곱셈이 덧셈으로 변환됨: $\log(a \cdot b) = \log a + \log b$</li>
<li>덧셈이 로그-합-지수(LogSumExp) 연산으로 변환됨: $\log(a + b) = \log a + \log(1 + \exp(\log b - \log a))$ (만약 $a &gt; b$)</li>
</ul>
<p>이러한 변환은 수치적 언더플로우를 방지할 뿐만 아니라, 수치 정밀도도 향상시킨다. 로그 스케일에서의 덧셈은 원래 값에 비례하는 오차가 아닌, 절대적 오차를 가진다.</p>
<h5 id="로그-합-지수logsumexp-트릭의-엄밀한-증명">로그-합-지수(LogSumExp) 트릭의 엄밀한 증명<a hidden class="anchor" aria-hidden="true" href="#로그-합-지수logsumexp-트릭의-엄밀한-증명">#</a></h5>
<p>LogSumExp 트릭은 다음과 같이 표현된다:</p>
<p>$$\log \sum_{i=1}^{n} \exp(x_i) = m + \log \sum_{i=1}^{n} \exp(x_i - m)$$</p>
<p>이것이 올바르다는 것을 증명하자:</p>
<p>$$\begin{align}
m + \log \sum_{i=1}^{n} \exp(x_i - m) &amp;= m + \log \sum_{i=1}^{n} \frac{\exp(x_i)}{\exp(m)} \
&amp;= m + \log \left( \frac{1}{\exp(m)} \sum_{i=1}^{n} \exp(x_i) \right) \
&amp;= m + \log \sum_{i=1}^{n} \exp(x_i) - m \
&amp;= \log \sum_{i=1}^{n} \exp(x_i)
\end{align}$$</p>
<p>일반적으로 $m = \max_i x_i$로 설정하면, $\exp(x_i - m) \leq 1$이 보장되어 오버플로우가 방지된다. 또한, 최소한 하나의 항은 정확히 1이 되어 언더플로우도 방지된다.</p>
<h4 id="75-ctc-손실의-확률적-해석과-최적화-이론">7.5 CTC 손실의 확률적 해석과 최적화 이론<a hidden class="anchor" aria-hidden="true" href="#75-ctc-손실의-확률적-해석과-최적화-이론">#</a></h4>
<p>CTC 손실은 최대 우도 추정(Maximum Likelihood Estimation, MLE)의 프레임워크 내에서 이해할 수 있다. 이는 다음과 같은 최적화 문제로 표현된다:</p>
<p>$$\theta^* = \arg\max_{\theta} \prod_{i=1}^{N} P(\mathbf{Y}_i|\mathbf{X}_i; \theta)$$</p>
<p>로그를 취하면:</p>
<p>$$\theta^* = \arg\max_{\theta} \sum_{i=1}^{N} \log P(\mathbf{Y}_i|\mathbf{X}_i; \theta)$$</p>
<p>이는 경사 상승법(gradient ascent)으로 최적화될 수 있으며, 이는 CTC 손실의 최소화와 동등하다:</p>
<p>$$\theta^* = \arg\min_{\theta} \sum_{i=1}^{N} -\log P(\mathbf{Y}_i|\mathbf{X}_i; \theta)$$</p>
<h5 id="최적화-알고리즘과-수렴-특성">최적화 알고리즘과 수렴 특성<a hidden class="anchor" aria-hidden="true" href="#최적화-알고리즘과-수렴-특성">#</a></h5>
<p>CTC 손실 경사면(loss landscape)은 일반적으로 다음과 같은 특성을 가진다:</p>
<ol>
<li><strong>초기 학습 단계에서의 불안정성</strong>: 손실 함수가 가파르게 감소하며, 그레디언트의 분산이 크다</li>
<li><strong>Plateau 지역</strong>: 학습 중간 단계에서 손실이 느리게 감소하는 구간이 나타날 수 있다</li>
<li><strong>복잡한 국소 최적점 구조</strong>: 여러 국소 최적점이 존재할 수 있으며, 이는 다양한 가능한 정렬에 해당한다</li>
</ol>
<p>이러한 특성을 고려하여, CTC 모델 훈련에 효과적인 최적화 전략:</p>
<ol>
<li><strong>학습률 스케줄링</strong>: 초기에는 낮은 학습률로 시작하여 점차 증가시키고, 이후 점진적으로 감소</li>
<li><strong>그레디언트 클리핑</strong>: 특히 초기 학습 단계에서 그레디언트 폭발을 방지하기 위해 그레디언트 노름에 상한을 설정</li>
<li><strong>모멘텀 기반 최적화</strong>: Adam이나 RMSprop와 같은 적응적 모멘텀 기반 최적화 알고리즘을 사용하여 plateau 지역을 효과적으로 탈출</li>
<li><strong>배치 정규화</strong>: 훈련 안정성을 향상시키고 더 빠른 수렴을 촉진</li>
</ol>
<h5 id="학습률-워밍업과-스케줄링의-이론적-근거">학습률 워밍업과 스케줄링의 이론적 근거<a hidden class="anchor" aria-hidden="true" href="#학습률-워밍업과-스케줄링의-이론적-근거">#</a></h5>
<p>CTC 학습의 초기 단계에서는 그레디언트가 불안정하고 크기가 클 수 있다. 이러한 이유로, 학습률 워밍업이 효과적이다:</p>
<ol>
<li>매우 낮은 학습률(예: 1e-6)로 시작</li>
<li>$N_{warmup}$ 스텝 동안 학습률을 목표값(예: 1e-3 또는 1e-4)까지 선형적으로 증가</li>
<li>그 후, 코사인 또는 지수 감소 스케줄링을 적용</li>
</ol>
<p>이러한 접근법의 이론적 근거는 다음과 같다:</p>
<ul>
<li>초기 낮은 학습률: 무작위 초기화된 모델의 불안정한 그레디언트로 인한 발산 방지</li>
<li>점진적 증가: 모델이 더 안정적인 그레디언트를 생성하게 되면서 학습 속도 향상</li>
<li>후기 감소: 미세 조정 단계에서 국소 최적점 주변에서의 진동 감소</li>
</ul>
<p>실증적 연구에 따르면, 이러한 스케줄링 전략은 CTC 모델의 최종 성능을 1-2% 개선할 수 있다.</p>
<h3 id="8-rnn-t-ctc의-한계를-넘어선-수학적-접근">8. RNN-T: CTC의 한계를 넘어선 수학적 접근<a hidden class="anchor" aria-hidden="true" href="#8-rnn-t-ctc의-한계를-넘어선-수학적-접근">#</a></h3>
<h4 id="81-rnn-t의-수학적-형식화">8.1. RNN-T의 수학적 형식화<a hidden class="anchor" aria-hidden="true" href="#81-rnn-t의-수학적-형식화">#</a></h4>
<p>RNN-T(Recurrent Neural Network Transducer)는 CTC의 조건부 독립 가정을 극복하기 위해 개발된 방법이다. RNN-T는 이전 출력 토큰들의 정보를 활용하여 다음과 같이 확률 모델을 정의한다:</p>
<p>$$P(\mathbf{Y}|\mathbf{X}) = \sum_{\pi \in \mathcal{A}(\mathbf{Y})} P(\pi|\mathbf{X})$$</p>
<p>$$P(\pi|\mathbf{X}) = \prod_{i=1}^{|\pi|} P(\pi_i|\mathbf{X}, \mathbf{y}_{&lt;u(i)})$$</p>
<p>여기서 $u(i)$는 시간 단계 $i$까지 생성된 non-blank 토큰의 수이고, $\mathbf{y}_{&lt;u(i)}$는 이전에 생성된 모든 토큰을 나타낸다.</p>
<p>이러한 접근법은 각 예측이 이전 출력 토큰들에 조건부로 이루어진다는 점에서 CTC와 근본적으로 다르다.</p>
<h4 id="82-rnn-t의-아키텍처적-구성">8.2. RNN-T의 아키텍처적 구성<a hidden class="anchor" aria-hidden="true" href="#82-rnn-t의-아키텍처적-구성">#</a></h4>
<p>RNN-T는 다음과 같은 세 가지 핵심 컴포넌트로 구성된다:</p>
<ol>
<li>
<p><strong>인코더(Encoder)</strong>: 입력 음향 특징 $\mathbf{X}$를 처리하여 은닉 표현 $\mathbf{h}^{enc}$를 생성</p>
<p>$$\mathbf{h}^{enc}_t = \text{Encoder}(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_t)$$</p>
</li>
<li>
<p><strong>예측 네트워크(Prediction Network)</strong>: 이전 출력 토큰들을 처리하여 언어 컨텍스트 표현 $\mathbf{h}^{pred}$를 생성</p>
<div>
$$
\mathbf{h}^{pred}_u = \text{Prediction}(y_1, y_2, \ldots, y_{u-1})
$$
</div>
</li>
<li>
<p><strong>조인트 네트워크(Joint Network)</strong>: 인코더와 예측 네트워크의 출력을 결합하여 최종 확률 분포 생성</p>
<div>
$$
\mathbf{z}_{t,u} = \text{Joint}(\mathbf{h}^{enc}_t, \mathbf{h}^{pred}_u)
$$
</div>
</li>
</ol>
<div>
$$
P(k|\mathbf{X}, \mathbf{y}_<{u}, t, u) = {softmax}(\mathbf{z}_{t,u})_k
$$
</div>
<p>이러한 아키텍처는 인코더를 통한 음향 모델링과 예측 네트워크를 통한 언어 모델링을 결합하여, 보다 강력한 시퀀스 변환 모델을 형성한다.</p>
<h4 id="83-복잡한-forward-backward-알고리즘">8.3. 복잡한 Forward-Backward 알고리즘<a hidden class="anchor" aria-hidden="true" href="#83-복잡한-forward-backward-알고리즘">#</a></h4>
<p>RNN-T에서의 Forward-Backward 알고리즘은 CTC보다 더 복잡하다. 2차원 격자 구조에서 동작하며, 각 셀 $(t,u)$는 시간 단계 $t$와 출력 위치 $u$에 해당한다.</p>
<p>Forward 변수 $\alpha(t,u)$는 시간 $t$까지의 음향 입력과 이전 $u$개의 출력 토큰을 고려할 때, 가능한 모든 정렬 경로의 확률 합을 나타낸다. 이는 다음과 같이 재귀적으로 계산된다:</p>
<p><strong>초기화:</strong></p>
<ul>
<li>$\alpha(0,0) = 1$</li>
<li>$\alpha(t,u) = 0$ for $t &lt; 0$ or $u &lt; 0$</li>
</ul>
<p><strong>재귀식:</strong></p>
<div>
$$
\alpha(t,u) = \alpha(t-1,u) \cdot P(blank|\mathbf{X}, \mathbf{y}_<{u}, t-1, u) + \alpha(t,u-1) \cdot P(y_u|\mathbf{X}, \mathbf{y}_<{u}, t, u-1)
$$
</div>
<p>여기서 두 항은 각각:</p>
<ol>
<li>시간 단계를 소비하고 blank를 출력하는 경우 (수평 이동)</li>
<li>출력 토큰을 생성하는 경우 (수직 이동)</li>
</ol>
<p>마찬가지로 Backward 변수 $\beta(t,u)$도 2차원 격자에서 계산된다.</p>
<h4 id="84-rnn-t와-ctc의-수학적-비교">8.4 RNN-T와 CTC의 수학적 비교<a hidden class="anchor" aria-hidden="true" href="#84-rnn-t와-ctc의-수학적-비교">#</a></h4>
<p>RNN-T와 CTC의 핵심적인 수학적 차이점은 다음과 같다:</p>
<table>
<thead>
<tr>
<th>측면</th>
<th>CTC</th>
<th>RNN-T</th>
</tr>
</thead>
<tbody>
<tr>
<td>확률 모델</td>
<td>$P(\pi|\mathbf{X}) = \prod_{t=1}^{T} P(\pi_t|\mathbf{X}, t)$</td>
<td>$P(\pi|\mathbf{X}) = \prod_{i=1}^{|\pi|} P(\pi_i|\mathbf{X}, \mathbf{y}_{&lt;u(i)})$</td>
</tr>
<tr>
<td>조건부 독립성</td>
<td>각 시간 단계의 출력이 서로 독립</td>
<td>이전 출력 토큰들에 조건부로 의존</td>
</tr>
<tr>
<td>계산 복잡도</td>
<td>$O(T \cdot U)$</td>
<td>$O(T \cdot U \cdot |\mathcal{V}|)$</td>
</tr>
<tr>
<td>탐색 공간</td>
<td>1차원 (시간 축)</td>
<td>2차원 (시간 × 출력)</td>
</tr>
<tr>
<td>모델링 능력</td>
<td>제한적 언어 모델링</td>
<td>강력한 내장 언어 모델링</td>
</tr>
</tbody>
</table>
<p>이러한 수학적 차이점으로 인해 RNN-T는 특히 다음과 같은 상황에서 CTC보다 우수한 성능을 보인다:</p>
<ol>
<li>장문의 발화 인식</li>
<li>문맥 의존적 언어 패턴 인식</li>
<li>희소한 데이터 환경</li>
</ol>
<p>그러나 이러한 우수성은 더 높은 계산 복잡도와 더 많은 훈련 데이터 요구사항이라는 비용을 수반한다.</p>
<h2 id="결론">결론<a hidden class="anchor" aria-hidden="true" href="#결론">#</a></h2>
<p>CTC와 RNN-T는 음성 인식에서 정렬 문제에 대한 수학적으로 우아한 해결책을 제시했다. 두 방법 모두 장단점이 있으며, 실제 응용에서는 사용 사례와 계산 리소스에 따라 적절히 선택하거나 결합하는 것이 중요하다.</p>
<p>수학적 관점에서, CTC는 간결하고 계산 효율적인 반면, RNN-T는 더 표현력이 뛰어나지만 계산 복잡도가 높다. 두 방법 모두 기존의 HMM 기반 접근법과 달리, 엔드투엔드 학습이 가능하고 명시적인 정렬 정보 없이도 훈련될 수 있다는 중요한 장점을 가지고 있다.</p>
<p>음성 인식 시스템을 개발하면서 내가 배운 가장 중요한 교훈은, 이론적인 우아함과 실용적인 성능 사이의 균형이 중요하다는 것이다. 가장 좋은 손실 함수는 수학적으로 미려할 뿐만 아니라, 실제 응용 환경의 제약 조건에 적합해야 한다는 점이다.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>[1] Graves, A., et al. &ldquo;Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.&rdquo; ICML 2006. <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">https://www.cs.toronto.edu/~graves/icml_2006.pdf</a></li>
<li>[2] Graves, A. &ldquo;Sequence Transduction with Recurrent Neural Networks.&rdquo; ICML Workshop 2012. <a href="https://arxiv.org/abs/1211.3711">https://arxiv.org/abs/1211.3711</a></li>
<li>[3] He, Y., et al. &ldquo;Streaming End-to-end Speech Recognition For Mobile Devices.&rdquo; ICASSP 2019. <a href="https://arxiv.org/abs/1811.06621">https://arxiv.org/abs/1811.06621</a></li>
<li>[4] Hannun, A. &ldquo;Sequence Modeling with CTC.&rdquo; Distill, 2017. <a href="https://distill.pub/2017/ctc/">https://distill.pub/2017/ctc/</a></li>
<li>[5] Battenberg, E., et al. &ldquo;Exploring Neural Transducers for End-to-End Speech Recognition.&rdquo; ASRU 2017. <a href="https://arxiv.org/abs/1707.07413">https://arxiv.org/abs/1707.07413</a></li>
<li>[6] Seunghyun, SEO. &ldquo;CTC Beam Search Decoding.&rdquo; <a href="https://seunghyunseo.github.io/speech/2021/10/20/ctc_beam_search/">https://seunghyunseo.github.io/speech/2021/10/20/ctc_beam_search/</a></li>
</ul>
<!-- 이미지 참고 사항 -->
<!-- 1. "CTC 확률 그래프" - CTC 경로와 라벨 매핑의 확률적 관계를 보여주는 다이어그램, ICML 2006 논문 참고 -->
<!-- 2. "Forward-Backward 변수 계산" - 동적 프로그래밍 테이블과 계산 과정 시각화 -->
<!-- 3. "RNN-T 아키텍처" - 인코더, 예측 네트워크, 조인트 네트워크의 수학적 관계 다이어그램 -->
<!-- 4. "CTC vs RNN-T 격자 구조" - 두 방식의 계산 격자 비교 시각화 -->

        </div>
        <div class="post-reward">
            <div style="padding: 0 0 0 0; margin: 0 0 0 0; width: 100%; font-size:16px; text-align: center;">
                <div id="QR" style="opacity: 0;">
                    <div id="wechat" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="wechat_qr" src="https://macsim2.github.io/img/wechat_pay.png" alt="wechat_pay"></a>
                        <p>微信</p>
                    </div>
                    <div id="alipay" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="alipay_qr" src="https://macsim2.github.io/img/alipay.png" alt="alipay"></a>
                        <p>支付宝</p>
                    </div>
                </div>
                
            </div>
        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://macsim2.github.io/posts/life/test/">
    <span class="title">« 이전 페이지</span>
    <br>
    <span>Test</span>
  </a>
  <a class="next" href="https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/">
    <span class="title">다음 페이지 »</span>
    <br>
    <span>TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on twitter"
       href="https://twitter.com/intent/tweet/?text=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f&amp;hashtags=deeplearning%2cASR%2cspeechrecognition%2cCTC%2cRNN-T%2csequencemodeling">
    <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on linkedin"
       href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f&amp;title=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac&amp;summary=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac&amp;source=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on reddit"
       href="https://reddit.com/submit?url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f&title=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on facebook"
       href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on whatsapp"
       href="https://api.whatsapp.com/send?text=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac%20-%20https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리 on telegram"
       href="https://telegram.me/share/url?text=CTC%20Loss%ec%99%80%20RNN-T%20Loss%ec%9d%98%20%eb%82%b4%eb%b6%80%20%ea%b5%ac%ec%a1%b0%ec%99%80%20%eb%8f%99%ec%9e%91%20%ec%9b%90%eb%a6%ac&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2fasr%2fctc_loss%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

        </footer>
    </div>

<div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://basic-18.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: '👉展开评论';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: '👇关闭评论';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js">
    </script>
    <script>
        twikoo.init({
            envId:  null ,
        el: "#tcomment",
            lang: 'en-US',
            region:  null ,
        path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        })
    </script>
</div>

</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2020-2025
        <a href="https://macsim2.github.io/" style="color:#939393;">macsim&#39;s Blog</a>
        All Rights Reserved
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;"></a>&nbsp;
    <span>
        <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=null"
           style="display:inline-block;text-decoration:none;height:20px;color:#939393;">
            <img src="" style="float:left;margin: 0px 5px 0px 0px;"/>
            
        </a>
    </span>
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '복사';

        function copyingDone() {
            copybutton.innerText = '복사 완료!';
            setTimeout(() => {
                copybutton.innerText = '복사';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>
</body>

</html>
