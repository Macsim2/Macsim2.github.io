<!DOCTYPE html>
<html lang="ko" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 | macsim&#39;s Blog</title>
<meta name="keywords" content="deeplearning, tensorrt, nvidia triton, inference optimization, model deployment, gpu acceleration">
<meta name="description" content="NVIDIA TensorRT와 Triton Inference Server의 내부 구조와 실전 최적화 경험을 공유합니다">
<meta name="author" content="macsim">
<link rel="canonical" href="https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://macsim2.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://macsim2.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://macsim2.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://macsim2.github.io/img/Q.gif">
<link rel="mask-icon" href="https://macsim2.github.io/img/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="ko" href="https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  

<meta property="og:title" content="TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합" />
<meta property="og:description" content="NVIDIA TensorRT와 Triton Inference Server의 내부 구조와 실전 최적화 경험을 공유합니다" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-30T15:30:00+09:00" />
<meta property="article:modified_time" content="2024-07-30T15:30:00+09:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합"/>
<meta name="twitter:description" content="NVIDIA TensorRT와 Triton Inference Server의 내부 구조와 실전 최적화 경험을 공유합니다"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "posts",
          "item": "https://macsim2.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "👨🏻‍💻 tech",
          "item": "https://macsim2.github.io/posts/tech/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합",
      "item": "https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합",
  "name": "TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합",
  "description": "NVIDIA TensorRT와 Triton Inference Server의 내부 구조와 실전 최적화 경험을 공유합니다",
  "keywords": [
    "deeplearning", "tensorrt", "nvidia triton", "inference optimization", "model deployment", "gpu acceleration"
  ],
  "articleBody": "AI 모델을 배포해보려 공부해본 엔지니어라면 누구나 이런 고민을 해봤을 것이다. “어떻게 하면 내 모델이 프로덕션 환경에서 더 빠르게 동작할 수 있을까?” 처음 이런 task를 접했을 때는 단순 C/C++ 프로그래밍을 통해서 해결할 수 있을 줄 알았다. 그러나, 물론 naive한 python 보다야 낫지만, request가 많아질 수록 다른 해결책이 필요하다고 생각이 들 것이다.\n그러던 와중에 나온 NVIDIA의 TensorRT 라는 놈이 있다. 오늘은 모델 최적화와 프로덕션 배포의 세계에서 강력한 도구로 자리잡은 TensorRT와 Triton Inference Server에 대해서 알아보자.\n이 글에서는 단순한 “TensorRT 사용법\"이나 “Triton 서버 설정법\"이 아닌, 이 두 기술의 내부 구조, 최적화 원리, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 얘기해보고자 한다.\nTensorRT란? TensorRT는 NVIDIA에서 개발한 고성능 딥러닝 추론 최적화 라이브러리로, 딥러닝 모델을 NVIDIA GPU에서 최대 성능으로 실행할 수 있도록 해준다. 이런 설명을 듣고 생각하면 일종의 CUDA 가속 추론 라이브러리 구나 생각이 들고, 으레 짐작할 수 있지만, TensorRT는 그 이상의 가치를 제공한다.\nPyTorch나 TensorFlow에서 모델을 학습시킬 때, 이 프레임워크들은 유연성과 학습 편의성에 초점을 맞추고 있다. 그러나 프로덕션 환경에서는 순수한 추론 성능과 효율성이 훨씬 중요해진다. 여기서 TensorRT가 등장한다.\nTensorRT의 핵심은 **계산 그래프 최적화(Computational Graph Optimization)**와 **하드웨어 최적화 커널(Hardware-Optimized Kernels)**이다. 여기서 부터 먼가 머리가 아파올 수 있다.. 계산 그래프를 최적화 하고 하드웨어 최적화 커널 이라니..? 무시무시한 최적화 방법인 것 같아 보인다. TensorRT는 모델의 구조를 분석하고 NVIDIA GPU에 최적화된 형태로 변환하여, 때로는 원본 모델보다 5-10배까지 빠른 추론 속도를 달성할 수 있다.\nTensorRT의 마법: 내부 최적화 메커니즘 그렇다면 어떻게 TensorRT가 이렇게 극적인 성능 향상을 가능하게 하는지 조금 더 구체적으로 살펴보자.\n1. 그래프 최적화 기법 TensorRT는 신경망 모델을 최적화하기 위해 여러 그래프 변환 기법을 사용한다:\n레이어 융합(Layer Fusion): 여러 레이어를 단일 최적화된 레이어로 결합 커널 튜닝(Kernel Auto-Tuning): 하드웨어에 맞게 최적의 커널 구현 선택 텐서 메모리 최적화: 메모리 사용량을 최소화하고 캐시 효율성 극대화 불필요한 연산 제거: 추론 시 필요없는 계산 생략 내 경험상 레이어 융합은 가장 극적인 성능 개선을 가져오는 최적화 중 하나였다. 특히 Convolution + BatchNorm + ReLU와 같은 일반적인 패턴은 단일 최적화 커널로 대체되어 메모리 액세스를 크게 줄일 수 있었다. 나중에 OpenAI의 Triton에 대해서도 posting 하려고 하는데 여기서의 메모리 액세스 를 줄인다는게 최적화 과정의 아주 큰 요소이다. 이 메모리 액세스를 어떻게 줄인것인가에 많은 연구자들이 목을 메고 있다.\nNVIDIA의 연구[7]에 따르면, 레이어 융합을 통해 중간 텐서의 메모리 액세스가 50% 이상 감소하고, 이는 대규모 모델에서 최대 25%의 추론 성능 향상으로 이어진다고 한다. Han 등의 연구[6]에서는 메모리 액세스 감소가 모바일 환경에서 에너지 효율성을 최대 3배까지 개선할 수 있음을 보여준다고 한다.\n// TensorRT의 레이어 융합 예시 (의사 코드) // 이런 세 개의 레이어가 있다고 가정해보자 IConvolutionLayer* conv = network-\u003eaddConvolution(...); IBatchNormLayer* bn = network-\u003eaddBatchNorm(...); IActivationLayer* relu = network-\u003eaddActivation(*bn-\u003egetOutput(0), ActivationType::kRELU); // TensorRT는 내부적으로 이를 단일 최적화 커널로 융합한다 // network-\u003eaddFusedConvBNRelu(...); (실제 API는 아님) 2. 정밀도 최적화(Precision Optimization) TensorRT의 가장 강력한 기능 중 하나는 **혼합 정밀도 연산(Mixed Precision Computing)**이다. 다양한 수치 정밀도를 지원한다:\nFP32: 32비트 단정밀도 부동 소수점 FP16: 16비트 반정밀도 부동 소수점 INT8: 8비트 정수 양자화 TF32: NVIDIA Ampere GPU부터 지원하는 19비트 텐서 부동 소수점 포맷 실제 프로젝트에서, FP16으로 전환하는 것만으로도 거의 정확도 손실 없이 2배 이상의 성능 향상을 얻을 수 있었다. INT8로 더 나아가면 대략 4배까지 처리속도가 향상될 수 있지만, Quantization 과정에서 정확도 저하가 있기에 trade-off를 따져야 했다.\n# TensorRT의 정밀도 설정 예시 import tensorrt as trt # builder 설정 builder = trt.Builder(TRT_LOGGER) network = builder.create_network(1 \u003c\u003c int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) config = builder.create_builder_config() # FP16 정밀도 활성화 config.set_flag(trt.BuilderFlag.FP16) # INT8 양자화 활성화 (보정 필요) config.set_flag(trt.BuilderFlag.INT8) config.int8_calibrator = calibrator # 보정기 인스턴스 # 엔진 생성 engine = builder.build_engine(network, config) 3. 동적 텐서 메모리와 실행 최적화 TensorRT는 실행 중에 텐서 메모리 할당을 최적화하기 위해 동적 메모리 관리를 사용한다. 이게 무슨 의미일까? 쉽게 설명하자면, 신경망이 연산을 수행할 때 필요한 메모리를 더 효율적으로 사용하는 방법들을 적용한다는 의미다. 여기에는 세 가지 핵심 전략이 있다.\n메모리 풀링(Memory Pooling) 기존 방식에서는 모델이 연산을 수행할 때마다 메모리를 할당하고 해제하는 과정을 반복했다. 이 과정은 GPU에게 “이 크기의 메모리가 필요해”, “이제 이 메모리는 필요 없어” 라고 계속 말하는 것과 같은데, 이는 상당한 오버헤드를 발생시킨다.\n메모리 풀링에서는 TensorRT가 미리 대량의 메모리 풀을 할당해두고, 필요할 때마다 이 풀에서 빠르게 메모리를 가져와 사용한다. 마치 공유 오피스에서 매번 책상을 사고 버리는 대신, 미리 준비된 책상을 필요할 때 사용하는 것과 같다.\n# 기존 방식 (의사 코드) for each_operation in model: memory = allocate_new_memory(size_needed) # 시간 소요 perform_operation_using(memory) free_memory(memory) # 또 시간 소요 # 메모리 풀링 방식 (의사 코드) memory_pool = allocate_large_pool_once() # 초기에 한 번만 비용 지불 for each_operation in model: memory = get_from_pool(memory_pool, size_needed) # 매우 빠름 perform_operation_using(memory) return_to_pool(memory) # 해제가 아닌 반환 (빠름) 텐서 재사용(Tensor Reuse) 딥러닝 모델 내부에서는 많은 중간 결과(텐서)가 생성된다. 이들은 보통 비슷한 크기를 가지는 경우가 많고, 한 번 사용된 후에는 더 이상 필요하지 않은 경우가 많다.\nTensorRT는 똑똑하게도 “이 텐서의 데이터는 더 이상 필요 없으니, 이 메모리 공간을 다른 비슷한 크기의 텐서를 위해 재사용할 수 있겠군!“이라고 판단한다. 이는 특히 메모리가 제한적인 엣지 디바이스에서 중요하다.\n예를 들어, ResNet과 같은 모델에서는 각 레이어의 출력 텐서가 비슷한 크기를 갖는다. 1번 레이어의 결과가 2번 레이어에 전달된 후에는 1번 레이어의 출력 메모리 공간을 3번 레이어의 출력을 저장하는 데 재사용할 수 있다.\n[초기 상태] Layer 1 출력 → 메모리 A 사용 Layer 2 출력 → 메모리 B 사용 Layer 3 출력 → 새 메모리 필요... [텐서 재사용 적용] Layer 1 출력 → 메모리 A 사용 Layer 2 출력 → 메모리 B 사용 Layer 3 출력 → 메모리 A 재사용 (Layer 1 출력은 이미 Layer 2에 전달됨) 실행 병렬화(Execution Parallelism) 그래프 관점에서 보면, 딥러닝 모델은 일부 연산들이 서로 독립적이라 동시에 실행될 수 있다. TensorRT는 이러한 독립적 연산들을 찾아내 병렬로 실행한다.\n예를 들어, 멀티 헤드 어텐션에서 각 “헤드\"는 독립적으로 계산될 수 있다. 또한 모델의 여러 브랜치 (예: ResNet의 residual connection과 main path)도 병렬로 처리될 수 있다.\nNVIDIA의 연구[7]에서는 다중 스트림 병렬화가 특히 Transformer 모델에서 효과적임을 보여주었다. 각 어텐션 헤드를 별도의 CUDA 스트림에 할당하는 기법만으로도 약 40%의 처리량 증가를 달성했다고 한다.\n// TensorRT의 작업 스케줄링 최적화 (의사 코드) // 실제 구현은 더 복잡하지만, 개념적으로는 이런 방식 // 독립적인 두 연산 병렬 실행 // A와 B는 서로 의존성이 없다고 가정 (예: 두 개의 독립된 컨볼루션) executeParallel([\u0026]() { executeOperation(operationA); // GPU의 일부 코어에서 실행 }, [\u0026]() { executeOperation(operationB); // 동시에 다른 코어에서 실행 }); // C는 A와 B의 결과에 의존 (예: A와 B의 결과를 합치는 연산) // A와 B가 모두 완료될 때까지 기다린 후 실행 executeOperation(operationC); 이런 최적화 방법을 통해 지연 시간이 감소 되고, 그에 따라 처리량도 자연스레 증가 할 수 있을것이다.\n실전 TensorRT 모델 변환과 최적화 실제 프로젝트에서 TensorRT를 사용할 때 알아야 할 중요한 워크플로우와 실무 팁을 살펴보자.\n1. 모델 변환 파이프라인 TensorRT 모델 변환은 일반적으로 다음 단계를 따른다:\n원본 모델 내보내기: PyTorch/TensorFlow 모델을 ONNX 또는 UFF 포맷으로 변환 TensorRT 엔진 빌드: ONNX/UFF 모델을 TensorRT 엔진으로 최적화 정밀도 선택 및 보정: 필요시 INT8 양자화를 위한 보정 수행 엔진 직렬화(Serialization): 최적화된 엔진을 디스크에 저장하여 로딩 시간 단축 ONNX를 통한 변환이 가장 안정적이고 지원되는 경로였지만, PyTorch 모델에 따라 ONNX 변환 과정에서 여러 난관을 마주치기도 했다.\n# PyTorch에서 TensorRT 변환 예시 import torch import tensorrt as trt # 1. ONNX로 내보내기 torch.onnx.export( model, dummy_input, \"model.onnx\", opset_version=13, do_constant_folding=True, input_names=[\"input\"], output_names=[\"output\"], dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}} ) # 2. TensorRT 엔진 생성 TRT_LOGGER = trt.Logger(trt.Logger.WARNING) builder = trt.Builder(TRT_LOGGER) network = builder.create_network(1 \u003c\u003c int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) parser = trt.OnnxParser(network, TRT_LOGGER) with open(\"model.onnx\", \"rb\") as f: parser.parse(f.read()) config = builder.create_builder_config() config.max_workspace_size = 1 \u003c\u003c 30 # 1GB engine = builder.build_engine(network, config) # 3. 엔진 저장 with open(\"model.trt\", \"wb\") as f: f.write(engine.serialize()) 2. 동적 형상(Dynamic Shapes) 처리의 함정 TensorRT 7.0부터 동적 입력 크기가 잘 지원되지만, 최적의 성능을 얻기 위해서는 **최적화 프로필(Optimization Profiles)**을 신중하게 설정해야 한다.\nNLP 모델을 배포했을 때 가변 길이 시퀀스를 효율적으로 처리하는 것이 큰 도전이었다. 예상 시퀀스 길이 범위에 맞게 최적화 프로필을 구성했을 때 가장 좋은 결과를 얻을 수 있었다.\n# 동적 형상을 위한 최적화 프로필 설정 config = builder.create_builder_config() profile = builder.create_optimization_profile() # 최소, 최적, 최대 입력 크기 지정 profile.set_shape( \"input\", # 입력 텐서 이름 (1, 3, 224, 224), # 최소 크기 (8, 3, 224, 224), # 최적 크기 (가장 자주 사용될 것으로 예상) (16, 3, 224, 224) # 최대 크기 ) config.add_optimization_profile(profile) 또한 다양한 배치 크기에 최적화된 다중 프로필을 사용하여 성능을 더욱 향상시킬 수 있었다.\n3. INT8 양자화와 보정의 과학 FP16보다 더 극적인 성능 향상을 위해 INT8 양자화를 적용할 수 있지만, 이는 세심한 보정(Calibration) 과정이 필요하다:\n보정 데이터셋 준비: 실제 데이터의 통계적 특성을 대표하는 샘플 준비 보정기(Calibrator) 구현: 다양한 보정 알고리즘 중 선택 (엔트로피, 최소-최대 등) 보정 실행: 보정 데이터를 통해 모델의 활성화 범위 분석 정확도 검증: 양자화 후 정확도 손실 평가 NVIDIA 개발자 블로그와 TVM 프로젝트 연구[8]에 따르면, 양자화 방식 중 특히 엔트로피 보정 방식이 가장 낮은 정보 손실로 양자화를 수행할 수 있어 정확도 측면에서 우수한 결과를 제공한다. 이 방법은 활성화 분포를 고려하여 양자화 스케일을 동적으로 조정하기 때문에 이미지 분류 모델에서 특히 효과적이다.\n# TensorRT INT8 보정 예시 import tensorrt as trt import numpy as np # 보정기 클래스 정의 class EntropyCalibrator(trt.IInt8EntropyCalibrator2): def __init__(self, calibration_data, batch_size, input_name): super().__init__() self.data = calibration_data self.batch_size = batch_size self.input_name = input_name self.current_idx = 0 self.device_input = cuda.mem_alloc(self.data[0].nbytes * self.batch_size) def get_batch_size(self): return self.batch_size def get_batch(self, names): if self.current_idx \u003e= len(self.data): return None batch = self.data[self.current_idx:self.current_idx+self.batch_size] self.current_idx += self.batch_size cuda.memcpy_htod(self.device_input, np.ascontiguousarray(batch)) return [self.device_input] def read_calibration_cache(self): # 이전 보정 캐시 읽기 (있는 경우) return None def write_calibration_cache(self, cache): # 보정 결과 저장 return None # 보정기 사용 calibration_data = [...] # 보정 데이터 준비 calibrator = EntropyCalibrator(calibration_data, batch_size=8, input_name=\"input\") config.int8_calibrator = calibrator config.set_flag(trt.BuilderFlag.INT8) 실제로 대규모 모델의 경우, INT8 양자화를 통해 최대 4배의 처리량 향상을 경험했지만, 이는 약간의 정확도 희생을 동반했다. 트레이드오프가 용인되는 비즈니스 시나리오에서만 활용했다.\nNVIDIA Triton Inference Server: 확장 가능한 모델 서빙의 핵심 TensorRT가 단일 모델의 성능을 최적화하는 데 초점을 맞춘다면, Triton Inference Server는 대규모 프로덕션 환경에서의 모델 서빙을 처리한다.\n1. Triton의 아키텍처와 주요 기능 Triton Inference Server는 다양한 프레임워크와 하드웨어에서 추론 서비스를 제공하기 위한 컨테이너화된 솔루션이다. NVIDIA 개발자 컨퍼런스[7]에서 소개된 Triton의 주요 기능은 다음과 같다:\n다중 프레임워크 지원: TensorRT, ONNX Runtime, TensorFlow, PyTorch 등 동적 배치 처리: 요청을 자동으로 배치화하여 처리량 최적화 동시 모델 실행: 여러 모델을 동시에 서빙 모델 앙상블: 복잡한 추론 파이프라인 구성 모델 버전 관리: 무중단 업데이트 지원 하드웨어 리소스 관리: GPU 메모리와 계산 자원 할당 최적화 2. 모델 리포지토리 구성과 최적화 Triton은 모델 리포지토리라는 표준화된 디렉토리 구조를 통해 모델을 관리한다. 이 구조는 각 모델의 다양한 버전과 구성을 체계적으로 관리할 수 있게 해준다.\nmodel_repository/ ├── model1/ │ ├── config.pbtxt # 모델 구성 파일 │ └── 1/ # 버전 1 │ └── model.plan # TensorRT 엔진 ├── model2/ │ ├── config.pbtxt │ ├── 1/ # 버전 1 │ │ └── model.onnx # ONNX 모델 │ └── 2/ # 버전 2 │ └── model.onnx # 업데이트된 ONNX 모델 └── ensemble_model/ ├── config.pbtxt # 앙상블 구성 └── 1/ # 버전 1 3. 동적 배치 처리와 수평적 확장 Triton의 강력한 기능 중 하나는 **동적 배치 처리(Dynamic Batching)**이다. 이는 개별 추론 요청을 자동으로 배치화하여 처리량을 극대화한다.\n# config.pbtxt의 동적 배치 설정 예시 name: \"resnet50\" platform: \"tensorrt_plan\" max_batch_size: 128 dynamic_batching { preferred_batch_size: [8, 16, 32, 64] max_queue_delay_microseconds: 5000 } NVIDIA의 연구[7]와 실제 테스트에 따르면, 적절한 배치 크기와 대기 시간 설정은 GPU 활용도를 최대 95%까지 높일 수 있는 것으로 나타났다. 이 설정을 통해 Triton은 최대 5ms 동안 요청을 대기시키면서 선호하는 배치 크기로 그룹화하여 처리한다.\n또한 Kubernetes와 같은 환경에서 Triton 서버를 수평적으로 확장하여 대규모 트래픽을 처리할 수 있었다:\n# Kubernetes 배포 예시 (간략화) apiVersion: apps/v1 kind: Deployment metadata: name: triton-server spec: replicas: 5 # 서버 인스턴스 수 selector: matchLabels: app: triton template: metadata: labels: app: triton spec: containers: - name: triton-server image: nvcr.io/nvidia/tritonserver:22.07-py3 args: [\"tritonserver\", \"--model-repository=/models\"] resources: limits: nvidia.com/gpu: 1 # 인스턴스당 GPU 수 volumeMounts: - name: model-volume mountPath: /models volumes: - name: model-volume persistentVolumeClaim: claimName: model-pvc 실전 TensorRT와 Triton 적용 사례와 도전 실제 프로젝트에서 TensorRT와 Triton을 적용하면서 마주친 다양한 도전과 해결책을 살펴보자.\n1. 복잡한 아키텍처 모델의 변환 도전 최신 AI 연구에서 나온 복잡한 아키텍처의 모델을 TensorRT로 변환하는 과정은 종종 도전적이었다. 특히 트랜스포머 기반 모델의 경우, 동적 형상과 함께 변환하는 것이 까다로웠다.\n예를 들어, BERT 모델을 TensorRT로 최적화할 때는 다음과 같은 접근 방식이 도움이 되었다:\n플러그인 활용: NVIDIA의 TensorRT 플러그인 사용 (예: Multi-Head Attention) 퓨전 허용 패턴: 특정 연산 시퀀스를 TensorRT가 최적화할 수 있도록 모델 구조 조정 최적의 ONNX opset 버전 선택: 모델 구조에 맞는 적절한 ONNX opset 사용 # BERT용 TensorRT 최적화 설정 예시 config = builder.create_builder_config() config.max_workspace_size = 4 \u003c\u003c 30 # 4GB # 플러그인 활용 plugin_registry = trt.get_plugin_registry() bert_plugin_creator = plugin_registry.get_plugin_creator(\"CustomBertPlugin\", \"1\", \"\") plugin = bert_plugin_creator.create_plugin(\"bert_plugin\", plugin_params) # 네트워크에 플러그인 레이어 추가 plugin_layer = network.add_plugin_v2([...], plugin) 2. 추론 지연 시간 vs 처리량 최적화 실시간 추론이 필요한 애플리케이션(예: 자율주행)과 대량 배치 처리가 필요한 애플리케이션(예: 오프라인 영상 분석) 사이에는 상충관계가 있다.\n지연 시간 최적화를 위해서는:\n작은 배치 크기 사용 스트림 병렬성 최소화 정확한 워크로드에 맞는 프로필 최적화 처리량 최적화를 위해서는:\n큰 배치 크기 활용 다중 CUDA 스트림으로 병렬 처리 메모리 사용량과 계산 효율성 균형 서비스의 SLA(Service Level Agreement)에 따라 이 두 목표 사이의 적절한 균형점을 찾아야 했다.\n# Triton에서 지연 시간 vs 처리량 최적화 설정 예시 instance_group { count: 2 # GPU당 모델 인스턴스 수 kind: KIND_GPU gpus: [0] } # 지연 시간 최적화 dynamic_batching { max_queue_delay_microseconds: 100 # 매우 짧은 대기 시간 } # 처리량 최적화 dynamic_batching { preferred_batch_size: [64, 128] max_queue_delay_microseconds: 5000 # 더 긴 대기 시간 허용 } 3. 메모리 최적화와 다중 모델 배포 대규모 멀티 모델 시스템에서는 GPU 메모리 관리가 중요한 도전 과제였다. Triton에서는 다음과 같은 전략을 사용했다:\n인스턴스 그룹 최적화: GPU당 적절한 모델 인스턴스 수 설정 모델 로드 정책: 필요시에만 메모리에 로드하는 정책 사용 우선순위 스케줄링: 중요한 모델에 리소스 우선 할당 # config.pbtxt의 메모리 최적화 설정 instance_group { count: 1 kind: KIND_GPU gpus: [0] } # 메모리 효율적인 로드 정책 model_transaction_policy { decoupled: false } dynamic_batching { ... } # 필요시에만 모델 로드 model_control_mode: EXPLICIT 특히 여러 모델을 동시에 서빙해야 하는 경우, 모델별 리소스 할당을 세심하게 조정하여 전체 시스템 성능을 최적화할 수 있었다.\n실전 디버깅과 성능 분석 도구 TensorRT와 Triton을 효과적으로 활용하려면 강력한 디버깅 및 성능 분석 도구가 필수적이다.\n1. NVIDIA Nsight Systems NVIDIA Nsight Systems는 GPU 작업의 타임라인을 시각화하여 병목 현상을 식별하는 데 유용하다. 특히 호스트와 디바이스 간의 동기화 이슈나 커널 실행 패턴을 분석하는 데 탁월했다.\n# Nsight Systems로 Triton 서버 프로파일링 nsys profile -t cuda,nvtx -o profile_report --capture-range=cudaProfilerApi \\ tritonserver --model-repository=/models 2. TensorRT trtexec 도구 trtexec는 TensorRT 엔진을 벤치마킹하고 분석하기 위한 명령줄 도구로, 다양한 설정에서 모델 성능을 빠르게 평가할 수 있다.\n# trtexec을 사용한 성능 벤치마킹 trtexec --onnx=model.onnx \\ --saveEngine=model.trt \\ --fp16 \\ --verbose \\ --shapes=input:8x3x224x224 \\ --iterations=100 \\ --avgRuns=10 이 도구를 통해 다양한 배치 크기, 정밀도 및 최적화 설정에 따른 성능 차이를 체계적으로 측정할 수 있었다.\n3. Triton 모델 분석기 Triton은 배포된 모델의 성능을 분석하기 위한 perf_analyzer 도구를 제공한다:\n# Triton 모델 성능 분석 perf_analyzer -m resnet50 \\ -u localhost:8000 \\ -i grpc \\ --concurrency-range 1:16 \\ --shape input:3,224,224 이 도구를 통해 동시성 수준에 따른 지연 시간과 처리량을 측정하고, 최적의 서버 구성을 결정할 수 있었다.\nTensorRT와 Triton의 현실적인 한계와 대안 TensorRT와 Triton은 강력하지만, 몇 가지 제한 사항도 존재한다:\n1. 모델 호환성 도전 모든 모델이 TensorRT로 완벽하게 변환되지는 않는다. 특히:\n커스텀 연산자: 표준 연산자가 아닌 경우 플러그인 개발 필요 동적 제어 흐름: 조건문과 루프가 복잡한 모델의 경우 변환 어려움 최신 아키텍처: 최신 연구 모델은 지원이 지연될 수 있음 이런 경우의 대안으로, 복잡한 부분만 PyTorch로 유지하고 나머지를 TensorRT로 최적화하는 하이브리드 접근법을 사용했다.\n2. 개발 편의성 vs 최적화 수준 TensorRT는 학습 프레임워크에 비해 개발 편의성이 떨어진다. 모델 최적화에 소요되는 엔지니어링 시간과 얻을 수 있는 성능 향상 사이의 균형을 고려해야 한다.\n간단한 경우, PyTorch 2.0의 torch.compile()과 같은 기능이 적절한 대안이 될 수 있었다.\n3. 멀티 클라우드 및 이종 환경 지원 NVIDIA GPU에 최적화된 TensorRT는 다른 하드웨어 환경에서는 활용할 수 없다. 멀티 클라우드 전략이나 이종 하드웨어 환경을 고려한다면, Apache TVM이나 ONNX Runtime과 같은 대안을 함께 검토해야 했다.\n결론 TensorRT와 Triton Inference Server는 딥러닝 모델의 프로덕션 배포에 있어 강력한 도구이다. 적절한 최적화와 구성을 통해 극적인 성능 향상과 비용 절감을 달성할 수 있지만, 이는 깊은 이해와 체계적인 접근을 필요로 한다. 결국은 많이 사용해보고 공부해야 적절한 최적화를 감당할 수 있을 것이라는 생각이 들었다.\n앞으로도 이런 최적화 방법론들은 계속 발전할 것이며, 엔지니어로서 이러한 도구의 내부 작동 방식을 이해하는 것은 효과적인 AI 시스템 구축에 있어 도움이 되지 않을까 싶다.\nReferences [1] NVIDIA TensorRT 공식 문서: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html [2] NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server [3] TensorRT 최적화 기술: https://on-demand.gputechconf.com/gtc/2019/presentation/s9431-tensorrt-inference-optimization.pdf [4] Triton 성능 모범 사례: https://github.com/triton-inference-server/server/blob/main/docs/user_guide/performance_tuning.md [5] NVIDIA Deep Learning Examples: https://github.com/NVIDIA/DeepLearningExamples [6] Han, S., Shen, H., Philipose, M., Agarwal, S., Wolman, A., \u0026 Krishnamurthy, A. (2016). MCDNN: An approximation-based execution framework for deep stream processing under resource constraints. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services (pp. 123-136). [7] Nvidia. (2022). TensorRT: A platform for deep learning inference. In GTC 2022. [8] Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., … \u0026 Ren, M. (2018). TVM: An automated end-to-end optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (pp. 578-594). ",
  "wordCount" : "7170",
  "inLanguage": "ko",
  "datePublished": "2024-07-30T15:30:00+09:00",
  "dateModified": "2024-07-30T15:30:00+09:00",
  "author":{
    "@type": "Person",
    "name": "macsim"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://macsim2.github.io/posts/tech/inference_optimzation/tensorrt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "macsim's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://macsim2.github.io/img/Q.gif"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://macsim2.github.io/" accesskey="h" title="Macsim&#39;s Blog (Alt + H)">Macsim&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://macsim2.github.io/" title="🏠 home">
                <span>🏠 home</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/about/" title="🙋 about">
                <span>🙋 about</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/archives/" title="archives">
                <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/posts/" title="📚 posts">
                <span>📚 posts</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/tags/" title="🧩 tags">
                <span>🧩 tags</span>
                </a>
            </li>
            <li>
                <a href="https://macsim2.github.io/search/" title="⏱️ search (Alt &#43; /)" accesskey=/>
                <span>⏱️ search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://macsim2.github.io/">🏠 홈</a>&nbsp;»&nbsp;<a href="https://macsim2.github.io/posts/">posts</a>&nbsp;»&nbsp;<a href="https://macsim2.github.io/posts/tech/">👨🏻‍💻 tech</a></div>
            <h1 class="post-title">
                TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합
            </h1>
            <div class="post-description">
                NVIDIA TensorRT와 Triton Inference Server의 내부 구조와 실전 최적화 경험을 공유합니다
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2024-07-30
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>7170 word
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>15 min
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>macsim
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://macsim2.github.io/tags/deeplearning/" style="color: var(--secondary)!important;">Deeplearning</a>
                &nbsp;<a href="https://macsim2.github.io/tags/tensorrt/" style="color: var(--secondary)!important;">Tensorrt</a>
                &nbsp;<a href="https://macsim2.github.io/tags/nvidia-triton/" style="color: var(--secondary)!important;">Nvidia Triton</a>
                &nbsp;<a href="https://macsim2.github.io/tags/inference-optimization/" style="color: var(--secondary)!important;">Inference Optimization</a>
                &nbsp;<a href="https://macsim2.github.io/tags/model-deployment/" style="color: var(--secondary)!important;">Model Deployment</a>
                &nbsp;<a href="https://macsim2.github.io/tags/gpu-acceleration/" style="color: var(--secondary)!important;">Gpu Acceleration</a>
            </span>
        </span>
    </span>
</span>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

                <span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://macsim2.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">목차</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#tensorrt%eb%9e%80" aria-label="TensorRT란?">TensorRT란?</a></li>
                <li>
                    <a href="#tensorrt%ec%9d%98-%eb%a7%88%eb%b2%95-%eb%82%b4%eb%b6%80-%ec%b5%9c%ec%a0%81%ed%99%94-%eb%a9%94%ec%bb%a4%eb%8b%88%ec%a6%98" aria-label="TensorRT의 마법: 내부 최적화 메커니즘">TensorRT의 마법: 내부 최적화 메커니즘</a><ul>
                        
                <li>
                    <a href="#1-%ea%b7%b8%eb%9e%98%ed%94%84-%ec%b5%9c%ec%a0%81%ed%99%94-%ea%b8%b0%eb%b2%95" aria-label="1. 그래프 최적화 기법">1. 그래프 최적화 기법</a></li>
                <li>
                    <a href="#2-%ec%a0%95%eb%b0%80%eb%8f%84-%ec%b5%9c%ec%a0%81%ed%99%94precision-optimization" aria-label="2. 정밀도 최적화(Precision Optimization)">2. 정밀도 최적화(Precision Optimization)</a></li>
                <li>
                    <a href="#3-%eb%8f%99%ec%a0%81-%ed%85%90%ec%84%9c-%eb%a9%94%eb%aa%a8%eb%a6%ac%ec%99%80-%ec%8b%a4%ed%96%89-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="3. 동적 텐서 메모리와 실행 최적화">3. 동적 텐서 메모리와 실행 최적화</a><ul>
                        
                <li>
                    <a href="#%eb%a9%94%eb%aa%a8%eb%a6%ac-%ed%92%80%eb%a7%81memory-pooling" aria-label="메모리 풀링(Memory Pooling)">메모리 풀링(Memory Pooling)</a></li>
                <li>
                    <a href="#%ed%85%90%ec%84%9c-%ec%9e%ac%ec%82%ac%ec%9a%a9tensor-reuse" aria-label="텐서 재사용(Tensor Reuse)">텐서 재사용(Tensor Reuse)</a></li>
                <li>
                    <a href="#%ec%8b%a4%ed%96%89-%eb%b3%91%eb%a0%ac%ed%99%94execution-parallelism" aria-label="실행 병렬화(Execution Parallelism)">실행 병렬화(Execution Parallelism)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%ec%8b%a4%ec%a0%84-tensorrt-%eb%aa%a8%eb%8d%b8-%eb%b3%80%ed%99%98%ea%b3%bc-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="실전 TensorRT 모델 변환과 최적화">실전 TensorRT 모델 변환과 최적화</a><ul>
                        
                <li>
                    <a href="#1-%eb%aa%a8%eb%8d%b8-%eb%b3%80%ed%99%98-%ed%8c%8c%ec%9d%b4%ed%94%84%eb%9d%bc%ec%9d%b8" aria-label="1. 모델 변환 파이프라인">1. 모델 변환 파이프라인</a></li>
                <li>
                    <a href="#2-%eb%8f%99%ec%a0%81-%ed%98%95%ec%83%81dynamic-shapes-%ec%b2%98%eb%a6%ac%ec%9d%98-%ed%95%a8%ec%a0%95" aria-label="2. 동적 형상(Dynamic Shapes) 처리의 함정">2. 동적 형상(Dynamic Shapes) 처리의 함정</a></li>
                <li>
                    <a href="#3-int8-%ec%96%91%ec%9e%90%ed%99%94%ec%99%80-%eb%b3%b4%ec%a0%95%ec%9d%98-%ea%b3%bc%ed%95%99" aria-label="3. INT8 양자화와 보정의 과학">3. INT8 양자화와 보정의 과학</a></li></ul>
                </li>
                <li>
                    <a href="#nvidia-triton-inference-server-%ed%99%95%ec%9e%a5-%ea%b0%80%eb%8a%a5%ed%95%9c-%eb%aa%a8%eb%8d%b8-%ec%84%9c%eb%b9%99%ec%9d%98-%ed%95%b5%ec%8b%ac" aria-label="NVIDIA Triton Inference Server: 확장 가능한 모델 서빙의 핵심">NVIDIA Triton Inference Server: 확장 가능한 모델 서빙의 핵심</a><ul>
                        
                <li>
                    <a href="#1-triton%ec%9d%98-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98%ec%99%80-%ec%a3%bc%ec%9a%94-%ea%b8%b0%eb%8a%a5" aria-label="1. Triton의 아키텍처와 주요 기능">1. Triton의 아키텍처와 주요 기능</a></li>
                <li>
                    <a href="#2-%eb%aa%a8%eb%8d%b8-%eb%a6%ac%ed%8f%ac%ec%a7%80%ed%86%a0%eb%a6%ac-%ea%b5%ac%ec%84%b1%ea%b3%bc-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="2. 모델 리포지토리 구성과 최적화">2. 모델 리포지토리 구성과 최적화</a></li>
                <li>
                    <a href="#3-%eb%8f%99%ec%a0%81-%eb%b0%b0%ec%b9%98-%ec%b2%98%eb%a6%ac%ec%99%80-%ec%88%98%ed%8f%89%ec%a0%81-%ed%99%95%ec%9e%a5" aria-label="3. 동적 배치 처리와 수평적 확장">3. 동적 배치 처리와 수평적 확장</a></li></ul>
                </li>
                <li>
                    <a href="#%ec%8b%a4%ec%a0%84-tensorrt%ec%99%80-triton-%ec%a0%81%ec%9a%a9-%ec%82%ac%eb%a1%80%ec%99%80-%eb%8f%84%ec%a0%84" aria-label="실전 TensorRT와 Triton 적용 사례와 도전">실전 TensorRT와 Triton 적용 사례와 도전</a><ul>
                        
                <li>
                    <a href="#1-%eb%b3%b5%ec%9e%a1%ed%95%9c-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98-%eb%aa%a8%eb%8d%b8%ec%9d%98-%eb%b3%80%ed%99%98-%eb%8f%84%ec%a0%84" aria-label="1. 복잡한 아키텍처 모델의 변환 도전">1. 복잡한 아키텍처 모델의 변환 도전</a></li>
                <li>
                    <a href="#2-%ec%b6%94%eb%a1%a0-%ec%a7%80%ec%97%b0-%ec%8b%9c%ea%b0%84-vs-%ec%b2%98%eb%a6%ac%eb%9f%89-%ec%b5%9c%ec%a0%81%ed%99%94" aria-label="2. 추론 지연 시간 vs 처리량 최적화">2. 추론 지연 시간 vs 처리량 최적화</a></li>
                <li>
                    <a href="#3-%eb%a9%94%eb%aa%a8%eb%a6%ac-%ec%b5%9c%ec%a0%81%ed%99%94%ec%99%80-%eb%8b%a4%ec%a4%91-%eb%aa%a8%eb%8d%b8-%eb%b0%b0%ed%8f%ac" aria-label="3. 메모리 최적화와 다중 모델 배포">3. 메모리 최적화와 다중 모델 배포</a></li></ul>
                </li>
                <li>
                    <a href="#%ec%8b%a4%ec%a0%84-%eb%94%94%eb%b2%84%ea%b9%85%ea%b3%bc-%ec%84%b1%eb%8a%a5-%eb%b6%84%ec%84%9d-%eb%8f%84%ea%b5%ac" aria-label="실전 디버깅과 성능 분석 도구">실전 디버깅과 성능 분석 도구</a><ul>
                        
                <li>
                    <a href="#1-nvidia-nsight-systems" aria-label="1. NVIDIA Nsight Systems">1. NVIDIA Nsight Systems</a></li>
                <li>
                    <a href="#2-tensorrt-trtexec-%eb%8f%84%ea%b5%ac" aria-label="2. TensorRT trtexec 도구">2. TensorRT trtexec 도구</a></li>
                <li>
                    <a href="#3-triton-%eb%aa%a8%eb%8d%b8-%eb%b6%84%ec%84%9d%ea%b8%b0" aria-label="3. Triton 모델 분석기">3. Triton 모델 분석기</a></li></ul>
                </li>
                <li>
                    <a href="#tensorrt%ec%99%80-triton%ec%9d%98-%ed%98%84%ec%8b%a4%ec%a0%81%ec%9d%b8-%ed%95%9c%ea%b3%84%ec%99%80-%eb%8c%80%ec%95%88" aria-label="TensorRT와 Triton의 현실적인 한계와 대안">TensorRT와 Triton의 현실적인 한계와 대안</a><ul>
                        
                <li>
                    <a href="#1-%eb%aa%a8%eb%8d%b8-%ed%98%b8%ed%99%98%ec%84%b1-%eb%8f%84%ec%a0%84" aria-label="1. 모델 호환성 도전">1. 모델 호환성 도전</a></li>
                <li>
                    <a href="#2-%ea%b0%9c%eb%b0%9c-%ed%8e%b8%ec%9d%98%ec%84%b1-vs-%ec%b5%9c%ec%a0%81%ed%99%94-%ec%88%98%ec%a4%80" aria-label="2. 개발 편의성 vs 최적화 수준">2. 개발 편의성 vs 최적화 수준</a></li>
                <li>
                    <a href="#3-%eb%a9%80%ed%8b%b0-%ed%81%b4%eb%9d%bc%ec%9a%b0%eb%93%9c-%eb%b0%8f-%ec%9d%b4%ec%a2%85-%ed%99%98%ea%b2%bd-%ec%a7%80%ec%9b%90" aria-label="3. 멀티 클라우드 및 이종 환경 지원">3. 멀티 클라우드 및 이종 환경 지원</a></li></ul>
                </li>
                <li>
                    <a href="#%ea%b2%b0%eb%a1%a0" aria-label="결론">결론</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><p>AI 모델을 배포해보려 공부해본 엔지니어라면 누구나 이런 고민을 해봤을 것이다. <strong>&ldquo;어떻게 하면 내 모델이 프로덕션 환경에서 더 빠르게 동작할 수 있을까?&rdquo;</strong>  <br>
처음 이런 task를 접했을 때는 단순 C/C++ 프로그래밍을 통해서 해결할 수 있을 줄 알았다. 그러나, 물론 naive한 python 보다야 낫지만, request가 많아질 수록 다른 해결책이 필요하다고 생각이 들 것이다.</p>
<p>그러던 와중에 나온 NVIDIA의 TensorRT 라는 놈이 있다. 오늘은 모델 최적화와 프로덕션 배포의 세계에서 강력한 도구로 자리잡은 TensorRT와 Triton Inference Server에 대해서 알아보자.</p>
<p>이 글에서는 단순한 &ldquo;TensorRT 사용법&quot;이나 &ldquo;Triton 서버 설정법&quot;이 아닌, 이 두 기술의 내부 구조, 최적화 원리, 그리고 실제 프로젝트에서 마주친 도전과 해결책에 대해 얘기해보고자 한다.</p>
<h2 id="tensorrt란">TensorRT란?<a hidden class="anchor" aria-hidden="true" href="#tensorrt란">#</a></h2>
<p>TensorRT는 NVIDIA에서 개발한 고성능 딥러닝 추론 최적화 라이브러리로, 딥러닝 모델을 NVIDIA GPU에서 최대 성능으로 실행할 수 있도록 해준다. 이런 설명을 듣고 생각하면 일종의 CUDA 가속 추론 라이브러리 구나 생각이 들고, 으레 짐작할 수 있지만, TensorRT는 그 이상의 가치를 제공한다.</p>
<p>PyTorch나 TensorFlow에서 모델을 학습시킬 때, 이 프레임워크들은 유연성과 학습 편의성에 초점을 맞추고 있다. 그러나 프로덕션 환경에서는 순수한 추론 성능과 효율성이 훨씬 중요해진다. 여기서 TensorRT가 등장한다.</p>
<p>TensorRT의 핵심은 **계산 그래프 최적화(Computational Graph Optimization)**와 **하드웨어 최적화 커널(Hardware-Optimized Kernels)**이다. 여기서 부터 먼가 머리가 아파올 수 있다.. 계산 그래프를 최적화 하고 하드웨어 최적화 커널 이라니..? 무시무시한 최적화 방법인 것 같아 보인다. TensorRT는 모델의 구조를 분석하고 NVIDIA GPU에 최적화된 형태로 변환하여, 때로는 원본 모델보다 5-10배까지 빠른 추론 속도를 달성할 수 있다.</p>
<!-- 검색 키워드: "TensorRT optimization workflow diagram", "ONNX to TensorRT conversion pipeline", "TensorRT layer fusion diagram"  
   이상적인 이미지는 원본 모델에서 ONNX 변환을 거쳐 TensorRT 엔진으로 최적화되는 전체 흐름과 레이어 융합, 메모리 최적화, 정밀도 조정 등의 핵심 기술이 시각화된 다이어그램입니다. -->
<p><img loading="lazy" src="/images/dl/tensorrt_optimization_flow.png" alt="TensorRT 최적화 파이프라인"  />
</p>
<h2 id="tensorrt의-마법-내부-최적화-메커니즘">TensorRT의 마법: 내부 최적화 메커니즘<a hidden class="anchor" aria-hidden="true" href="#tensorrt의-마법-내부-최적화-메커니즘">#</a></h2>
<p>그렇다면 어떻게 TensorRT가 이렇게 극적인 성능 향상을 가능하게 하는지 조금 더 구체적으로 살펴보자.</p>
<h3 id="1-그래프-최적화-기법">1. 그래프 최적화 기법<a hidden class="anchor" aria-hidden="true" href="#1-그래프-최적화-기법">#</a></h3>
<p>TensorRT는 신경망 모델을 최적화하기 위해 여러 그래프 변환 기법을 사용한다:</p>
<ul>
<li><strong>레이어 융합(Layer Fusion)</strong>: 여러 레이어를 단일 최적화된 레이어로 결합</li>
<li><strong>커널 튜닝(Kernel Auto-Tuning)</strong>: 하드웨어에 맞게 최적의 커널 구현 선택</li>
<li><strong>텐서 메모리 최적화</strong>: 메모리 사용량을 최소화하고 캐시 효율성 극대화</li>
<li><strong>불필요한 연산 제거</strong>: 추론 시 필요없는 계산 생략</li>
</ul>
<p>내 경험상 <strong>레이어 융합</strong>은 가장 극적인 성능 개선을 가져오는 최적화 중 하나였다. 특히 Convolution + BatchNorm + ReLU와 같은 일반적인 패턴은 단일 최적화 커널로 대체되어 메모리 액세스를 크게 줄일 수 있었다.
나중에 OpenAI의 Triton에 대해서도 posting 하려고 하는데 여기서의 <strong>메모리 액세스</strong> 를 줄인다는게 최적화 과정의 아주 큰 요소이다. 이 메모리 액세스를 어떻게 줄인것인가에 많은 연구자들이 목을 메고 있다.</p>
<p>NVIDIA의 연구[7]에 따르면, 레이어 융합을 통해 중간 텐서의 메모리 액세스가 50% 이상 감소하고, 이는 대규모 모델에서 최대 25%의 추론 성능 향상으로 이어진다고 한다. Han 등의 연구[6]에서는 메모리 액세스 감소가 모바일 환경에서 에너지 효율성을 최대 3배까지 개선할 수 있음을 보여준다고 한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// TensorRT의 레이어 융합 예시 (의사 코드)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// 이런 세 개의 레이어가 있다고 가정해보자
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>IConvolutionLayer<span style="color:#f92672">*</span> conv <span style="color:#f92672">=</span> network<span style="color:#f92672">-&gt;</span>addConvolution(...);
</span></span><span style="display:flex;"><span>IBatchNormLayer<span style="color:#f92672">*</span> bn <span style="color:#f92672">=</span> network<span style="color:#f92672">-&gt;</span>addBatchNorm(...);
</span></span><span style="display:flex;"><span>IActivationLayer<span style="color:#f92672">*</span> relu <span style="color:#f92672">=</span> network<span style="color:#f92672">-&gt;</span>addActivation(<span style="color:#f92672">*</span>bn<span style="color:#f92672">-&gt;</span>getOutput(<span style="color:#ae81ff">0</span>), ActivationType<span style="color:#f92672">::</span>kRELU);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// TensorRT는 내부적으로 이를 단일 최적화 커널로 융합한다
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// network-&gt;addFusedConvBNRelu(...); (실제 API는 아님)
</span></span></span></code></pre></div><h3 id="2-정밀도-최적화precision-optimization">2. 정밀도 최적화(Precision Optimization)<a hidden class="anchor" aria-hidden="true" href="#2-정밀도-최적화precision-optimization">#</a></h3>
<p>TensorRT의 가장 강력한 기능 중 하나는 **혼합 정밀도 연산(Mixed Precision Computing)**이다. 다양한 수치 정밀도를 지원한다:</p>
<ul>
<li><strong>FP32</strong>: 32비트 단정밀도 부동 소수점</li>
<li><strong>FP16</strong>: 16비트 반정밀도 부동 소수점</li>
<li><strong>INT8</strong>: 8비트 정수 양자화</li>
<li><strong>TF32</strong>: NVIDIA Ampere GPU부터 지원하는 19비트 텐서 부동 소수점 포맷</li>
</ul>
<p>실제 프로젝트에서, FP16으로 전환하는 것만으로도 <strong>거의 정확도 손실 없이 2배 이상의 성능 향상</strong>을 얻을 수 있었다. INT8로 더 나아가면 대략 4배까지 처리속도가 향상될 수 있지만, Quantization 과정에서 정확도 저하가 있기에 trade-off를 따져야 했다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># TensorRT의 정밀도 설정 예시</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># builder 설정</span>
</span></span><span style="display:flex;"><span>builder <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Builder(TRT_LOGGER)
</span></span><span style="display:flex;"><span>network <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_network(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> int(trt<span style="color:#f92672">.</span>NetworkDefinitionCreationFlag<span style="color:#f92672">.</span>EXPLICIT_BATCH))
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_builder_config()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FP16 정밀도 활성화</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>FP16)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># INT8 양자화 활성화 (보정 필요)</span>
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>INT8)
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>int8_calibrator <span style="color:#f92672">=</span> calibrator  <span style="color:#75715e"># 보정기 인스턴스</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 엔진 생성</span>
</span></span><span style="display:flex;"><span>engine <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>build_engine(network, config)
</span></span></code></pre></div><h3 id="3-동적-텐서-메모리와-실행-최적화">3. 동적 텐서 메모리와 실행 최적화<a hidden class="anchor" aria-hidden="true" href="#3-동적-텐서-메모리와-실행-최적화">#</a></h3>
<p>TensorRT는 실행 중에 텐서 메모리 할당을 최적화하기 위해 <strong>동적 메모리 관리</strong>를 사용한다. 이게 무슨 의미일까? 쉽게 설명하자면, 신경망이 연산을 수행할 때 필요한 메모리를 더 효율적으로 사용하는 방법들을 적용한다는 의미다. 여기에는 세 가지 핵심 전략이 있다.</p>
<h4 id="메모리-풀링memory-pooling">메모리 풀링(Memory Pooling)<a hidden class="anchor" aria-hidden="true" href="#메모리-풀링memory-pooling">#</a></h4>
<p>기존 방식에서는 모델이 연산을 수행할 때마다 메모리를 할당하고 해제하는 과정을 반복했다. 이 과정은 GPU에게 &ldquo;이 크기의 메모리가 필요해&rdquo;, &ldquo;이제 이 메모리는 필요 없어&rdquo; 라고 계속 말하는 것과 같은데, 이는 상당한 오버헤드를 발생시킨다.</p>
<p>메모리 풀링에서는 TensorRT가 미리 대량의 메모리 풀을 할당해두고, 필요할 때마다 이 풀에서 빠르게 메모리를 가져와 사용한다. 마치 공유 오피스에서 매번 책상을 사고 버리는 대신, 미리 준비된 책상을 필요할 때 사용하는 것과 같다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 기존 방식 (의사 코드)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> each_operation <span style="color:#f92672">in</span> model:
</span></span><span style="display:flex;"><span>    memory <span style="color:#f92672">=</span> allocate_new_memory(size_needed)  <span style="color:#75715e"># 시간 소요</span>
</span></span><span style="display:flex;"><span>    perform_operation_using(memory)
</span></span><span style="display:flex;"><span>    free_memory(memory)  <span style="color:#75715e"># 또 시간 소요</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 메모리 풀링 방식 (의사 코드)</span>
</span></span><span style="display:flex;"><span>memory_pool <span style="color:#f92672">=</span> allocate_large_pool_once()  <span style="color:#75715e"># 초기에 한 번만 비용 지불</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> each_operation <span style="color:#f92672">in</span> model:
</span></span><span style="display:flex;"><span>    memory <span style="color:#f92672">=</span> get_from_pool(memory_pool, size_needed)  <span style="color:#75715e"># 매우 빠름</span>
</span></span><span style="display:flex;"><span>    perform_operation_using(memory)
</span></span><span style="display:flex;"><span>    return_to_pool(memory)  <span style="color:#75715e"># 해제가 아닌 반환 (빠름)</span>
</span></span></code></pre></div><h4 id="텐서-재사용tensor-reuse">텐서 재사용(Tensor Reuse)<a hidden class="anchor" aria-hidden="true" href="#텐서-재사용tensor-reuse">#</a></h4>
<p>딥러닝 모델 내부에서는 많은 중간 결과(텐서)가 생성된다. 이들은 보통 비슷한 크기를 가지는 경우가 많고, 한 번 사용된 후에는 더 이상 필요하지 않은 경우가 많다.</p>
<p>TensorRT는 똑똑하게도 &ldquo;이 텐서의 데이터는 더 이상 필요 없으니, 이 메모리 공간을 다른 비슷한 크기의 텐서를 위해 재사용할 수 있겠군!&ldquo;이라고 판단한다. 이는 특히 메모리가 제한적인 엣지 디바이스에서 중요하다.</p>
<p>예를 들어, ResNet과 같은 모델에서는 각 레이어의 출력 텐서가 비슷한 크기를 갖는다. 1번 레이어의 결과가 2번 레이어에 전달된 후에는 1번 레이어의 출력 메모리 공간을 3번 레이어의 출력을 저장하는 데 재사용할 수 있다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>[초기 상태]
</span></span><span style="display:flex;"><span>Layer 1 출력 → 메모리 A 사용
</span></span><span style="display:flex;"><span>Layer 2 출력 → 메모리 B 사용
</span></span><span style="display:flex;"><span>Layer 3 출력 → 새 메모리 필요...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[텐서 재사용 적용]
</span></span><span style="display:flex;"><span>Layer 1 출력 → 메모리 A 사용
</span></span><span style="display:flex;"><span>Layer 2 출력 → 메모리 B 사용
</span></span><span style="display:flex;"><span>Layer 3 출력 → 메모리 A 재사용 (Layer 1 출력은 이미 Layer 2에 전달됨)
</span></span></code></pre></div><h4 id="실행-병렬화execution-parallelism">실행 병렬화(Execution Parallelism)<a hidden class="anchor" aria-hidden="true" href="#실행-병렬화execution-parallelism">#</a></h4>
<p>그래프 관점에서 보면, 딥러닝 모델은 일부 연산들이 서로 독립적이라 동시에 실행될 수 있다. TensorRT는 이러한 독립적 연산들을 찾아내 병렬로 실행한다.</p>
<p>예를 들어, 멀티 헤드 어텐션에서 각 &ldquo;헤드&quot;는 독립적으로 계산될 수 있다. 또한 모델의 여러 브랜치 (예: ResNet의 residual connection과 main path)도 병렬로 처리될 수 있다.</p>
<p>NVIDIA의 연구[7]에서는 다중 스트림 병렬화가 특히 Transformer 모델에서 효과적임을 보여주었다. 각 어텐션 헤드를 별도의 CUDA 스트림에 할당하는 기법만으로도 약 40%의 처리량 증가를 달성했다고 한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// TensorRT의 작업 스케줄링 최적화 (의사 코드)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// 실제 구현은 더 복잡하지만, 개념적으로는 이런 방식
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// 독립적인 두 연산 병렬 실행
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// A와 B는 서로 의존성이 없다고 가정 (예: 두 개의 독립된 컨볼루션)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>executeParallel([<span style="color:#f92672">&amp;</span>]() {
</span></span><span style="display:flex;"><span>  executeOperation(operationA);  <span style="color:#75715e">// GPU의 일부 코어에서 실행
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}, [<span style="color:#f92672">&amp;</span>]() {
</span></span><span style="display:flex;"><span>  executeOperation(operationB);  <span style="color:#75715e">// 동시에 다른 코어에서 실행
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>});
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// C는 A와 B의 결과에 의존 (예: A와 B의 결과를 합치는 연산)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// A와 B가 모두 완료될 때까지 기다린 후 실행
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>executeOperation(operationC);
</span></span></code></pre></div><p>이런 최적화 방법을 통해 지연 시간이 감소 되고, 그에 따라 처리량도 자연스레 증가 할 수 있을것이다.</p>
<h2 id="실전-tensorrt-모델-변환과-최적화">실전 TensorRT 모델 변환과 최적화<a hidden class="anchor" aria-hidden="true" href="#실전-tensorrt-모델-변환과-최적화">#</a></h2>
<p>실제 프로젝트에서 TensorRT를 사용할 때 알아야 할 중요한 워크플로우와 실무 팁을 살펴보자.</p>
<h3 id="1-모델-변환-파이프라인">1. 모델 변환 파이프라인<a hidden class="anchor" aria-hidden="true" href="#1-모델-변환-파이프라인">#</a></h3>
<p>TensorRT 모델 변환은 일반적으로 다음 단계를 따른다:</p>
<ol>
<li><strong>원본 모델 내보내기</strong>: PyTorch/TensorFlow 모델을 ONNX 또는 UFF 포맷으로 변환</li>
<li><strong>TensorRT 엔진 빌드</strong>: ONNX/UFF 모델을 TensorRT 엔진으로 최적화</li>
<li><strong>정밀도 선택 및 보정</strong>: 필요시 INT8 양자화를 위한 보정 수행</li>
<li><strong>엔진 직렬화(Serialization)</strong>: 최적화된 엔진을 디스크에 저장하여 로딩 시간 단축</li>
</ol>
<p>ONNX를 통한 변환이 가장 안정적이고 지원되는 경로였지만, PyTorch 모델에 따라 ONNX 변환 과정에서 여러 난관을 마주치기도 했다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># PyTorch에서 TensorRT 변환 예시</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. ONNX로 내보내기</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
</span></span><span style="display:flex;"><span>    model, dummy_input, <span style="color:#e6db74">&#34;model.onnx&#34;</span>,
</span></span><span style="display:flex;"><span>    opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">13</span>,
</span></span><span style="display:flex;"><span>    do_constant_folding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    input_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;input&#34;</span>],
</span></span><span style="display:flex;"><span>    output_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;output&#34;</span>],
</span></span><span style="display:flex;"><span>    dynamic_axes<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;input&#34;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#34;batch_size&#34;</span>}, <span style="color:#e6db74">&#34;output&#34;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#34;batch_size&#34;</span>}}
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. TensorRT 엔진 생성</span>
</span></span><span style="display:flex;"><span>TRT_LOGGER <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Logger(trt<span style="color:#f92672">.</span>Logger<span style="color:#f92672">.</span>WARNING)
</span></span><span style="display:flex;"><span>builder <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>Builder(TRT_LOGGER)
</span></span><span style="display:flex;"><span>network <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_network(<span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> int(trt<span style="color:#f92672">.</span>NetworkDefinitionCreationFlag<span style="color:#f92672">.</span>EXPLICIT_BATCH))
</span></span><span style="display:flex;"><span>parser <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>OnnxParser(network, TRT_LOGGER)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;model.onnx&#34;</span>, <span style="color:#e6db74">&#34;rb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>parse(f<span style="color:#f92672">.</span>read())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_builder_config()
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>max_workspace_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">&lt;&lt;</span> <span style="color:#ae81ff">30</span>  <span style="color:#75715e"># 1GB</span>
</span></span><span style="display:flex;"><span>engine <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>build_engine(network, config)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 엔진 저장</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;model.trt&#34;</span>, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(engine<span style="color:#f92672">.</span>serialize())
</span></span></code></pre></div><h3 id="2-동적-형상dynamic-shapes-처리의-함정">2. 동적 형상(Dynamic Shapes) 처리의 함정<a hidden class="anchor" aria-hidden="true" href="#2-동적-형상dynamic-shapes-처리의-함정">#</a></h3>
<p>TensorRT 7.0부터 동적 입력 크기가 잘 지원되지만, 최적의 성능을 얻기 위해서는 **최적화 프로필(Optimization Profiles)**을 신중하게 설정해야 한다.</p>
<p>NLP 모델을 배포했을 때 가변 길이 시퀀스를 효율적으로 처리하는 것이 큰 도전이었다. 예상 시퀀스 길이 범위에 맞게 최적화 프로필을 구성했을 때 가장 좋은 결과를 얻을 수 있었다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 동적 형상을 위한 최적화 프로필 설정</span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_builder_config()
</span></span><span style="display:flex;"><span>profile <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_optimization_profile()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 최소, 최적, 최대 입력 크기 지정</span>
</span></span><span style="display:flex;"><span>profile<span style="color:#f92672">.</span>set_shape(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;input&#34;</span>,                          <span style="color:#75715e"># 입력 텐서 이름</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>),                 <span style="color:#75715e"># 최소 크기</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>),                 <span style="color:#75715e"># 최적 크기 (가장 자주 사용될 것으로 예상)</span>
</span></span><span style="display:flex;"><span>    (<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>)                 <span style="color:#75715e"># 최대 크기</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>add_optimization_profile(profile)
</span></span></code></pre></div><p>또한 다양한 배치 크기에 최적화된 <strong>다중 프로필</strong>을 사용하여 성능을 더욱 향상시킬 수 있었다.</p>
<h3 id="3-int8-양자화와-보정의-과학">3. INT8 양자화와 보정의 과학<a hidden class="anchor" aria-hidden="true" href="#3-int8-양자화와-보정의-과학">#</a></h3>
<p>FP16보다 더 극적인 성능 향상을 위해 INT8 양자화를 적용할 수 있지만, 이는 세심한 보정(Calibration) 과정이 필요하다:</p>
<ol>
<li><strong>보정 데이터셋 준비</strong>: 실제 데이터의 통계적 특성을 대표하는 샘플 준비</li>
<li><strong>보정기(Calibrator) 구현</strong>: 다양한 보정 알고리즘 중 선택 (엔트로피, 최소-최대 등)</li>
<li><strong>보정 실행</strong>: 보정 데이터를 통해 모델의 활성화 범위 분석</li>
<li><strong>정확도 검증</strong>: 양자화 후 정확도 손실 평가</li>
</ol>
<p>NVIDIA 개발자 블로그와 TVM 프로젝트 연구[8]에 따르면, 양자화 방식 중 특히 <strong>엔트로피 보정</strong> 방식이 가장 낮은 정보 손실로 양자화를 수행할 수 있어 정확도 측면에서 우수한 결과를 제공한다. 이 방법은 활성화 분포를 고려하여 양자화 스케일을 동적으로 조정하기 때문에 이미지 분류 모델에서 특히 효과적이다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># TensorRT INT8 보정 예시</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorrt <span style="color:#66d9ef">as</span> trt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 보정기 클래스 정의</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EntropyCalibrator</span>(trt<span style="color:#f92672">.</span>IInt8EntropyCalibrator2):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, calibration_data, batch_size, input_name):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data <span style="color:#f92672">=</span> calibration_data
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> batch_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>input_name <span style="color:#f92672">=</span> input_name
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device_input <span style="color:#f92672">=</span> cuda<span style="color:#f92672">.</span>mem_alloc(self<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>nbytes <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>batch_size)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_batch_size</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_batch</span>(self, names):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>current_idx <span style="color:#f92672">&gt;=</span> len(self<span style="color:#f92672">.</span>data):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>        batch <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>data[self<span style="color:#f92672">.</span>current_idx:self<span style="color:#f92672">.</span>current_idx<span style="color:#f92672">+</span>self<span style="color:#f92672">.</span>batch_size]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>current_idx <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        cuda<span style="color:#f92672">.</span>memcpy_htod(self<span style="color:#f92672">.</span>device_input, np<span style="color:#f92672">.</span>ascontiguousarray(batch))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> [self<span style="color:#f92672">.</span>device_input]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_calibration_cache</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 이전 보정 캐시 읽기 (있는 경우)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_calibration_cache</span>(self, cache):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 보정 결과 저장</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 보정기 사용</span>
</span></span><span style="display:flex;"><span>calibration_data <span style="color:#f92672">=</span> [<span style="color:#f92672">...</span>]  <span style="color:#75715e"># 보정 데이터 준비</span>
</span></span><span style="display:flex;"><span>calibrator <span style="color:#f92672">=</span> EntropyCalibrator(calibration_data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, input_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;input&#34;</span>)
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>int8_calibrator <span style="color:#f92672">=</span> calibrator
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>set_flag(trt<span style="color:#f92672">.</span>BuilderFlag<span style="color:#f92672">.</span>INT8)
</span></span></code></pre></div><p>실제로 대규모 모델의 경우, INT8 양자화를 통해 <strong>최대 4배의 처리량 향상</strong>을 경험했지만, 이는 약간의 정확도 희생을 동반했다. 트레이드오프가 용인되는 비즈니스 시나리오에서만 활용했다.</p>
<h2 id="nvidia-triton-inference-server-확장-가능한-모델-서빙의-핵심">NVIDIA Triton Inference Server: 확장 가능한 모델 서빙의 핵심<a hidden class="anchor" aria-hidden="true" href="#nvidia-triton-inference-server-확장-가능한-모델-서빙의-핵심">#</a></h2>
<p>TensorRT가 단일 모델의 성능을 최적화하는 데 초점을 맞춘다면, Triton Inference Server는 <strong>대규모 프로덕션 환경에서의 모델 서빙</strong>을 처리한다.</p>
<h3 id="1-triton의-아키텍처와-주요-기능">1. Triton의 아키텍처와 주요 기능<a hidden class="anchor" aria-hidden="true" href="#1-triton의-아키텍처와-주요-기능">#</a></h3>
<p>Triton Inference Server는 다양한 프레임워크와 하드웨어에서 추론 서비스를 제공하기 위한 컨테이너화된 솔루션이다. NVIDIA 개발자 컨퍼런스[7]에서 소개된 Triton의 주요 기능은 다음과 같다:</p>
<ul>
<li><strong>다중 프레임워크 지원</strong>: TensorRT, ONNX Runtime, TensorFlow, PyTorch 등</li>
<li><strong>동적 배치 처리</strong>: 요청을 자동으로 배치화하여 처리량 최적화</li>
<li><strong>동시 모델 실행</strong>: 여러 모델을 동시에 서빙</li>
<li><strong>모델 앙상블</strong>: 복잡한 추론 파이프라인 구성</li>
<li><strong>모델 버전 관리</strong>: 무중단 업데이트 지원</li>
<li><strong>하드웨어 리소스 관리</strong>: GPU 메모리와 계산 자원 할당 최적화</li>
</ul>
<!-- 검색 키워드: "NVIDIA Triton Inference Server architecture", "Triton server multiple backends diagram", "Triton inference workflow"  
   이상적인 이미지는 Triton 서버의 구성 요소와 클라이언트-서버 통신 흐름, TensorRT/ONNX/TensorFlow 등 다양한 백엔드 지원을 보여주는 아키텍처 다이어그램입니다. -->
<p><img loading="lazy" src="/images/dl/triton_architecture.png" alt="Triton 서버 아키텍처"  />
</p>
<h3 id="2-모델-리포지토리-구성과-최적화">2. 모델 리포지토리 구성과 최적화<a hidden class="anchor" aria-hidden="true" href="#2-모델-리포지토리-구성과-최적화">#</a></h3>
<p>Triton은 <strong>모델 리포지토리</strong>라는 표준화된 디렉토리 구조를 통해 모델을 관리한다. 이 구조는 각 모델의 다양한 버전과 구성을 체계적으로 관리할 수 있게 해준다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>model_repository/
</span></span><span style="display:flex;"><span>  ├── model1/
</span></span><span style="display:flex;"><span>  │     ├── config.pbtxt        # 모델 구성 파일
</span></span><span style="display:flex;"><span>  │     └── 1/                  # 버전 1
</span></span><span style="display:flex;"><span>  │         └── model.plan      # TensorRT 엔진
</span></span><span style="display:flex;"><span>  ├── model2/
</span></span><span style="display:flex;"><span>  │     ├── config.pbtxt
</span></span><span style="display:flex;"><span>  │     ├── 1/                  # 버전 1 
</span></span><span style="display:flex;"><span>  │     │   └── model.onnx      # ONNX 모델
</span></span><span style="display:flex;"><span>  │     └── 2/                  # 버전 2
</span></span><span style="display:flex;"><span>  │         └── model.onnx      # 업데이트된 ONNX 모델
</span></span><span style="display:flex;"><span>  └── ensemble_model/
</span></span><span style="display:flex;"><span>        ├── config.pbtxt        # 앙상블 구성
</span></span><span style="display:flex;"><span>        └── 1/                  # 버전 1
</span></span></code></pre></div><h3 id="3-동적-배치-처리와-수평적-확장">3. 동적 배치 처리와 수평적 확장<a hidden class="anchor" aria-hidden="true" href="#3-동적-배치-처리와-수평적-확장">#</a></h3>
<p>Triton의 강력한 기능 중 하나는 **동적 배치 처리(Dynamic Batching)**이다. 이는 개별 추론 요청을 자동으로 배치화하여 처리량을 극대화한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># config.pbtxt의 동적 배치 설정 예시
</span></span><span style="display:flex;"><span>name: &#34;resnet50&#34;
</span></span><span style="display:flex;"><span>platform: &#34;tensorrt_plan&#34;
</span></span><span style="display:flex;"><span>max_batch_size: 128
</span></span><span style="display:flex;"><span>dynamic_batching {
</span></span><span style="display:flex;"><span>  preferred_batch_size: [8, 16, 32, 64]
</span></span><span style="display:flex;"><span>  max_queue_delay_microseconds: 5000
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>NVIDIA의 연구[7]와 실제 테스트에 따르면, 적절한 배치 크기와 대기 시간 설정은 GPU 활용도를 최대 95%까지 높일 수 있는 것으로 나타났다. 이 설정을 통해 Triton은 최대 5ms 동안 요청을 대기시키면서 선호하는 배치 크기로 그룹화하여 처리한다.</p>
<p>또한 Kubernetes와 같은 환경에서 Triton 서버를 <strong>수평적으로 확장</strong>하여 대규모 트래픽을 처리할 수 있었다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># Kubernetes 배포 예시 (간략화)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">triton-server</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">5</span>  <span style="color:#75715e"># 서버 인스턴스 수</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">triton</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">triton</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">triton-server</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nvcr.io/nvidia/tritonserver:22.07-py3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;tritonserver&#34;</span>, <span style="color:#e6db74">&#34;--model-repository=/models&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">nvidia.com/gpu</span>: <span style="color:#ae81ff">1</span>  <span style="color:#75715e"># 인스턴스당 GPU 수</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">model-volume</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/models</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">model-volume</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">model-pvc</span>
</span></span></code></pre></div><h2 id="실전-tensorrt와-triton-적용-사례와-도전">실전 TensorRT와 Triton 적용 사례와 도전<a hidden class="anchor" aria-hidden="true" href="#실전-tensorrt와-triton-적용-사례와-도전">#</a></h2>
<p>실제 프로젝트에서 TensorRT와 Triton을 적용하면서 마주친 다양한 도전과 해결책을 살펴보자.</p>
<h3 id="1-복잡한-아키텍처-모델의-변환-도전">1. 복잡한 아키텍처 모델의 변환 도전<a hidden class="anchor" aria-hidden="true" href="#1-복잡한-아키텍처-모델의-변환-도전">#</a></h3>
<p>최신 AI 연구에서 나온 복잡한 아키텍처의 모델을 TensorRT로 변환하는 과정은 종종 도전적이었다. 특히 <strong>트랜스포머 기반 모델</strong>의 경우, 동적 형상과 함께 변환하는 것이 까다로웠다.</p>
<p>예를 들어, BERT 모델을 TensorRT로 최적화할 때는 다음과 같은 접근 방식이 도움이 되었다:</p>
<ol>
<li><strong>플러그인 활용</strong>: NVIDIA의 TensorRT 플러그인 사용 (예: Multi-Head Attention)</li>
<li><strong>퓨전 허용 패턴</strong>: 특정 연산 시퀀스를 TensorRT가 최적화할 수 있도록 모델 구조 조정</li>
<li><strong>최적의 ONNX opset 버전 선택</strong>: 모델 구조에 맞는 적절한 ONNX opset 사용</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># BERT용 TensorRT 최적화 설정 예시</span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> builder<span style="color:#f92672">.</span>create_builder_config()
</span></span><span style="display:flex;"><span>config<span style="color:#f92672">.</span>max_workspace_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">&lt;&lt;</span> <span style="color:#ae81ff">30</span>  <span style="color:#75715e"># 4GB</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 플러그인 활용</span>
</span></span><span style="display:flex;"><span>plugin_registry <span style="color:#f92672">=</span> trt<span style="color:#f92672">.</span>get_plugin_registry()
</span></span><span style="display:flex;"><span>bert_plugin_creator <span style="color:#f92672">=</span> plugin_registry<span style="color:#f92672">.</span>get_plugin_creator(<span style="color:#e6db74">&#34;CustomBertPlugin&#34;</span>, <span style="color:#e6db74">&#34;1&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>plugin <span style="color:#f92672">=</span> bert_plugin_creator<span style="color:#f92672">.</span>create_plugin(<span style="color:#e6db74">&#34;bert_plugin&#34;</span>, plugin_params)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 네트워크에 플러그인 레이어 추가</span>
</span></span><span style="display:flex;"><span>plugin_layer <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>add_plugin_v2([<span style="color:#f92672">...</span>], plugin)
</span></span></code></pre></div><h3 id="2-추론-지연-시간-vs-처리량-최적화">2. 추론 지연 시간 vs 처리량 최적화<a hidden class="anchor" aria-hidden="true" href="#2-추론-지연-시간-vs-처리량-최적화">#</a></h3>
<p>실시간 추론이 필요한 애플리케이션(예: 자율주행)과 대량 배치 처리가 필요한 애플리케이션(예: 오프라인 영상 분석) 사이에는 상충관계가 있다.</p>
<p><strong>지연 시간 최적화</strong>를 위해서는:</p>
<ul>
<li>작은 배치 크기 사용</li>
<li>스트림 병렬성 최소화</li>
<li>정확한 워크로드에 맞는 프로필 최적화</li>
</ul>
<p><strong>처리량 최적화</strong>를 위해서는:</p>
<ul>
<li>큰 배치 크기 활용</li>
<li>다중 CUDA 스트림으로 병렬 처리</li>
<li>메모리 사용량과 계산 효율성 균형</li>
</ul>
<p>서비스의 SLA(Service Level Agreement)에 따라 이 두 목표 사이의 적절한 균형점을 찾아야 했다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Triton에서 지연 시간 vs 처리량 최적화 설정 예시</span>
</span></span><span style="display:flex;"><span>instance_group {
</span></span><span style="display:flex;"><span>  count: <span style="color:#ae81ff">2</span>  <span style="color:#75715e"># GPU당 모델 인스턴스 수</span>
</span></span><span style="display:flex;"><span>  kind: KIND_GPU
</span></span><span style="display:flex;"><span>  gpus: [<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 지연 시간 최적화</span>
</span></span><span style="display:flex;"><span>dynamic_batching {
</span></span><span style="display:flex;"><span>  max_queue_delay_microseconds: <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># 매우 짧은 대기 시간</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 처리량 최적화</span>
</span></span><span style="display:flex;"><span>dynamic_batching {
</span></span><span style="display:flex;"><span>  preferred_batch_size: [<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>]
</span></span><span style="display:flex;"><span>  max_queue_delay_microseconds: <span style="color:#ae81ff">5000</span>  <span style="color:#75715e"># 더 긴 대기 시간 허용</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="3-메모리-최적화와-다중-모델-배포">3. 메모리 최적화와 다중 모델 배포<a hidden class="anchor" aria-hidden="true" href="#3-메모리-최적화와-다중-모델-배포">#</a></h3>
<p>대규모 멀티 모델 시스템에서는 <strong>GPU 메모리 관리</strong>가 중요한 도전 과제였다. Triton에서는 다음과 같은 전략을 사용했다:</p>
<ul>
<li><strong>인스턴스 그룹 최적화</strong>: GPU당 적절한 모델 인스턴스 수 설정</li>
<li><strong>모델 로드 정책</strong>: 필요시에만 메모리에 로드하는 정책 사용</li>
<li><strong>우선순위 스케줄링</strong>: 중요한 모델에 리소스 우선 할당</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># config.pbtxt의 메모리 최적화 설정
</span></span><span style="display:flex;"><span>instance_group {
</span></span><span style="display:flex;"><span>  count: 1
</span></span><span style="display:flex;"><span>  kind: KIND_GPU
</span></span><span style="display:flex;"><span>  gpus: [0]
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 메모리 효율적인 로드 정책
</span></span><span style="display:flex;"><span>model_transaction_policy {
</span></span><span style="display:flex;"><span>  decoupled: false
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dynamic_batching { ... }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 필요시에만 모델 로드
</span></span><span style="display:flex;"><span>model_control_mode: EXPLICIT
</span></span></code></pre></div><p>특히 여러 모델을 동시에 서빙해야 하는 경우, <strong>모델별 리소스 할당</strong>을 세심하게 조정하여 전체 시스템 성능을 최적화할 수 있었다.</p>
<h2 id="실전-디버깅과-성능-분석-도구">실전 디버깅과 성능 분석 도구<a hidden class="anchor" aria-hidden="true" href="#실전-디버깅과-성능-분석-도구">#</a></h2>
<p>TensorRT와 Triton을 효과적으로 활용하려면 강력한 디버깅 및 성능 분석 도구가 필수적이다.</p>
<h3 id="1-nvidia-nsight-systems">1. NVIDIA Nsight Systems<a hidden class="anchor" aria-hidden="true" href="#1-nvidia-nsight-systems">#</a></h3>
<p><a href="https://developer.nvidia.com/nsight-systems">NVIDIA Nsight Systems</a>는 GPU 작업의 타임라인을 시각화하여 병목 현상을 식별하는 데 유용하다. 특히 호스트와 디바이스 간의 동기화 이슈나 커널 실행 패턴을 분석하는 데 탁월했다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Nsight Systems로 Triton 서버 프로파일링</span>
</span></span><span style="display:flex;"><span>nsys profile -t cuda,nvtx -o profile_report --capture-range<span style="color:#f92672">=</span>cudaProfilerApi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    tritonserver --model-repository<span style="color:#f92672">=</span>/models
</span></span></code></pre></div><h3 id="2-tensorrt-trtexec-도구">2. TensorRT trtexec 도구<a hidden class="anchor" aria-hidden="true" href="#2-tensorrt-trtexec-도구">#</a></h3>
<p><code>trtexec</code>는 TensorRT 엔진을 벤치마킹하고 분석하기 위한 명령줄 도구로, 다양한 설정에서 모델 성능을 빠르게 평가할 수 있다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># trtexec을 사용한 성능 벤치마킹</span>
</span></span><span style="display:flex;"><span>trtexec --onnx<span style="color:#f92672">=</span>model.onnx <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --saveEngine<span style="color:#f92672">=</span>model.trt <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --fp16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --verbose <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --shapes<span style="color:#f92672">=</span>input:8x3x224x224 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>        --avgRuns<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>
</span></span></code></pre></div><p>이 도구를 통해 다양한 배치 크기, 정밀도 및 최적화 설정에 따른 성능 차이를 체계적으로 측정할 수 있었다.</p>
<h3 id="3-triton-모델-분석기">3. Triton 모델 분석기<a hidden class="anchor" aria-hidden="true" href="#3-triton-모델-분석기">#</a></h3>
<p>Triton은 배포된 모델의 성능을 분석하기 위한 <code>perf_analyzer</code> 도구를 제공한다:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Triton 모델 성능 분석</span>
</span></span><span style="display:flex;"><span>perf_analyzer -m resnet50 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>              -u localhost:8000 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>              -i grpc <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>              --concurrency-range 1:16 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>              --shape input:3,224,224
</span></span></code></pre></div><p>이 도구를 통해 동시성 수준에 따른 지연 시간과 처리량을 측정하고, 최적의 서버 구성을 결정할 수 있었다.</p>
<h2 id="tensorrt와-triton의-현실적인-한계와-대안">TensorRT와 Triton의 현실적인 한계와 대안<a hidden class="anchor" aria-hidden="true" href="#tensorrt와-triton의-현실적인-한계와-대안">#</a></h2>
<p>TensorRT와 Triton은 강력하지만, 몇 가지 제한 사항도 존재한다:</p>
<h3 id="1-모델-호환성-도전">1. 모델 호환성 도전<a hidden class="anchor" aria-hidden="true" href="#1-모델-호환성-도전">#</a></h3>
<p>모든 모델이 TensorRT로 완벽하게 변환되지는 않는다. 특히:</p>
<ul>
<li><strong>커스텀 연산자</strong>: 표준 연산자가 아닌 경우 플러그인 개발 필요</li>
<li><strong>동적 제어 흐름</strong>: 조건문과 루프가 복잡한 모델의 경우 변환 어려움</li>
<li><strong>최신 아키텍처</strong>: 최신 연구 모델은 지원이 지연될 수 있음</li>
</ul>
<p>이런 경우의 대안으로, 복잡한 부분만 PyTorch로 유지하고 나머지를 TensorRT로 최적화하는 <strong>하이브리드 접근법</strong>을 사용했다.</p>
<h3 id="2-개발-편의성-vs-최적화-수준">2. 개발 편의성 vs 최적화 수준<a hidden class="anchor" aria-hidden="true" href="#2-개발-편의성-vs-최적화-수준">#</a></h3>
<p>TensorRT는 학습 프레임워크에 비해 개발 편의성이 떨어진다. 모델 최적화에 소요되는 엔지니어링 시간과 얻을 수 있는 성능 향상 사이의 균형을 고려해야 한다.</p>
<p>간단한 경우, <a href="https://pytorch.org/docs/stable/generated/torch.compile.html">PyTorch 2.0의 torch.compile()</a>과 같은 기능이 적절한 대안이 될 수 있었다.</p>
<h3 id="3-멀티-클라우드-및-이종-환경-지원">3. 멀티 클라우드 및 이종 환경 지원<a hidden class="anchor" aria-hidden="true" href="#3-멀티-클라우드-및-이종-환경-지원">#</a></h3>
<p>NVIDIA GPU에 최적화된 TensorRT는 다른 하드웨어 환경에서는 활용할 수 없다. 멀티 클라우드 전략이나 이종 하드웨어 환경을 고려한다면, <a href="https://tvm.apache.org/">Apache TVM</a>이나 <a href="https://onnxruntime.ai/">ONNX Runtime</a>과 같은 대안을 함께 검토해야 했다.</p>
<h2 id="결론">결론<a hidden class="anchor" aria-hidden="true" href="#결론">#</a></h2>
<p>TensorRT와 Triton Inference Server는 딥러닝 모델의 프로덕션 배포에 있어 강력한 도구이다. 적절한 최적화와 구성을 통해 극적인 성능 향상과 비용 절감을 달성할 수 있지만, 이는 깊은 이해와 체계적인 접근을 필요로 한다. 결국은 많이 사용해보고 공부해야 적절한 최적화를 감당할 수 있을 것이라는 생각이 들었다.</p>
<p>앞으로도 이런 최적화 방법론들은 계속 발전할 것이며, 엔지니어로서 이러한 도구의 내부 작동 방식을 이해하는 것은 효과적인 AI 시스템 구축에 있어 도움이 되지 않을까 싶다.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>[1] NVIDIA TensorRT 공식 문서: <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html</a></li>
<li>[2] NVIDIA Triton Inference Server: <a href="https://github.com/triton-inference-server/server">https://github.com/triton-inference-server/server</a></li>
<li>[3] TensorRT 최적화 기술: <a href="https://on-demand.gputechconf.com/gtc/2019/presentation/s9431-tensorrt-inference-optimization.pdf">https://on-demand.gputechconf.com/gtc/2019/presentation/s9431-tensorrt-inference-optimization.pdf</a></li>
<li>[4] Triton 성능 모범 사례: <a href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/performance_tuning.md">https://github.com/triton-inference-server/server/blob/main/docs/user_guide/performance_tuning.md</a></li>
<li>[5] NVIDIA Deep Learning Examples: <a href="https://github.com/NVIDIA/DeepLearningExamples">https://github.com/NVIDIA/DeepLearningExamples</a></li>
<li>[6] Han, S., Shen, H., Philipose, M., Agarwal, S., Wolman, A., &amp; Krishnamurthy, A. (2016). MCDNN: An approximation-based execution framework for deep stream processing under resource constraints. In Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services (pp. 123-136).</li>
<li>[7] Nvidia. (2022). TensorRT: A platform for deep learning inference. In GTC 2022.</li>
<li>[8] Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., &hellip; &amp; Ren, M. (2018). TVM: An automated end-to-end optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (pp. 578-594).</li>
</ul>

        </div>
        <div class="post-reward">
            <div style="padding: 0 0 0 0; margin: 0 0 0 0; width: 100%; font-size:16px; text-align: center;">
                <div id="QR" style="opacity: 0;">
                    <div id="wechat" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="wechat_qr" src="https://macsim2.github.io/img/wechat_pay.png" alt="wechat_pay"></a>
                        <p>微信</p>
                    </div>
                    <div id="alipay" style="display: inline-block">
                        <a class="fancybox" rel="group">
                            <img id="alipay_qr" src="https://macsim2.github.io/img/alipay.png" alt="alipay"></a>
                        <p>支付宝</p>
                    </div>
                </div>
                
            </div>
        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://macsim2.github.io/posts/tech/asr/ctc_loss/">
    <span class="title">« 이전 페이지</span>
    <br>
    <span>CTC Loss와 RNN-T Loss의 내부 구조와 동작 원리</span>
  </a>
  <a class="next" href="https://macsim2.github.io/posts/tech/inference_optimzation/onnx_graph/">
    <span class="title">다음 페이지 »</span>
    <br>
    <span>ONNX Graph의 내부 구조와 최적화 여정</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on twitter"
       href="https://twitter.com/intent/tweet/?text=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f&amp;hashtags=deeplearning%2ctensorrt%2cnvidiatriton%2cinferenceoptimization%2cmodeldeployment%2cgpuacceleration">
    <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on linkedin"
       href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f&amp;title=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9&amp;summary=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9&amp;source=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on reddit"
       href="https://reddit.com/submit?url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f&title=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on facebook"
       href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on whatsapp"
       href="https://api.whatsapp.com/send?text=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9%20-%20https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share TensorRT와 Triton: 딥러닝 추론 최적화의 강력한 조합 on telegram"
       href="https://telegram.me/share/url?text=TensorRT%ec%99%80%20Triton%3a%20%eb%94%a5%eb%9f%ac%eb%8b%9d%20%ec%b6%94%eb%a1%a0%20%ec%b5%9c%ec%a0%81%ed%99%94%ec%9d%98%20%ea%b0%95%eb%a0%a5%ed%95%9c%20%ec%a1%b0%ed%95%a9&amp;url=https%3a%2f%2fmacsim2.github.io%2fposts%2ftech%2finference_optimzation%2ftensorrt%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

        </footer>
    </div>

<div id="disqus_thread"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://basic-18.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: '👉展开评论';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: '👇关闭评论';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js">
    </script>
    <script>
        twikoo.init({
            envId:  null ,
        el: "#tcomment",
            lang: 'en-US',
            region:  null ,
        path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        })
    </script>
</div>

</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2020-2025
        <a href="https://macsim2.github.io/" style="color:#939393;">macsim&#39;s Blog</a>
        All Rights Reserved
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;"></a>&nbsp;
    <span>
        <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=null"
           style="display:inline-block;text-decoration:none;height:20px;color:#939393;">
            <img src="" style="float:left;margin: 0px 5px 0px 0px;"/>
            
        </a>
    </span>
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '복사';

        function copyingDone() {
            copybutton.innerText = '복사 완료!';
            setTimeout(() => {
                copybutton.innerText = '복사';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\n————————————————\r\n' +
                    '版权声明：本文为「'+"macsim's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>
</body>

</html>
